---
title: "Chapter 2: Optimal Classification"
author: "Shao-Ting Chiu"
date: "2022-09-02"
---

## Stoachastic analysis

<iframe width="560" height="315" src="https://www.youtube.com/embed/NzHs1eQg78s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Classification without predictors

$$
\hat{Y}=
\begin{cases}
    0, & P(Y=0) \geq P(Y=1)\\ 
    1, & P(Y=1) > P(Y=0)\\
\end{cases}
$$

- $E[Y] = P(Y=1)$

::: {.callout-important}
### Posterior-probability function

$$\eta(x) = E[Y|X=x] = P(Y=1|X=x), \quad x\in R^d$$

:::


## Classification error

$$\epsilon[\psi] = p(\psi(X)\neq Y) = p(\{(x,y)|\psi(x)\neq y\})$$

The classification error is determined by **the feature-label distribution** $P_{X,Y}$

## Class-specific errors


## Bayes Classifier

$$\psi^{*} = \arg\min_{\psi\in\mathcal{C}} P(\psi(X) \neq Y) $$

![](img/proof_bayesclas.png)


## Feature transformation

$$\epsilon^{*}(x,y) \geq \epsilon^{*}(X', y)\quad \text{with } X = t^{-1}(X')$$


