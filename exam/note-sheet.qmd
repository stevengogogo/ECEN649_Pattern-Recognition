---
format: 
    pdf:
        classoption: [letter, landscape]
        margin-bottom: 3mm
        margin-left: 3mm
        margin-right: 3mm
        margin-top: 3mm  
        number-depth: 1
        fontsize: 8pt
        indent: 0m
        keep-tex: false
        header-includes: |
            \usepackage{setspace}
            \setstretch{0.5}
            \usepackage{lipsum} 
            \usepackage{multicol}
            \setlength\columnseprule{0.5pt}
            \usepackage{enumitem}
            \setlist[itemize,1]{leftmargin=\dimexpr 0.13in}
            \usepackage{supertabular}
        include-before-body: src/prebody.tex
        include-after-body: src/postbody.tex
--- 

## Chapter 1: 

## Chapter 2: Optimal Classification

- **Error of classifier**.: $\epsilon[\psi(x)] = P(\psi(X)\neq Y) = \underbrace{p(\psi(X)=1|Y=0)}_{\epsilon^{0}=\int_{\{x|\psi(x)=1\}} p(x|Y=0)dx}P(Y=0)+\underbrace{p(\psi(X)=0|Y=1)}_{\epsilon^{1}=\int_{\{x|\psi(x)=0\}} p(x|Y=1)dx}P(Y=1)$
- **Cond. error**: $\epsilon[\psi|X]=P(\psi(X)\neq Y|X=x) = P(\psi(X)=0, Y=1 | X=x) + P(\psi(X)=1, Y=0|X=x)= I_{\{\psi(x)=0\}}\eta(x) + I_{\{\psi(x)=1\}}(1-\eta(x))$
- **Post.prob.func.**: $\eta(x) = E[Y|X=x] = P(Y=1|X=x)$
- **Sensitivity**: $1-\epsilon^{1}[\psi]$; **Specificity**: $1-\epsilon^{0}[\psi]$
- **Thm.** *Bayes classifier*: 
\begin{equation}\psi^{*}(x)=\arg\max_i P(Y=i|X=x)=\begin{cases}
    1, & \eta(x) >\frac{1}{2}\\ 
    0, & \text{otherwise}
\end{cases}\end{equation}
- **Thm. Bayes Error**: $\epsilon^{*} = P(Y=0)\epsilon^{0}[\psi^{*}] + P(Y=1)\epsilon^{1}[\psi^{*}]= E[\min\{\eta(X), 1-\eta(x)\}] = \frac{1}{2} - \frac{1}{2}E[|2\eta(X)-1|]$

- **Bayes class.**: $\psi^{*}(x)= \begin{cases}1 & \overbrace{D^{*}(x)}^{\text{opt. discriminant}}> \overbrace{k^{*}}^{\text{opt. threshold}} \\  &= P(Y=1)p(x|Y=1)>\\ &P(Y=0)p(x|Y=0) \\ 0, & \text{otherwise} \end{cases}$


- $D^{*}(x) = \ln\frac{p(x|Y=1)}{p(x|Y=0)}$; $k^{*} = \ln \frac{P(Y=0)}{P(Y=1)}$

---

**Gaussian Prob.**: $p(x|Y=i) = \frac{1}{\sqrt{(2\pi)^{d}\det(\Sigma_i)}}\exp[\frac{1}{2}(x-\mu)^T \Sigma_{i}^{-1} (x-\mu_i)]$

- $D^{*}(x) = \frac{1}{2}(x-\mu_0)^T \Sigma^{-1}_0 (x-\mu_0) - \frac{1}{2}(x-\mu_1)^T\Sigma_{1}^{-1}(x-\mu_1) + \frac{1}{2}\ln \frac{\det(\Sigma_0)}{\det(\Sigma_1)}$

**Homo. Case**: Let $\|x_0 - x_1\|_{\Sigma} = \sqrt{(x_0-x_1)^{T}\Sigma^{-1}(x_0 - x_1)}$
    \begin{align*}
    \psi^{*}_{L}(x)%
    &= \begin{cases}1,& \|x-\mu_1\|^{2}_{\Sigma}<\| x-\mu_0\|^{2}_{\Sigma} + 2 \ln\frac{P(Y=1)}{P(Y=0)}\\& = a^T x +b > 0\\ 0, &\text{otherwise}\end{cases}
    \end{align*}

- $a = \Sigma^{-1}(\mu_1 - \mu_0)$ / $b= (\mu_0 - \mu_1)^T \Sigma^{-1}(\frac{}{2})$; $b=(\mu_0 - \mu_1)^T \Sigma^{-1}(\frac{\mu_0 + \mu_1}{2}) + \ln\frac{P(Y=1)}{P(Y=0)}$
- $\epsilon^{*}_{L} = c\Phi(\frac{k^{*}-\frac{1}{2}\delta^2}{\delta}) + (1-c)\Phi(\frac{-k^{*}-\frac{1}{2}\delta^2}{\delta}), \delta=\sqrt{(\mu_1 -\mu_0)^T \Sigma^{-1} (\mu_1 - \mu_0)}$

**Heter. Case**: $\psi^{*}_{Q}(x) = \begin{cases}1, & x^T A x + b^T x+c>0, \\ 0, & \text{otherwise}\end{cases}$

- $A = \frac{1}{2}(\Sigma^{-1}_{0} - \Sigma^{-1}_{1}); b = \Sigma^{-1}_{1}\mu_1 - \Sigma^{-1}_{0}\mu_0; c = \frac{1}{2}(\mu_{0}^{T}\Sigma^{-1}_{0}\mu_0 - \mu_{1}^{T}\Sigma^{-1}_{1} \mu_1) + \frac{1}{2}\ln \frac{\det \Sigma_0}{\det \Sigma_1} + \ln\frac{P(Y=1)}{P(Y=0)}$

## Chapter 3: Sample-Based Classification

- **No-Free-Lunch**: One can never know if their finite-sample performance will be satisfactory, no matter how large $n$ is.

## Chapter 4: Parametric Classification

- **Linear Discriminant Analysis (LDA)**: $\hat{\Sigma}^{ML}_{0} = \frac{1}{N_0 - 1}\sum^{n}_{i=1} (X_i - \hat{\mu}_0)(X_i-\hat{\mu}_{0})^T I_{Y_i = 0}$, $\hat{\Sigma} = \frac{\hat{\Sigma}_{0} + \hat{\Sigma}_1}{2}$
    - Boundary: $a^{T}_{n}x+b_n = k_n$. 
        - $a_{n} = \hat{\Sigma}^{-1}(\hat{\mu}_1 - \hat{\mu}_0) = \begin{bmatrix}a_1 \\ a_2\end{bmatrix}$
        - $b_n = (\hat{\mu}_{0} - \hat{\mu}_{1})^{T} \hat{\Sigma}^{-1} (\frac{\hat{\mu}_0 + \hat{\mu}_1}{2}) = number$

- **Diagnoal LDA**: Make $\hat{\Sigma} \to \hat{\Sigma}_D = \begin{bmatrix}\Sigma_{1,1} & 0\\ 0 & \Sigma_{2,2}\end{bmatrix}$

- **Nearest-Mean Class.(NMC)**: $\hat{\Sigma}_M = \begin{bmatrix} \hat{\sigma}_{ij}^{2} & 0 \\ 0 & \hat{\sigma}_{ij}^{2} \end{bmatrix}$. $\hat{\sigma}^2 = \sum^{d}_{k=1}(\hat{\Sigma})_{kk}$. Given $k_{n} = 0$, $a=\hat{\mu}_1 - \hat{\mu}_0$ $b = (\hat{\mu}_{0} - \hat{\mu}_1)^T \left(\frac{\hat{\mu}_0 + \hat{\mu}_1}{2}\right)$. Boundary is $\perp$ means


## Chapter 5: 

- Histogram Classification: \begin{equation}
    W_{n,h}(x,X_i) = \begin{cases}
        \frac{1}{N_h(x)}, & X_i \in A_h(x)\\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

- **Thm. Cover-Hart**: $\epsilon_{NN} = E[2\eta(X)(1-\eta(x))]$

### Bayes Error


\lipsum[2-7]


## Key points \& Definitions


- The \underline{posterior probability function} is needed to define the Bayes classifier.
- \underline{Bayes error} is optimal error
- \underline{LDA} is parameteric



1. What are the minimum and the maximal values the Bayes error can take on in binary classification? Explain what each case means.

sample answer

2. Why is the expected classification error $\mu = E[\text{error}_n]$ not a function of the training data.

3. What does it mean to say that an error estimator is optimistically biased?


4. Is a consistent classification rule always better than a non-consistent one and why?


5. If a classifier is overfitted, will its apparent error (i.e., the error on the training data) tend to be smaller, larger, or the same as the true error? Explain why.


7. Describe the basic difference between filter and wrapper feature selection.

9. What is the penalty term in an SVM and what is it used for?

10. How many points does the minimal nonlinearly-separable problem in 2 dimensions have? Give an example.

## Math facts

- Bayes: $P(Y=0|X=x) = \frac{P(Y=0)P(x|Y=0)}{P(x)}$
- $\det\begin{bmatrix}a & b\\ c & d\end{bmatrix} = ad-bc$ ;  $\begin{bmatrix}a & b\\ c & d\end{bmatrix}^{-1} = \frac{1}{ad-bc}\begin{bmatrix}d & -b \\ -c & a\end{bmatrix}$

\lipsum[2-5]