[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECEN649: Pattern Recognition",
    "section": "",
    "text": "Preface\n\nThis course aims to introduce the basic elements of Pattern Recognition,focusing on critical mathematical and statistical aspects underlying classification. After a review of probability theory, we discuss fundamental concepts of classification, such as the optimal classifier and classification consistency, followed by a study of several families of classification rules, error estimation, dimensionality reduction, and model selection, including an introduction to Vapnik-Chervonenkis theory. Time permitting, clustering and regression will be also covered. Performance will be assessed by means of a midterm, problem sets, and a final class project. There will be coding assignments based on python and sklearn as part of the problem sets. Previous coursework in probability theory and programming skills are assumed. — ECEN649 Syllabus\n\n\nCourse: ECEN649 (special topic), 2022Fall\nInstructor: Dr. Ulisses Braga-Neto\nBook Website\nFinal project"
  },
  {
    "objectID": "intro.html#website",
    "href": "intro.html#website",
    "title": "1  Introduction",
    "section": "1.1 Website",
    "text": "1.1 Website\nhttps://braganeto.engr.tamu.edu/book-website/\n\nAll the learning materials is here."
  },
  {
    "objectID": "ch1.html#lecture",
    "href": "ch1.html#lecture",
    "title": "2  Chapter 01",
    "section": "2.1 Lecture",
    "text": "2.1 Lecture\n\nCh1 slides"
  },
  {
    "objectID": "ch1.html#pattern-recognition",
    "href": "ch1.html#pattern-recognition",
    "title": "2  Chapter 01",
    "section": "2.2 Pattern Recognition",
    "text": "2.2 Pattern Recognition\n\nA pattern is the opposite of randomness.\nOn the other hand, there is “randomness” between two events if they are independent.\n\n\nFor example, musical preference is independent of the occurrence of heart disease."
  },
  {
    "objectID": "ch1.html#machine-learning",
    "href": "ch1.html#machine-learning",
    "title": "2  Chapter 01",
    "section": "2.3 Machine Learning",
    "text": "2.3 Machine Learning\n\nPattern Recognition and Machine Learning have substantial overlap with each other.\n\n\nStatistical pattern recognition\nSynthetic pattern recognition\n\non the other hand, it is not statistically reasoning."
  },
  {
    "objectID": "ch1.html#basic-mathematical-setting",
    "href": "ch1.html#basic-mathematical-setting",
    "title": "2  Chapter 01",
    "section": "2.4 Basic Mathematical Setting",
    "text": "2.4 Basic Mathematical Setting\n\nA vector of measurements\n\n\\(X\\in R^d\\)\nknown as a feature vector\na target \\(Y\\in R\\) to be predicted\n\nFeature vector\n\n\\(X\\)\n\nTarget vector\n\n\\(Y\\)\n\nThe relationship between \\(X\\) and \\(Y\\) (Figure 2.1)\n\nrarely determinsitic\nThere is no function \\(f\\) such that \\(Y=f(X)\\)\nbut express as a joint probability distribution \\(P_{X,Y}\\)\n\nSource of uncertainty\n\nLatent factors\n\n\\(Y\\) depends on factors that are not available.\n\nMeasurement noise\n\nThe values of the predictor \\(X\\) itself\n\n\n\n\n\n\nFigure 2.1: X-Y relation. There is stochastic relationship. The proble is to model this relationship."
  },
  {
    "objectID": "ch1.html#prediction",
    "href": "ch1.html#prediction",
    "title": "2  Chapter 01",
    "section": "2.5 Prediction",
    "text": "2.5 Prediction\n\nA predictor\n\n\\(\\psi: R^d \\to R\\)\n\\(\\psi(X) \\to Y\\)\n\nPredictor (\\(\\psi\\)) uses information about the joint feature-label distribution \\(P_{X,Y}\\)\n\nDirect knowledge about \\(P_{X,Y}\\)\n\nDistribution information\n\nIndirect knowlege about \\(P_{X,Y}\\)\n\nI.I.D. sample \\(S_n =\\{(X_1,Y_1),...,(X_n, Y_n)\\}\\)\n\ntraining dat\n\n\n\n\n\nThe pure data-driven method will ultimately fail.\n\n\nProbabilistic method\n\nClassical\nBayesian method\n\n\n\nWhy is not everyone using Bayesian?\n\nBayesian method is complicated, especially the Bayesian inference.\n\nOptimal precictor\n\n\\(\\psi^{*}(X)\\) with complete knowledge of \\(F_{X,Y}\\)\n\nObstacles\n\nKnowlege of \\(P_{X,Y}\\) is unavailable\nData-driven prediction rule must rely solely on \\(S_{n}\\)\nCertain data-driven predictors can approach the optimal predictor as \\(n\\to \\infty\\)\nHowever, the convergence rate mush be arbitrarily slow in the worst case.\nNo-free-lunch theorem\n\nfor finite \\(n\\), which is the practical case, having knowledge about \\(F_{X,Y}\\) is necessary to guarantee good performance."
  },
  {
    "objectID": "ch1.html#prediction-error",
    "href": "ch1.html#prediction-error",
    "title": "2  Chapter 01",
    "section": "2.6 Prediction error",
    "text": "2.6 Prediction error\n\n\n\nFigure 2.2: Prediction error\n\n\n\nQuadratic loss\n\n\\(\\mathcal{l}(\\psi(X),Y) = (Y-\\psi(X))^2\\)\n\nAbsolute difference loss\n\n\\(\\mathcal{l}(\\psi(X),Y) = |Y-\\psi(X)|\\)\n\nMisclassification loss\n\n\\(\\mathcal{l}(\\psi(X),Y) = 1, 0 (Y\\neq \\psi(X), Y=\\psi(X))\\)\n\n\n\nWhen your target is a lable, you are taling about classification."
  },
  {
    "objectID": "ch1.html#supervised-vs.-unsupervisied-learning",
    "href": "ch1.html#supervised-vs.-unsupervisied-learning",
    "title": "2  Chapter 01",
    "section": "2.7 Supervised vs. Unsupervisied learning",
    "text": "2.7 Supervised vs. Unsupervisied learning\n\n\\(Y\\in \\{0,1,\\cdots, c-1\\}\\)\nVariable \\(Y\\) is called a label to emphasize that it has no numerica meaning.\nBinary classification\nExpection of a random variable is an event.\n\n\n2.7.1 Categories\n\nRegerssion\nUnsupervised learning\n\nError of the operation is not straightforward\nEx: PCA and clustering\n\nSemi-supervised learning\n\nTarget \\(Y\\) is available for only a subpopulation of the feature vector \\(X\\)\nSome \\(X\\) doesn’t have \\(Y\\)\nSee more explanation1\n\nReinforcement learning\n\ndecision are made in continuous interaction with an environment.\nThe objective is to minimize a cost over the long run.\nDynamic programming"
  },
  {
    "objectID": "ch1.html#basic-mathematical-setting-1",
    "href": "ch1.html#basic-mathematical-setting-1",
    "title": "2  Chapter 01",
    "section": "2.8 Basic mathematical setting",
    "text": "2.8 Basic mathematical setting\n\\(\\begin{align} \\epsilon[\\psi] = E[I_{Y\\neq \\psi(X)}] &= 0\\cdot P(S_A=0) + 1\\cdot P(S_A=1)\\\\  &= P(S_A=1) \\end{align}\\)"
  },
  {
    "objectID": "ch1.html#scissor-effect",
    "href": "ch1.html#scissor-effect",
    "title": "2  Chapter 01",
    "section": "2.9 Scissor effect",
    "text": "2.9 Scissor effect\n\n\n\nScissor Effect."
  },
  {
    "objectID": "ch2.html#stoachastic-analysis",
    "href": "ch2.html#stoachastic-analysis",
    "title": "3  Chapter 2: Optimal Classification",
    "section": "3.1 Stoachastic analysis",
    "text": "3.1 Stoachastic analysis"
  },
  {
    "objectID": "ch2.html#classification-without-predictors",
    "href": "ch2.html#classification-without-predictors",
    "title": "3  Chapter 2: Optimal Classification",
    "section": "3.2 Classification without predictors",
    "text": "3.2 Classification without predictors\n\\[\n\\hat{Y}=\n\\begin{cases}\n    0, & P(Y=0) \\geq P(Y=1)\\\\\n    1, & P(Y=1) > P(Y=0)\\\\\n\\end{cases}\n\\]\n\n\\(E[Y] = P(Y=1)\\)\n\n\n\n\n\n\n\nPosterior-probability function\n\n\n\n\\[\\eta(x) = E[Y|X=x] = P(Y=1|X=x), \\quad x\\in R^d\\]"
  },
  {
    "objectID": "ch2.html#classification-error",
    "href": "ch2.html#classification-error",
    "title": "3  Chapter 2: Optimal Classification",
    "section": "3.3 Classification error",
    "text": "3.3 Classification error\n\\[\\epsilon[\\psi] = p(\\psi(X)\\neq Y) = p(\\{(x,y)|\\psi(x)\\neq y\\})\\]\nThe classification error is determined by the feature-label distribution \\(P_{X,Y}\\)"
  },
  {
    "objectID": "ch2.html#class-specific-errors",
    "href": "ch2.html#class-specific-errors",
    "title": "3  Chapter 2: Optimal Classification",
    "section": "3.4 Class-specific errors",
    "text": "3.4 Class-specific errors"
  },
  {
    "objectID": "ch2.html#bayes-classifier",
    "href": "ch2.html#bayes-classifier",
    "title": "3  Chapter 2: Optimal Classification",
    "section": "3.5 Bayes Classifier",
    "text": "3.5 Bayes Classifier\n\\[\\psi^{*} = \\arg\\min_{\\psi\\in\\mathcal{C}} P(\\psi(X) \\neq Y) \\]"
  },
  {
    "objectID": "ch2.html#feature-transformation",
    "href": "ch2.html#feature-transformation",
    "title": "3  Chapter 2: Optimal Classification",
    "section": "3.6 Feature transformation",
    "text": "3.6 Feature transformation\n\\[\\epsilon^{*}(x,y) \\geq \\epsilon^{*}(X', y)\\quad \\text{with } X = t^{-1}(X')\\]"
  },
  {
    "objectID": "ch3.html#no-free-lunch-theorem",
    "href": "ch3.html#no-free-lunch-theorem",
    "title": "4  Chapter 3: Sampled-Based classification",
    "section": "4.1 No free lunch theorem",
    "text": "4.1 No free lunch theorem"
  },
  {
    "objectID": "ch4.html#outline",
    "href": "ch4.html#outline",
    "title": "5  Chapter 4: Parametric Classification",
    "section": "5.1 Outline",
    "text": "5.1 Outline\n\nGaussian discriminant\n\nLinear Discriminant analysis\nQuadratic discriminant analysis\n\nLogistic classification\nRegularized discriminant analysis\nBayesian parametric classification"
  },
  {
    "objectID": "ch4.html#parametric-plug-in-rules",
    "href": "ch4.html#parametric-plug-in-rules",
    "title": "5  Chapter 4: Parametric Classification",
    "section": "5.2 Parametric Plug-in Rules",
    "text": "5.2 Parametric Plug-in Rules\n\nFeature-label distribution is coded into pdf\n\n\\(\\{p(x|\\theta)|\\theta\\in \\Theta \\subseteq R^m\\}\\)\n\\(\\theta_{0,n}\\) and \\(\\theta_{1,n}\\) be estimators of \\(\\theta^{*}_{0}\\) and \\(\\theta^{*}_{1}\\) based on sample data \\(S_n = \\{(X_1, Y_1),\\dots, (X_n,Y_n)\\}\\).\n\nSample space discriminant\n\n\\(D_n(x) = \\ln\\frac{p(x|\\theta_{1,n})}{p(x|\\theta_{0,n})}\\)\n\n\n\n5.2.1 Strategy for the knowledge about the prior\nLet \\(c_0=P(Y=0)\\) and \\(c_1 = P(Y=1)\\),\n|Knowledge about prior||"
  },
  {
    "objectID": "hw/hw1.html#homework-description",
    "href": "hw/hw1.html#homework-description",
    "title": "6  Homework 1",
    "section": "6.1 Homework Description",
    "text": "6.1 Homework Description\n\nCourse: ECEN649, Fall2022\nProblems (from Chapter 2 in the book): 2.1 , 2.3 (a,b), 2.4, 2.7, 2.9, 2.17 (a,b)\nNote: the book is available electronically on the Evans library website.\n\n\nDeadline: Sept. 26th, 11:59 pm"
  },
  {
    "objectID": "hw/hw1.html#computational-enviromnent-setup",
    "href": "hw/hw1.html#computational-enviromnent-setup",
    "title": "6  Homework 1",
    "section": "6.2 Computational Enviromnent Setup",
    "text": "6.2 Computational Enviromnent Setup\n\n6.2.1 Third-party libraries\n\n%matplotlib inline\nimport sys # system information\nimport matplotlib # plotting\nimport scipy as st # scientific computing\nimport pandas as pd # data managing\nimport numpy as np # numerical comuptation\nimport scipy.optimize as opt\nimport sympy as sp\nimport matplotlib.pyplot as plt\nfrom scipy.special import erf\nfrom numpy.linalg import inv, det\nfrom scipy.linalg import block_diag\nfrom scipy.stats import norm # for problem 2.17 (b)\nfrom scipy.stats import multivariate_normal\n# Matplotlib setting\nplt.rcParams['text.usetex'] = True\nmatplotlib.rcParams['figure.dpi']= 300\nRES_GRID = 90\n\n\n\n6.2.2 Version\n\nprint(sys.version)\nprint(matplotlib.__version__)\nprint(st.__version__)\nprint(np.__version__)\nprint(pd.__version__)\n\n3.8.14 (default, Sep  6 2022, 23:26:50) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]\n3.3.1\n1.5.2\n1.19.1\n1.1.1"
  },
  {
    "objectID": "hw/hw1.html#problem-2.1",
    "href": "hw/hw1.html#problem-2.1",
    "title": "6  Homework 1",
    "section": "6.3 Problem 2.1",
    "text": "6.3 Problem 2.1\n\nSuppose that \\(X\\) is a discrete feature vector, with distribution concentrated over a countable set \\(D=\\{x^{1}, x^2, \\dots\\}\\) in \\(R^{d}\\). Derive the discrete versions of (2.3), (2.4), (2.8), (2.9), (2.11), (2.30), (2.34), and (2.36)\nHint: Note that if \\(X\\) has a discrete distribution, then integration becomes summation, \\(P(X=x_k)\\), for \\(x_k\\in D\\), play the role of \\(p(x)\\), and \\(P(X=x_k | Y=y)\\), for \\(x_k \\in D\\), play the role of \\(p(x|Y=y)\\), for \\(y=0,1\\).\n\n\n6.3.1 (2.3)\n\nFrom Braga-Neto (2020, 16) \\[\\begin{align}\nP(X\\in E, Y=0) &= \\int_E P(Y=0)p(x|Y=0)dx\\\\\nP(X\\in E, Y=1) &= \\int_E P(Y=1)p(x|Y=1)dx\\\\\n\\end{align}\\]\n\nLet \\(x_k = [x_1,\\dots, x_d]\\) be the feature vector of \\(X\\) in set \\(D\\in R^d\\),\n\\[\\begin{align}\n    P(X\\in D, Y=0) &= P(X=[x_1,\\dots, x_d], Y=0)\\\\\n                   &= \\sum_{X_k\\in D} P(Y=0)P(X=[x_1,\\dots, x_d] |Y=0)\\\\\n    P(X\\in D, Y=1) &= P(X=[x_1,\\dots, x_d], Y=1)\\\\\n                   &= \\sum_{X_k\\in D} P(Y=1)P(X=[x_1,\\dots, x_d] |Y=1)\\\\\n\\end{align}\\]\n\n\n6.3.2 (2.4)\n\nFrom Braga-Neto (2020, 17)\n\n\\[\\begin{align}\n    P(Y=0|X=x_k) &= \\frac{P(Y=0)p(X=x_k | Y=0)}{p(X=x_k)}\\\\\n                 &= \\frac{P(Y=0)p(X=x_k |Y=0)}{P(Y=0)p(X=x_k |Y=0) + P(Y=1)p(X=x_k|Y=1)}\\\\\n\\end{align}\\]\n\\[\\begin{align}\n    P(Y=1|X=x_k) &= \\frac{P(Y=1)p(X=x_k | Y=1)}{p(X=x_k)}\\\\\n                 &= \\frac{P(Y=1)p(X=x_k |Y=1)}{P(Y=0)p(X=x_k |Y=0) + P(Y=1)p(X=x_k|Y=1)}\\\\\n\\end{align}\\]\n\n\n6.3.3 (2.8)\n\nFrom Braga-Neto (2020, 18)\n\n\\[\\epsilon^{0}[\\psi] = P(\\psi(X)=1 | Y=0) = \\sum_{\\{x_k|\\psi(x_k)=1\\}} p(x_k|Y=0)\\]\n\\[\\epsilon^{1}[\\psi] = P(\\psi(X)=0 | Y=1) = \\sum_{\\{x_k|\\psi(x_k)=1\\}} p(x_k|Y=1)\\]\n\n\n6.3.4 (2.9)\n\nFrom Braga-Neto (2020, 18)\n\n\\[\\epsilon[\\psi] = \\sum_{\\{x_k|\\psi(x)=1\\}} P(Y=0)p(x_k | Y=0) + \\sum_{\\{x_k|\\psi=0\\}}P(Y= 1)p(x_k|Y=1)\\]\n\n\n6.3.5 (2.11)\n\nFrom Braga-Neto (2020, 19)\n\n\\[\\epsilon[\\psi] = E[\\epsilon[\\psi|X=x_k]] = \\sum_{x_{k}\\in D} \\epsilon[\\psi|X=x_k]p(x_k)\\]\n\n\n6.3.6 (2.30)\n\nFrom Braga-Neto (2020, 24).\n\n\\[\\epsilon^{*} = \\sum_{x_k\\in X} \\left[ I_{\\eta(X=x_k)\\leq 1-\\eta(X=x_k)}\\eta(X=x_k) + I_{\\eta(X=x_k) > 1 - \\eta(X=x_k)(1-\\eta(X=x_k))}\\right] p(X=x_k)\\]\n\n\n6.3.7 (2.34)\n\nFrom Braga-Neto (2020, 25).\n\n\\[\\begin{align}\n    \\epsilon^{*} &= P(Y=0)\\epsilon^{0}[\\psi^*] + P(Y=1)\\epsilon^1 [\\psi^*]\\\\\n                 &= \\sum_{\\{x_k|P(Y=1)p(x_k|Y=1)>P(Y=0)p(x_k|Y=0)\\}} P(Y=0)p(x_k|Y=0) \\\\\n    &+ \\sum_{\\{x_k|P(Y=1)p(x_k|Y=1)\\leq P(Y=0)p(x_k|Y=0)\\}} P(Y=1)p(x_k|Y=1)\n\\end{align}\\]\n\n\n6.3.8 (2.36)\n\nFrom Braga-Neto (2020, 25)\n\n\\[E[\\eta(X)] = \\sum_{x_k\\in R^d} P(Y=1|X=x_k)p(x_k) = P(Y=1)\\]"
  },
  {
    "objectID": "hw/hw1.html#problem-2.3",
    "href": "hw/hw1.html#problem-2.3",
    "title": "6  Homework 1",
    "section": "6.4 Problem 2.3",
    "text": "6.4 Problem 2.3\n\nThis problem seeks to characterize the case \\(\\epsilon^{*}=0\\).\n\n\n6.4.1 (a)\n\nProve the “Zero-One Law” for perfect discrimination: \\[\\epsilon^{*} = 0 \\Leftrightarrow \\eta(X) = 0 \\text{ or } 1 \\quad \\text{with probability } 1. \\tag{6.1}\\]\n\nThe optimal Bayes classifier is defined in Braga-Neto (2020, 20). That is\n\\[\\begin{equation}\n    \\psi^{*}(x) = \\arg\\max_{i} P(Y=i|X=x) =\n    \\begin{cases}\n        1, & \\eta(x) > \\frac{1}{2}\\\\\n        0, & \\text{otherwise}\n    \\end{cases}\n\\end{equation}\\]\nPart 1: \\(\\eta(X)=1\\)\n\\[\\eta(X) = E[Y|X=x] = P(Y=1|X=x) = 1\\]\n\\[\\because \\eta(X)=1>\\frac{1}{2} \\therefore \\psi^{*}(x) = 1\\]\n\\[\\begin{align}\n    \\epsilon^{*} &= \\epsilon[\\psi^{*}(X) | X=x]\\\\\n    &= I_{\\psi^{*}(x) = 0}P(Y=1 | X=x) + I_{\\psi^{*}(x) = 1} P(Y=0|X=x)\\\\\n    &= \\underbrace{I_{\\psi^{*}(x) = 0}e}_{=0}\\underbrace{\\eta(X)}_{=1}+ \\underbrace{I_{\\psi^{*}(x) = 1}}_{=1} \\underbrace{(1-\\eta(X))}_{=0}\\\\\n    &= 0\n\\end{align}\\]\nPart 2: \\(\\eta(X)=0\\)\nSimilarly,\n\\[\\because \\eta(X)=0 \\leq \\frac{1}{2} \\therefore \\psi^{*}(x) = 0\\]\n\\[\\begin{align}\n    \\epsilon^{*} &= \\epsilon[\\psi^{*}(X) | X=x]\\\\\n    &= I_{\\psi^{*}(x) = 0}P(Y=1 | X=x) + I_{\\psi^{*}(x) = 1} P(Y=0|X=x)\\\\\n    &= \\underbrace{I_{\\psi^{*}(x) = 0}}_{=1}\\underbrace{\\eta(X)}_{=0}+ \\underbrace{I_{\\psi^{*}(x) = 1}}_{=0} \\underbrace{(1-\\eta(X))}_{=1}\\\\\n    &= 0\n\\end{align}\\]\nIn conclusion, both cases shows that \\(\\epsilon^{*} = 0\\).\n\n\n6.4.2 (b)\n\nShow that\n\\[\\epsilon^{*} = 0 \\Leftrightarrow \\text{ there is a function } f \\text{ s.t. } Y=f(X) \\text{ with probability } 1\\]\n\n\\[\\begin{equation}\n    \\eta(X) = Pr(Y=1 | X=x) =\n    \\begin{cases}\n        1, & f(X)=1\\\\\n        0, & f(X)=0\n    \\end{cases}\n\\end{equation}\\]\nThe sceneraio is same as Problem 3.7 (a).\n\nGiven \\(\\eta(X) = 1\\)\n\n\\(\\epsilon^{*} = 0\\)\n\nGiven \\(\\eta(X) = 0\\)\n\n\\(\\epsilon^{*} = 0\\)\n\n\n\\(\\epsilon^{*} =0\\) for both cases."
  },
  {
    "objectID": "hw/hw1.html#problem-2.4",
    "href": "hw/hw1.html#problem-2.4",
    "title": "6  Homework 1",
    "section": "6.5 Problem 2.4",
    "text": "6.5 Problem 2.4\n\nThis problem concerns the extension to the multiple-class case of some of the concepts derived in this chapter. Let \\(Y\\in \\{0,1,\\dots,c-1\\}\\), where \\(c\\) is the number of classes, and let \\[\\eta_{i}(x) = P(Y=i|X=x), \\quad i=0,1,\\dots, c-1,\\]\nfor each \\(x\\in R^d\\). We need to remember that these probabilities are not indpendent, but satisfy \\(\\eta_0(x) + \\eta_1(x)+\\dots+\\eta_{c-1}(x) = 1\\), for each \\(x\\in R^{d}\\), so that one of the functions is redundant. In the two-class case, this is made explicit by using a single \\(\\eta(x)\\), but using the redundant set above proves advantageous in the multiple-class case, as seen below.\nHint: you should answer the following items in sequence, using the previous answers in the solution of the following ones\n\n\n6.5.1 (a)\n\nGiven a classifier \\(\\psi: R^d \\rightarrow \\{0,1,\\dots, c-1\\}\\), show that its conditional error \\(P(\\psi(X)\\neq Y|X=x)\\) is given by\n\\[P(\\psi(X)\\neq Y|X=x) = 1-\\sum^{c-1}_{i=1} I_{\\psi(x)=i}\\eta_{i}(x) = 1 - \\eta_{\\psi(x)}(x) \\tag{6.2}\\]\n\nUse the “Law of Total Probability” (Braga-Neto 2020, sec. A.53),\n\\[P(\\psi(X) =  Y| X=x) + P(\\psi(X)\\neq Y| X=x) = 1 \\tag{6.3}\\]\n\\(\\therefore\\) We can derive the probability of error via\n\\[\\begin{align}\n    P(\\psi(X)\\neq Y| X=x) &= 1 - P(\\psi(X) =  Y| X=x)\\\\\n    &= 1 - \\sum_{i=0}^{c-1}P(\\psi(x)=i, Y=i | X=x)\\\\\n    &= 1 - \\sum_{i=0}^{c-1}I_{\\psi(x)=i}P(Y=i | X=x)\\\\\n    &= 1 - \\sum_{i=0}^{c-1}I_{\\psi(x)=i}\\eta_i(x)\n\\end{align}\\]\nCombining together, Equation 6.3 implies Equation 6.2.\n\n\n6.5.2 (b)\n\nAssuming that \\(X\\) has a density, show that the classification error of \\(\\psi\\) is given by\n\\[\\epsilon = 1 - \\sum_{i=0}^{c-1} \\int_{\\{x|\\psi(x)=i\\}}\\eta_i(x)p(x)dx.\\]\n\nLet \\(\\{x|\\psi(x) = i\\}\\) be the set of \\(\\psi(x) = i\\) in \\(X\\).\nUse the multiplication rule (Braga-Neto 2020, sec. A1.3)\n\\[\\begin{align}\n\\epsilon &= E[\\epsilon[\\psi(x) | X=x]]\\\\\n&= 1 - \\int_{R^d} P(\\psi(X) = Y| X=x) p(x) dx\\\\\n&= 1 - \\sum_{i=0}^{c-1} \\int_{R^d} p(\\psi(X)=i, Y=i| X=x) p(x) dx\\\\\n&= 1 - \\sum_{i=0}^{c-1} \\int_{R^d} \\underbrace{p(\\psi(X)=i| X=x)}_{=1 \\text{ if } \\{x|\\psi(x) = i\\}; 0, \\text{ otherwise.}}p(Y=i|X=x) p(x) dx\\\\\n& = 1 - \\sum_{i=0}^{c-1} \\int_{\\{x|\\psi(x) = i\\}} 1\\cdot p(Y=i|X=x) p(x) dx\\\\\n& = 1 - \\sum_{i=0}^{c-1} \\int_{\\{x|\\psi(x) = i\\}} p(Y=i|X=x) p(x) dx\n\\end{align}\\]\n\n\n6.5.3 (c)\n\nProve that the Bayes classifier is given by\n\\[\\psi^{*}(x) = \\arg\\max_{i=0,1,\\dots, c-1} \\eta_{i}(x), \\quad x\\in R^d \\tag{6.4}\\]\nHint: Start by considering the difference between conditional expected errors \\(P(\\psi(X)\\neq Y|X=x) - P(\\psi^{*}(X)\\neq Y|X=x)\\).\n\nAccording to Braga-Neto (2020, 20), a Bayes classifier (\\(\\psi^{*}\\)) is defined as\n\\[\\psi^{*} = \\arg\\min_{\\psi\\in \\mathcal{C}} P(\\psi(X) \\neq Y)\\]\nover the set \\(\\mathcal{C}\\) of all classifiers. We need to show that the error of any \\(\\psi\\in \\mathcal{C}\\) has the conditional error rate:\n\\[\\epsilon[\\psi | X=x] \\geq \\epsilon[\\psi^{*} | X=x], \\quad \\text{ for all } x\\in R^d \\tag{6.5}\\]\nFrom Equation 6.2, classifiers have the error rates:\n\\[\\begin{align}\nP(\\psi^{*}(X)\\neq |X=x) &= 1 - \\sum_{i=1}^{c-1} I_{\\psi^{*}(x)=i}\\eta_i(x)\\\\\nP(\\psi(X)\\neq |X=x) &= 1 - \\sum_{i=1}^{c-1} I_{\\psi(x)=i}\\eta_i(x)\n\\end{align}\\]\nTherefore,\n\\[\\begin{align}\n    P(\\psi(X)\\neq Y|X=x) - P(\\psi^{*}(X)\\neq Y|X=x) &= (1 - \\sum_{i=1}^{c-1} I_{\\psi(x)=i}\\eta_i(x)) - (1 - \\sum_{i=1}^{c-1} I_{\\psi^{*}(x)=i}\\eta_i(x))\\\\\n    &= \\sum_{i=1}^{c-1} (I_{\\psi^{*}(x)=i} - I_{\\psi(x)=i})\\eta_i(x)\n\\end{align}\\]\n\\(\\because\\)\n\n\\(I_{\\psi^{*}(x)=i^{*}} = 1\\) when \\(i^{*}\\) satisfies \\(\\eta_{i^{*}}(x) = \\max_{i=0,1,\\dots,c-1}\\eta(x) = \\eta_{\\max}(x)\\)\n\\(I_{\\psi(x)=i'}=1\\) when \\(\\psi(x) = i'\\) for \\(i'\\in 0,1,\\dots, c-1\\)\n\n\\(\\therefore\\)\nif \\(i^* \\neq i'\\)\n\\[\\begin{align}\n     P(\\psi(X)\\neq Y|X=x) - P(\\psi^{*}(X)\\neq Y|X=x) &= (1-0)\\eta_{i^{*}}(x) + (0-1)\\eta_{i'}(x)\\\\\n     &= \\eta_{i^{*}}(x) - \\eta_{i'}(x)\\\\\n     &= \\eta_{\\max}(x) - \\eta_{i'}(x)\\\\\n     &\\geq 0\n\\end{align}\\]\nif \\(i^* = i'\\)\n\\[P(\\psi(X)\\neq Y|X=x) - P(\\psi^{*}(X)\\neq Y|X=x) = \\eta_{i^{*}}(x) - \\eta_{i'}(x)=0\\]\nTherefore, there is no classifier \\(\\psi\\in \\mathcal{C}\\) can have conditional error rate lower than Bayes classifier Equation 6.4.\n\n\n6.5.4 (d)\n\nShow that the Bayes error is given by\n\\[\\epsilon^{*} = 1 - E[\\max_{i=0,1,\\dots, c-1} \\eta_{i}(X)]\\]\n\nFrom Problem 2.4.b,\n\nNoted that, \\(\\{x|\\psi^{*}(x)=i\\} = \\emptyset \\text{ if } i\\neq i^{*}\\)\n\n\\[\\begin{align}\n    \\epsilon[\\psi^{*}] &= E[\\epsilon[\\psi^{*}(x)|X=x]]\\\\\n    &= 1 - \\sum_{i=0}^{c-1} \\int_{\\{x|\\psi^{*}(x)=i\\}}\\eta_i(x)p(x)dx\\\\\n    &= 1 - \\int_{\\{x|\\psi^{*}(x)=i^*\\}}\\eta_{\\max}(x)p(x)dx\\\\\n    &= 1 - E[\\eta_{\\max}(x)]\n\\end{align}\\]\n\n\n6.5.5 (e)\n\nShow that the maximum Bayes error possible is \\(1-\\frac{1}{c}\\).\n\n\\[\\max \\epsilon[\\psi^{*}] = 1 - \\min E[\\max_{i=0,1,\\dots,c-1} \\eta_i(X)] \\tag{6.6}\\]\nalso,\ngiven\n\\[\\eta_1(x) = \\eta_2(x) = \\dots = \\eta_{c-1}(x)\\]\n\\[\\sum_{i=1}^{c-1} \\eta_i(x) = 1\\]\nwe can get that \\[\\min\\max\\eta(X) = \\frac{1}{c} \\tag{6.7}\\]\nCombining Equation 6.6 and Equation 6.7 together, the maximum Bayes error is \\(1-\\frac{1}{c}\\)"
  },
  {
    "objectID": "hw/hw1.html#problem-2.7",
    "href": "hw/hw1.html#problem-2.7",
    "title": "6  Homework 1",
    "section": "6.6 Problem 2.7",
    "text": "6.6 Problem 2.7\n\nConsider the following univariate Gaussian class-conditional densities:\n\\[p(x|Y=0) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{(x-3)^2}{2}) \\tag{6.8}\\] \\[p(x|Y=1)=\\frac{1}{3\\sqrt{2\\pi}}\\exp(-\\frac{(x-4)^2}{18}) \\tag{6.9}\\] Assume that the classes are equally likely, i.e., \\(P(Y=0)=P(Y=1)=\\frac{1}{2}\\)\n\nThe PDF of Guasssian distribution is1\n\\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp(- \\frac{(x-\\mu)^2}{2\\sigma^2})\\]\n\n\n\n\n\n\nTable 6.1:  Parameters of Gaussian PDFs. \n  \n    \n      \n      Parameters\n      Values\n    \n  \n  \n    \n      0\n      $$\\mu_0$$\n      3\n    \n    \n      1\n      $$\\mu_1$$\n      4\n    \n    \n      2\n      $$\\sigma_0$$\n      1\n    \n    \n      3\n      $$\\sigma_1$$\n      3\n    \n  \n\n\n\n\n\n\n6.6.1 (a)\n\nDraw the densities and determine the Bayes classifier graphically.\n\n\nThe plot is dispayed in Figure 6.1.\nThe decision bounding was determined by the right intersection of both distributions. I applied Brent’s method to find the intersection2.\n\nIntuitively, the intersection on the right has the minimum \\(\\epsilon^0\\) to the right and \\(\\epsilon^1\\) to the left.\n\n\n\nclass Gauss:\n    def __init__(self, scale, mean, var):\n        self.scale = scale\n        self.mean = mean \n        self.var = var\n    def pdf(self, x):\n        return 1/(self.scale*np.sqrt(2*np.pi))*np.exp(-1*(x-self.mean)**2/self.var)\n    def plot(self, ax, x_bound=[-5,13], nticks=200, **args):\n        xs = np.linspace(x_bound[0], x_bound[1], nticks)\n        ps = [self.pdf(x) for x in xs]\n        ax.plot(xs, ps, **args)\n\n\ng0 = Gauss(1,3,2)\ng1 = Gauss(3,4,18)\n\n## Boundaries\ndec_x = [1.26, 4.49] # see problem 2.7 (b) for derivation\n\n## Plotting\nfig, ax = plt.subplots()\ng0.plot(ax, color=\"black\",label=\"$p(x|Y=0) = \\\\frac{1}{\\\\sqrt{2\\\\pi}}\\\\exp(-\\\\frac{(x-3)^2}{2})$\")\ng1.plot(ax, color=\"gray\",linestyle=\"--\",label=\"$p(x|Y=1)=\\\\frac{1}{3\\\\sqrt{2\\\\pi}}\\exp(-\\\\frac{(x-4)^2}{18})$\")\nax.axvline(x=dec_x[0], label=\"Bayes decision boundary\")\nax.axvline(x=dec_x[1])\nax.set_xlabel(\"$x$\")\nax.set_ylabel(\"$PDF$\")\nax.annotate(\"$\\psi^{*}(x)=1$\", (7.5,0.2))\nax.annotate(\"$\\psi^{*}(x)=1$\", (-5,0.2))\nax.annotate(\"$\\psi^{*}(x)=0$\", (1.4,0.2))\nax.legend(loc=\"upper right\");\n\n\n\n\nFigure 6.1: Univariate gaussian densities.\n\n\n\n\n\n\n6.6.2 (b)\n\nDetermine the Bayes classifier.\n\nAccording to Braga-Neto (2020, 22), the Bayes classifier can be defined by\n\\[\\begin{equation}\n    \\psi^{*}(x) =\n    \\begin{cases}\n        1, & D^{*}(x) > k^*\\\\\n        0, & \\text{otherwise}\n    \\end{cases}\n\\end{equation}\\]\nwhere \\(D^*(x) = \\ln \\frac{p(x|Y=1)}{p(x|Y=0)}\\), \\(k^* = \\ln \\frac{P(Y=0)}{P(Y=1)}\\). Now, take Equation 6.8 and Equation 6.9 into the forumula.\n\\[k^{*} = \\ln \\frac{1}{1} = 0\\]\n\\[\\begin{align}\n    D^*(x) &= \\ln \\frac{p(x|Y=1)}{p(x|Y=0)}\\\\\n           &= -\\ln \\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{(x-3)^2}{2})}{\\frac{1}{3\\sqrt{2\\pi}}\\exp(-\\frac{(x-4)^2}{18})}\\\\\n           &= -\\ln \\left[ 3 \\cdot \\exp(-\\frac{(x-3)^2}{2} + \\frac{(x-4)^2}{18}) \\right]\\\\\n           &= -\\ln 3 +\\frac{(x-3)^2}{2} - \\frac{(x-4)^2}{18}\\\\\n           &= \\frac{4}{9}x^2 - \\frac{23}{9}x - (\\ln(3) + \\frac{65}{18})\n\\end{align}\\]\nThus, the Bayes classifier for distinguishing Equation 6.8 and Equation 6.9 is\n\\[\\begin{equation}\n    \\psi^{*}(x) =\n    \\begin{cases}\n        1, & \\frac{4}{9}x^2 - \\frac{23}{9}x - (\\ln(3) + \\frac{65}{18})> 0\\\\\n        0, & \\text{otherwise}\n    \\end{cases}\n\\end{equation}\\]\nwith the boundaries\n\\[\\begin{equation}\n    x =\n    \\begin{cases}\n        \\frac{1}{8}(23-3\\sqrt{1+16\\ln(3)}) &\\approx 1.26\\\\\n        \\frac{1}{8}(23+3\\sqrt{1+16\\ln(3)}) &\\approx 4.49\n    \\end{cases}\n\\end{equation}\\]\nNoted that there are two boundaries for \\(D^*(x) = 0\\) becasue \\(D^*(x)\\) is a second order equation of \\(x\\).\n\\[\\begin{align}\n    \\psi^{*}(x) &=\n    \\begin{cases}\n        1, & \\left[ x- (\\frac{1}{8}(23-3\\sqrt{1+16\\ln(3)}))\\right]\\left[ x-(\\frac{1}{8}(23+3\\sqrt{1+16\\ln(3)}))\\right]> 0\\\\\n        0, & \\text{otherwise}\n    \\end{cases}\\\\\n    &=\\begin{cases}\n        1, x < (\\frac{1}{8}(23-3\\sqrt{1+16\\ln(3)})) \\lor x>(\\frac{1}{8}(23+3\\sqrt{1+16\\ln(3)}))\\\\\n        0, \\text{ otherwise }\n    \\end{cases}\\\\\n    &\\approx \\begin{cases}\n        1, x < 1.26 \\lor x> 4.49\\\\\n        0, \\text{ otherwise }\n    \\end{cases}\n\\end{align}\\]\n\n\n6.6.3 (c)\n\nDetermine the specificity and sensitivity of the Bayes classifier.\nHint: use the standard Gaussian CDF \\(\\psi(x)\\)\n\nLet left and right boundaries be \\(\\frac{1}{8}(23-3\\sqrt{1+16\\ln(3)}) = b_1\\) and \\(\\frac{1}{8}(23+3\\sqrt{1+16\\ln(3)})= b_2\\),\n\n\nTable 6.2: The definition of sensitivity and specificity from Braga-Neto (2020, 18)\n\n\nSensitivity\nSpecificity\n\n\n\n\n\\(1-\\epsilon^1[\\psi]\\)\n\\(1-\\epsilon^0[\\psi]\\)\n\n\n\n\nThe standard normal CDF is3. Use the definition of \\(\\epsilon^{0}[\\psi]\\) and \\(\\epsilon^{1}[\\psi]\\) in Braga-Neto (2020, 18)\n\\[F(x) = p(X<x) = \\frac{1}{2}\\left[ 1 + erf(\\frac{x-\\mu}{\\sigma\\sqrt{2}})\\right]\\]\n\\[\\begin{align}\n    \\epsilon^{0}[\\psi^{*}(x)] &= P(\\psi(X) = 1 | Y=0)\\\\\n    &= \\int_{\\{x|\\psi(x)= 1\\}}p(x|Y=0)dx\\\\\n    &= \\int_{-\\infty}^{b_1} p(x|Y=0)dx + \\int_{b_2}^{\\infty} p(x|Y=0)dx\\\\\n    &= F_{X_0}(b_1) + 1 - F_{X_0}(b_2)\\\\\n    &= \\frac{1}{2}[1+erf(\\frac{b_1-\\mu_0}{\\sigma_0\\sqrt{2}})] + 1 - \\frac{1}{2}[1+erf(\\frac{b_2-\\mu_0}{\\sigma_0\\sqrt{2}})]\\\\\n    &= \\underline{1 + \\frac{1}{2}(erf(\\frac{b_1-\\mu_0}{\\sigma_0\\sqrt{2}}) - erf(\\frac{b_2-\\mu_0}{\\sigma_0\\sqrt{2}}))}\n\\end{align}\\]\n\\[\\begin{align}\n    \\epsilon^{1}[\\psi^{*}(x)] &= P(\\psi(X) = 0 | Y=1)\\\\\n    &= \\int_{\\{x|\\psi(x)= 0\\}}p(x|Y=1)dx\\\\\n    &= \\int_{b_1}^{b_2} p(x|Y=1)dx\\\\\n    &= F_{X_1}(b_2) - F_{X_1}(b1)\\\\\n    &= \\underline{\\frac{1}{2}\\left[ erf(\\frac{b_2 - \\mu_1}{\\sigma_1 \\sqrt{2}}) - erf(\\frac{b_1 - \\mu_1}{\\sigma_1 \\sqrt{2}})  \\right]}\\\\\n\\end{align}\\]\nThe exact values of error estimation are shown in Table 6.3 (epsilon_0 is \\(\\epsilon_0\\), and epsilon_1 \\(\\epsilon_1\\)). Specificity and sensitivity are calculated with their definitions shown in Table 6.2,\n\n# Calculation of error statistics\ndef nerf(b, mu, std):\n    return erf( (b - mu) / (std*np.sqrt(2)))\n\nb1 = (1/8)*(23-3*np.sqrt(1+16*np.log(3)))\nb2 = (1/8)*(23+3*np.sqrt(1+16*np.log(3)))\ne0 = 1 + 0.5*(nerf(b1,mu0, std0) - nerf(b2,mu0, std0))\ne1 = 0.5*(nerf(b2,mu1, std1) - nerf(b1, mu1, std1))\n\nsensi = 1 - e1\nspec = 1 - e0\nbayesError = 0.5*(e0+e1)\n\n\n\n\n\n\n\nTable 6.3:  Exact values of type 0 and type 1 error rates. \n  \n    \n      \n      Error statistics\n      Value\n    \n  \n  \n    \n      0\n      epsilon_0\n      0.108752\n    \n    \n      1\n      epsilon_1\n      0.384628\n    \n    \n      2\n      Sensitivity\n      0.615372\n    \n    \n      3\n      Specificity\n      0.891248\n    \n    \n      4\n      Overall Bayes error\n      0.246690\n    \n  \n\n\n\n\n\n\n\n6.6.4 (d)\n\nDetermine the overall Bayes error.\n\nUse the derivation in Braga-Neto (2020, 18),\n\\[\\begin{align}\n    \\epsilon[\\psi^{*}(X)] &= P(\\psi(X) \\neq Y)\\\\\n    &= P(\\psi(X)=1, Y=0)  + P(\\psi(X)=0, Y=1)\\\\\n    &= P(Y=0) P(\\psi(X)=1 | Y=0)  + P(Y=1)P(\\psi(X)=0 | Y=1)\\\\\n    &= P(Y=0)\\epsilon^0 + P(Y=1)\\epsilon^1\\\\\n    &= \\underline{\\frac{1}{2}(\\epsilon^0 + \\epsilon^1)}\n\\end{align}\\]\nThe excat Bayes error is displayed in Table 6.4.\n\n\n\n\n\n\nTable 6.4:  Exact values of Bayes error. \n  \n    \n      \n      Error statistics\n      Value\n    \n  \n  \n    \n      0\n      Overall Bayes error\n      0.24669"
  },
  {
    "objectID": "hw/hw1.html#problem-2.9",
    "href": "hw/hw1.html#problem-2.9",
    "title": "6  Homework 1",
    "section": "6.7 Problem 2.9",
    "text": "6.7 Problem 2.9\n\nObtain the optimal decision boundary in the Gaussian model with \\(P(Y = 0) = P(Y = 1)\\) and\nIn each case draw the optimal decision boundary, along with the class means and class conditional density contours, indicating the 0- and 1-decision regions.\n\nSince \\(\\Sigma_0 \\neq \\Sigma_1\\) happens in the following subproblems, these are heteroskedastic cases. As mentioned in Braga-Neto (2020, sec. 2.5.2). The Bayes classifier is\n\\[\\begin{equation}\n    \\psi^{*}_{Q}(x) =\n    \\begin{cases}\n        1, x^T A x + b^T x + c > 0\\\\\n        0, \\text{ otherwise }\n    \\end{cases}\n\\end{equation}\\]\nwhere\n\\[\\begin{align}\n    A &= \\frac{1}{2}(\\Sigma^{-1}_{0} - \\Sigma^{-1}_{1})\\\\\n    b &= \\Sigma^{-1}_{1} \\mu_1 - \\Sigma^{-1}_{0}\\mu_0\\\\\n    c &= \\frac{1}{2}(\\mu^{T}_{0}\\Sigma_{0}^{-1}\\mu_0 - \\mu_{1}^{T}\\Sigma^{-1}_{1}\\mu_1) + \\frac{1}{2}\\ln\\frac{\\det(\\Sigma_0)}{\\det(\\Sigma_1)} + \\ln \\frac{P(Y=1)}{P(Y=0)}\n\\end{align}\\]\nLet \\(x\\) be the vector of sample values,\n\\[\\begin{align}\n    x =\n    \\begin{bmatrix}\n        x_0\\\\\n        x_1\n    \\end{bmatrix}\n\\end{align}\\]\nThe following is the Python implementation with NumPy4 5.\n\ndef compose(U,D):\n    return U.transpose()@inv(D)@U\n\ndef mA(sigma0, sigma1):\n    sigma0_inv = inv(sigma0)\n    sigma1_inv = inv(sigma1)\n    return 0.5*(sigma0_inv - sigma1_inv)\n\ndef mb(sigma0, sigma1, mu0, mu1):\n    sigma0_inv = inv(sigma0)\n    sigma1_inv = inv(sigma1)\n    return sigma1_inv@mu1 - sigma0_inv@mu0\n\ndef mc(sigma0, sigma1, mu0, mu1, Py):\n    return  0.5*(compose(mu0, sigma0) - compose(mu1, sigma1)) +\\\n        0.5*np.log(det(sigma0)/det(sigma1)) +\\\n            np.log(Py/(1-Py))\n\ndef BayesBound(x, sigma0, sigma1, mu0, mu1, Py):\n    xax = x.transpose() @ mA(sigma0, sigma1)@x\n    bx = mb(sigma0, sigma1, mu0, mu1).transpose() @ x\n    c = mc(sigma0, sigma1, mu0, mu1, Py)\n    \n    return float(xax + bx + c)\n\nclass GaussianBayesClassifier:\n    def __init__(self, sigma0, sigma1, mu0, mu1, Py):\n        self.sigma0 = sigma0\n        self.sigma1 = sigma1\n        self.mu0 = mu0\n        self.mu1 = mu1\n        self.Py = Py\n\n        # Inferred Matrix\n        self.mA = mA(sigma0, sigma1)\n        self.mb = mb(sigma0, sigma1, mu0, mu1)\n        self.mc = mc(sigma0, sigma1, mu0, mu1, Py)\n\n    def BayesBound(self, x):\n        return BayesBound(x, self.sigma0, self.sigma1, self.mu0, self.mu1, self.Py)\n\n    def psi(self, x):\n        \"\"\"\n        Bayes classification\n        \"\"\"\n        pred = 0\n        if self.BayesBound(x) > 0:\n            pred = 1\n        return pred\n    def solveD(self):\n        x0, x1 = sp.symbols('x0 x1')\n        x = sp.Matrix([[x0],[x1]])\n        return sp.simplify(x.T * bc.mA * x + bc.mb.T * x + bc.mc)\n    def plot2D(self, psi_annotates=[[0.3,0.3], [0.7,0.7]]):\n        fig, ax = plt.subplots()\n\n        # Create girds\n        xlist = np.linspace(-3.5,3.5,RES_GRID)\n        ylist = np.linspace(-3.5,3.5,RES_GRID)\n        X, Y = np.meshgrid(xlist, ylist)\n        pos = np.dstack((X,Y))\n        \n        # Compute Bayes classification\n        Z = np.zeros(X.shape)\n        for i in range(0, Z.shape[0]):\n            for j in range(0, Z.shape[1]):\n                x = np.matrix([X[i,j], Y[i,j]]).T \n                Z[i,j] = self.psi(x)\n        \n        # Compute Gaussia pdf\n        rv0 = multivariate_normal(np.array(self.mu0.T)[0], self.sigma0)\n        rv1 = multivariate_normal(np.array(self.mu1.T)[0], self.sigma1)\n        Z0 = rv0.pdf(pos)\n        Z1 = rv1.pdf(pos)\n\n        # Plot contours\n        cmap = plt.get_cmap('Set1', 2)\n        ax.contour(X,Y,Z, cmap=cmap, levels=[0])\n        ax.contour(X,Y,Z0, alpha=0.4)\n        ax.contour(X,Y,Z1, alpha=0.4)\n        ax.plot(self.mu0[0,0], self.mu0[1,0], marker=\"o\", color=\"k\",label=\"$\\mu_0$\")\n        ax.plot(self.mu1[0,0], self.mu1[1,0], marker=\"o\", color=\"gray\",label=\"$\\mu_1$\")\n        if psi_annotates==None:\n            psi_annotates = [[self.mu0[0,0], self.mu0[1,0]],[self.mu1[0,0], self.mu1[1,0]]]\n\n        # Annotate decisions\n        for ann in psi_annotates:\n            i = X[int(X.shape[0]*ann[0]), int(X.shape[1]*ann[1])]\n            j = Y[int(Y.shape[0]*ann[0]), int(Y.shape[1]*ann[1])]\n            x = np.matrix([i, j]).T \n            if self.psi(x) > 0:\n                ax.annotate(\"$\\psi^{*}(x) = 1$\", (i, j))\n            else:\n                ax.annotate(\"$\\psi^{*}(x) = 0$\", (i, j))\n        \n        # legend and label settings\n        ax.set_xlabel(\"$x_1$\")\n        ax.set_ylabel(\"$x_2$\")\n        ax.legend()\n        return fig, ax\n\n\n6.7.1 (a)\n\n\\[\\begin{equation*}\n\\mu_0 =(0,0)^T, \\mu_1=(2,0)^T, \\Sigma_0 =\n\\begin{pmatrix}\n2 & 0 \\\\\n0 & 1\n\\end{pmatrix},\n\\Sigma_1 =\n\\begin{pmatrix}\n2 & 0 \\\\\n0 & 4\n\\end{pmatrix}\n\\end{equation*}\\]\n\n\nbc = GaussianBayesClassifier(np.matrix([[2,0],[0,1]]),\\\n                             np.matrix([[2,0],[0,4]]),\\\n                             np.matrix([0,0]).T,\\\n                             np.matrix([2,0]).T, 0.5)\nbc.plot2D()\nprint(bc.solveD())\n\nUserWarning: No contour levels were found within the data range.\n  ax.contour(X,Y,Z, cmap=cmap, levels=[0])\n\n\nMatrix([[1.0*x0 + 0.375*x1**2 - 1.69314718055995]])\n\n\n\n\n\nFigure 6.2: Bayes Classifier for 2D Gussian (a)\n\n\n\n\n\nThe boundary is\n\n\\[D(x) = x_{0} + 0.375*x_{1}^{2} - 1.69 = 0\\]\n\n\n6.7.2 (b)\n\n\\[\\begin{equation*}\n\\mu_0 =(0,0)^T, \\mu_1=(2,0)^T, \\Sigma_0 =\n\\begin{pmatrix}\n2 & 0 \\\\\n0 & 1\n\\end{pmatrix},\n\\Sigma_1 =\n\\begin{pmatrix}\n4 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\end{equation*}\\]\n\n\nbc = GaussianBayesClassifier(np.matrix([[2,0],[0,1]]),\\\n                             np.matrix([[4,0],[0,1]]),\\\n                             np.matrix([0,0]).T,np.matrix([2,0]).T,\\\n                             0.5)\n\n\nfig, ax = bc.plot2D()\nax.annotate(\"$\\psi^{*}(x) = 1$\", (-7.5, -1))\nax.set_xlim(-8,3)\nax.axvline(x=-5.28216220230503, color=\"red\")\n\nprint(bc.solveD())\n\nUserWarning: No contour levels were found within the data range.\n  ax.contour(X,Y,Z, cmap=cmap, levels=[0])\n\n\nMatrix([[0.125*x0**2 + 0.5*x0 - 0.846573590279973]])\n\n\n\n\n\nFigure 6.3: Bayes Classifier for 2D Gussian (b)\n\n\n\n\n\nAnalytical solution to the boundary \\[D(x) = 0.125*x_{0}^{2} + 0.5*x_{0} - 0.85 = 0\\]\n\\(x=-5.28, 1.28\\) The solution are two vertical lines.\n\n\n\n6.7.3 (c)\n\n\\[\\begin{equation*}\n\\mu_0 =(0,0)^T, \\mu_1=(0,0)^T, \\Sigma_0 =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix},\n\\Sigma_1 =\n\\begin{pmatrix}\n2 & 0 \\\\\n0 & 2\n\\end{pmatrix}\n\\end{equation*}\\]\n\n\nbc = GaussianBayesClassifier(np.matrix([[1,0],[0,1]]),\\\n                             np.matrix([[2,0],[0,2]]),\\\n                             np.matrix([0,0]).T,\\\n                             np.matrix([0,0]).T,\\\n                             0.5)\nbc.plot2D(psi_annotates= [[0.4,0.3], [0.7,0.7]])\nprint(bc.solveD())\n\nUserWarning: No contour levels were found within the data range.\n  ax.contour(X,Y,Z, cmap=cmap, levels=[0])\n\n\nMatrix([[0.25*x0**2 + 0.25*x1**2 - 0.693147180559945]])\n\n\n\n\n\nFigure 6.4: Bayes Classifier for 2D Gussian (c)\n\n\n\n\n\nAnalytical solution to the boundary \\[D(x) =0.25*x_{0}^{2} + 0.25*x_1^{2} - 0.69 = 0\\]\n\nThe solution is a circle.\n\n\n6.7.4 (d)\n\n\\[\\begin{equation*}\n\\mu_0 =(0,0)^T, \\mu_1=(0,0)^T, \\Sigma_0 =\n\\begin{pmatrix}\n2 & 0 \\\\\n0 & 1\n\\end{pmatrix},\n\\Sigma_1 =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 2\n\\end{pmatrix}\n\\end{equation*}\\]\n\n\nbc = GaussianBayesClassifier(np.matrix([[2,0],[0,1]]),\\\n                             np.matrix([[1,0],[0,2]]),\\\n                             np.matrix([0,0]).T,\\\n                             np.matrix([0,0]).T,\\\n                             0.5)\nfig, ax = bc.plot2D(psi_annotates= [[0.3,0.4], [0.6,0.7]])\nax.annotate(\"$\\psi^{*}(x) = 1$\", (0, 2))\nax.annotate(\"$\\psi^{*}(x) = 0$\", (-3, 0))\nprint(bc.solveD())\n\nUserWarning: No contour levels were found within the data range.\n  ax.contour(X,Y,Z, cmap=cmap, levels=[0])\n\n\nMatrix([[-0.25*x0**2 + 0.25*x1**2]])\n\n\n\n\n\nFigure 6.5: Bayes Classifier for 2D Gussian (d)\n\n\n\n\n\nAnalytical solution to the boundary \\[D(x) = -0.25x_{0}^{2} + 0.25*x_{1}^{2} = 0\\]\nNoted that \\(D([0,0]^T) = 0\\), then \\(\\phi^{*}([0,0]^T)=0\\) for the origin."
  },
  {
    "objectID": "hw/hw1.html#python-assignment-problem-2.17",
    "href": "hw/hw1.html#python-assignment-problem-2.17",
    "title": "6  Homework 1",
    "section": "6.8 Python Assignment: Problem 2.17",
    "text": "6.8 Python Assignment: Problem 2.17\n\nThis problem concerns the Gaussian model for synthetic data generation in Braga-Neto (2020, sec. A8.1).\n\n\n6.8.1 (a)\n\nDerive a general expression for the Bayes error for the homoskedastic case with \\(\\mu_0 = (0,\\dots,0), \\mu_1=(1,\\dots,1)\\), and \\(P(Y=0)=P(Y=1)\\). Your answer should be in terms of \\(k, \\sigma^{2}_{1}, \\dots, \\sigma^{2}_{k}, l_1, \\dots, l_k,\\) and \\(\\rho_{1},\\dots, \\rho_k\\).\nHint: Use the fact that\n\\[\\begin{equation}\n\\begin{bmatrix}\n1 & \\sigma & \\cdots & \\sigma\\\\\n\\sigma & 1 & \\cdots & \\sigma\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\sigma & \\sigma & \\cdots & 1\n\\end{bmatrix}^{-1}_{l\\times l} = \\frac{1}{(1-\\sigma)(1+(l-1)\\sigma)}\n\\begin{bmatrix}\n1+(l-2)\\sigma & -\\sigma \\cdots & -\\sigma\\\\\n-\\sigma & 1+(l-2)\\sigma & \\cdots &-\\sigma\\\\\n\\vdots & \\vdots  & \\ddots & \\vdots\\\\\n-\\sigma & -\\sigma & \\cdots & 1+(l-2) & \\sigma\n\\end{bmatrix}\n\\end{equation}\\]\n\nUse Equation 2.53 in Braga-Neto (2020, 31). Given \\(P(Y=0)=P(Y=1)=0.5\\), then\n\\[\\begin{equation}\n    \\epsilon_{L}^{*} = \\Phi(- \\frac{\\delta}{2})\n\\end{equation}\\]\nwhere \\(\\delta = \\sqrt{(\\mu_1 - \\mu_0)^T \\Sigma^{-1}(\\mu_1 - \\mu0)}\\).\n\\[\\begin{equation}\n    \\Sigma_{d\\times d} =\n    \\begin{bmatrix}\n        \\Sigma_{l_1 \\times l_1}(\\sigma^{2}_{1}, \\rho_{1}) & 0 & \\cdots & 0\\\\\n        0 & \\Sigma_{l_2 \\times l_2}(\\sigma^{2}_{2}, \\rho_{2}) & \\cdots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\cdots & \\Sigma_{l_k \\times l_k}(\\sigma^{2}_{k}, \\rho_{k})\n    \\end{bmatrix}\n\\end{equation}\\]\nwhere \\(l_1 + \\cdots + l_k = d\\).\n\\[\\begin{align}\n    \\Sigma_{l_k \\times l_k}(\\sigma^{2}_{i}, \\rho_{i}) &= \\sigma^2_{i}\n    \\begin{bmatrix}\n        1 & \\rho_i & \\cdots & \\rho_i \\\\\n        \\rho_i & 1 & \\cdots & \\rho_i \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        \\rho_i & \\rho_i & \\cdots & 1\n    \\end{bmatrix}\\\\\n    %\n    %\n    \\Sigma^{-1}_{l_k \\times l_k}(\\sigma^{2}_{i}, \\rho_{i}) &= \\frac{1}{\\sigma_{i}^{2}(1-\\rho_i)(1+(l_k - 1)\\rho_i)}\n    \\begin{bmatrix}\n        1 + (l_k - 2)\\rho_i & -\\rho_i & \\cdots & -\\rho_i\\\\\n        -\\rho_i & 1 + (l_k - 2)\\rho_i &  \\cdots & -\\rho_i\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        -\\rho_i & -\\rho_i & \\cdots & 1 + (l_k - 2)\\rho_i\\\\\n    \\end{bmatrix}\\\\\n    &=\n    \\begin{bmatrix}\n        \\frac{1 + (l_k - 2)\\rho_i}{\\sigma_{i}^{2}(1-\\rho_i)(1+(l_k - 1)\\rho_i)}& \\frac{-\\rho_i}{\\sigma_{i}^{2}(1-\\rho_i)(1+(l_k - 1)\\rho_i)} & \\cdots & \\frac{-\\rho_i}{\\sigma_{i}^{2}(1-\\rho_i)(1+(l_k - 1)\\rho_i)}\\\\\n        \\frac{-\\rho_i}{\\sigma_{i}^{2}(1-\\rho_i)(1+(l_k - 1)\\rho_i)} &  \\frac{1 + (l_k - 2)\\rho_i}{\\sigma_{i}^{2}(1-\\rho_i)(1+(l_k - 1)\\rho_i)} & \\cdots & \\frac{-\\rho_i}{\\sigma_{i}^{2}(1-\\rho_i)(1+(l_k - 1)\\rho_i)}\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        \\frac{-\\rho_i}{\\sigma_{i}^{2}(1-\\rho_i)(1+(l_k - 1)\\rho_i)} & \\cdots & \\frac{-\\rho_i}{\\sigma_{i}^{2}(1-\\rho_i)(1+(l_k - 1)\\rho_i)} & \\frac{1 + (l_k - 2)\\rho_i}{\\sigma_{i}^{2}(1-\\rho_i)(1+(l_k - 1)\\rho_i)}\\label{eq:inv}\\\\\n    \\end{bmatrix}\n\\end{align}\\]\nThe element-wised summation of all terms in \\(\\Sigma_{l_k \\times l_k}(\\sigma^{2}_{i}, \\rho_{i})\\) is\n\\[\\begin{align}\n    sum(\\Sigma_{l_k \\times l_k}(\\sigma^{2}_{k}, \\rho_{k})) &= \\frac{1}{\\sigma_{i}^{2}(1-\\rho_k)(1+(l_k - 1)\\rho_k)}(- l_{k}^{2}\\rho_k + l_k(1+(l_k-1)\\rho_k))\\\\\n    & = \\frac{l_k(1-\\rho_k)}{\\sigma_{k}^{2}(1-\\rho_k)(1+(l_k - 1)\\rho_k)}\n\\end{align}\\]\nThus, the inverse of covariance matrix is\n\\[\\begin{equation}\n    \\Sigma^{-1}_{d\\times d} =\n    \\begin{bmatrix}\n        \\Sigma^{-1}_{l_1 \\times l_1}(\\sigma^{2}_{1}, \\rho_{1}) & 0 & \\cdots & 0\\\\\n        0 & \\Sigma^{-1}_{l_2 \\times l_2}(\\sigma^{2}_{2}, \\rho_{2}) & \\cdots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\cdots & \\Sigma^{-1}_{l_k \\times l_k}(\\sigma^{2}_{k}, \\rho_{k})\n    \\end{bmatrix}\n\\end{equation}\\]\nCombining together,\n\\[\\begin{align}\n    \\epsilon^{*}_{L} = \\Phi(\\frac{-1}{2}\\delta) &= \\Phi\\left(\\frac{-1}{2}\\sqrt{(\\mu_1 - \\mu_0)^T \\Sigma^{-1}(\\mu_1 - \\mu_0)}\\right)\\\\\n    &= \\Phi\\left(\\frac{-1}{2}\\sqrt{%\n        \\underbrace{%\n    \\begin{bmatrix}\n        1 & \\cdots & 1\n    \\end{bmatrix}\n    }_{1\\times d}%\n    \\underbrace{\n    \\begin{bmatrix}\n        \\Sigma^{-1}_{l_1 \\times l_1}(\\sigma^{2}_{1}, \\rho_{1}) & 0 & \\cdots & 0\\\\\n        0 & \\Sigma^{-1}_{l_2 \\times l_2}(\\sigma^{2}_{2}, \\rho_{2}) & \\cdots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\cdots & \\Sigma^{-1}_{l_k \\times l_k}(\\sigma^{2}_{k}, \\rho_{k})\n    \\end{bmatrix}\n    }_{d\\times d}%\n    \\underbrace{%\n    \\begin{bmatrix}\n            1 \\\\\n            \\vdots\\\\\n            1\n    \\end{bmatrix}\n    }_{d \\times 1}\n    }\\right)\\\\\n    &= \\Phi\\left( \\sum_{i=1}^{k}\\frac{l_i(1-\\rho_i)}{\\sigma_{i}^{2}(1-\\rho_i)(1+(l_i - 1)\\rho_i)} \\right)\n\\end{align}\\]\n\n\n6.8.2 (b)\n\nSpecialize the previous formula for equal-sized blocks \\(l_1 = \\cdots = l_k = l\\) with equal correlations \\(\\rho_1 = \\cdots = \\rho_k = \\rho\\), and constant variance \\(\\sigma_{1}^{2} = \\cdots, \\sigma_{k}^{2} = \\sigma^2\\). Write the resulting formula in terms of \\(d,l,\\sigma\\) and \\(\\rho\\).\n\n\n6.8.2.1 i.\n\nUsing the python function norm.cdf in the scipy.stats module, plot the Bayes error as a function of \\(\\sigma\\in [0.01,3 ]\\) for \\(d=20, l=4\\), and four different correlation values \\(\\rho=0,0.25,0.5,0.75\\) (plot one curve for each value). Confirm that the Bayes error increasese monotonically with \\(\\sigma\\) from \\(0\\) to \\(0.5\\) for each value of \\(\\rho\\), and that Bayes error for larger \\(\\rho\\) is uniformly larger than that for smaller \\(\\rho\\). The latter fact shows that correlation between the features is detrimental to classification.\n\nAs shown in Figure 6.6, the correlation does have the detrimental effect on classification, and monotoniously increases Bayes error.\n\n# Implementation\ndef get_VecM(length, val):\n    return np.ones((1, length)).T * val\n\nclass GeneralMultiGaussian:\n    def __init__(self, d, l, sig, rho, Imu0=0, Imu1=1):\n        self.d = d\n        self.l = l\n        self.sig = sig\n        self.rho = rho\n        self.Imu0 = Imu0\n        self.Imu1 = Imu1\n        self.mu0 = get_VecM(d, Imu0)\n        self.mu1 = get_VecM(d, Imu1)\n        # cluster of covariance matrix\n        self.subCovInv = self.cluster_cov_inv() \n        self.CovInv = self.cov_inv()\n\n    def cluster_cov_inv(self):\n        sig = self.sig\n        l = self.l\n        rho = self.rho\n        scale = 1/((sig**2)*(1-rho)*(1+(l-1)*rho))\n        diag_val = 1 + (l-2)*rho\n        covInv = np.ones((l,l)) * (-1*rho)\n        covInv = np.matrix(covInv)\n        np.fill_diagonal(covInv, diag_val)\n        return scale*covInv\n\n    def cov_inv(self):\n        d = self.d; l = self.l\n        subMs = [self.subCovInv for i in range(0,int(d/l)+1)]\n        return block_diag(*subMs)[0:d, 0:d]\n\n    def bayesError(self):\n        mu0 = self.mu0\n        mu1 = self.mu1\n        mud = mu1 - mu0\n        CovInv = self.CovInv\n        delta = -0.5 * np.sqrt(mud.T @ CovInv @ mud) \n        return norm.cdf(delta)\n\n\n# Parameter Setting\nsigs = np.linspace(0.01 ,3 , 50)\nd = 20\nl = 4\nrhos = [0, 0.25, 0.5, 0.75]\n\n# Measurement of Bayes errors\nerrorD = dict()\nfor rho in rhos:\n    errorD[rho] = np.zeros(len(sigs))\n\nfor rho in errorD.keys():\n    for (i, sig) in enumerate(sigs):\n        gm = GeneralMultiGaussian(d, l, sig, rho)\n        err = gm.bayesError()\n        errorD[rho][i] = err\n\n# Plotting\nfig, ax = plt.subplots()\nfor rho in rhos:\n    ax.plot(sigs, errorD[rho], label=\"$\\\\rho={}$\".format(rho))\nax.set_xlabel(\"$\\\\sigma$\")\nax.set_ylabel(\"Bayes Error\")\nax.legend();\n\n\n\n\nFigure 6.6: The relation of Bayes error with standard deviation and covariance.\n\n\n\n\n\n\n6.8.2.2 ii.\n\nPlot the Bayes error as a function of \\(d=2,4,6,8,\\dots, 40\\), with fixed block size \\(l=4\\) and variance \\(\\sigma^2=1\\) and \\(\\rho=0,0.25,0.5,0.75\\) (plot one curve for each value). Confirm that the Bayes error decreases monotonically to \\(0\\) with increasing dimensionality, with faster convergence for smaller correlation values.\n\nThe plot is shown in Figure 6.7.\n\n# Parameter setting\nds = np.arange(4, 40, 4, dtype=int)\nl = 4\nsig = 1\n\n# Measure errors\nerrorD2 = dict()\nfor rho in rhos:\n    errorD2[rho] = np.zeros(len(ds))\n\nfor rho in errorD2.keys():\n    for (i, d) in enumerate(ds):\n        gm = GeneralMultiGaussian(d, l, sig, rho)\n        err = gm.bayesError()\n        errorD2[rho][i] = err\n\n# Plotting\nfig, ax = plt.subplots()\nfor rho in rhos:\n    ax.plot(ds, errorD2[rho], label=\"$\\\\rho={}$\".format(rho))\nax.set_xlabel(\"$d$\")\nax.set_ylabel(\"Bayes Error\")\nax.legend();\n\n\n\n\nFigure 6.7: Bayes error as a function of dimension.\n\n\n\n\n\n\n6.8.2.3 iii.\n\nPlot the Bayes error as a function of the correlation \\(\\rho \\in [0,1]\\) for constant variance \\(\\sigma^2 =2\\) and fixed \\(d=20\\) with varying block size \\(l=1,2,4,10\\) (plot one curve for each value). Confirm that the Bayes error increases monotonically with increasing correlation. Notice that the rate of increase is particularly large near \\(\\rho=0\\), which shows that the Bayes error is very sensitive to correlation in the near-independent region.\n\nThe plot is show in Figure 6.8.\n\n# Parameter setting\nd = 20\nls = [1,2,4,10]\nsig = 2\nrhos = np.linspace(0.,0.99,40)\n\n# Measure errors\nerrorD3 = dict()\nfor l in ls:\n    errorD3[l] = np.zeros(len(rhos))\n\nfor l in ls:\n    for (i, rho) in enumerate(rhos):\n        gm = GeneralMultiGaussian(d, l, sig, rho)\n        err = gm.bayesError()\n        errorD3[l][i] = err\n\n# Plotting\nfig, ax = plt.subplots()\nfor l in ls:\n    ax.plot(rhos, errorD3[l], label=\"$\\\\l={}$\".format(l))\nax.set_xlabel(\"$\\\\rho$\")\nax.set_ylabel(\"Bayes Error\")\nax.legend();\n\n\n\n\nFigure 6.8: Bayes error as a function of correlation.\n\n\n\n\n\n\n\n\nBraga-Neto, Ulisses. 2020. Fundamentals of Pattern Recognition and Machine Learning. Springer."
  },
  {
    "objectID": "hw/hw2.html#homework-description",
    "href": "hw/hw2.html#homework-description",
    "title": "7  Homework 2",
    "section": "7.1 Homework Description",
    "text": "7.1 Homework Description\n\nCourse: ECEN649, Fall2022\n\n\nProblems from the book:\n3.6 (10 pt)\n4.2 (10 pt)\n4.3 (10 pt)\n4.4 (10 pt)\n4.8 (20 pt)\n\n\nDeadline: Oct. 12th, 11:59 am"
  },
  {
    "objectID": "hw/hw2.html#computational-enviromnent-setup",
    "href": "hw/hw2.html#computational-enviromnent-setup",
    "title": "7  Homework 2",
    "section": "7.2 Computational Enviromnent Setup",
    "text": "7.2 Computational Enviromnent Setup\n\n7.2.1 Third-party libraries\n\n%matplotlib inline\nimport sys # system information\nimport matplotlib # plotting\nimport scipy.stats as st # scientific computing\nimport pandas as pd # data managing\nimport numpy as np # numerical comuptation\nimport numba\nimport sklearn as sk\nfrom numpy import linalg as LA\nimport scipy as sp\nimport scipy.optimize as opt\nimport sympy as sp\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import inv, det\nfrom numpy.random import multivariate_normal as mvn\nfrom numpy.random import binomial as binom\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA #problem 4.8\nfrom sklearn.model_selection import train_test_split\n# Matplotlib setting\nplt.rcParams['text.usetex'] = True\nmatplotlib.rcParams['figure.dpi']= 300\nnp.random.seed(20221011)\n\n\n\n7.2.2 Version\n\nprint(sys.version)\nprint(matplotlib.__version__)\nprint(sp.__version__)\nprint(np.__version__)\nprint(pd.__version__)\nprint(sk.__version__)\n\n3.8.14 (default, Sep  6 2022, 23:26:50) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]\n3.3.1\n1.6.2\n1.19.1\n1.1.1\n1.1.2"
  },
  {
    "objectID": "hw/hw2.html#problem-3.6-python-assignment",
    "href": "hw/hw2.html#problem-3.6-python-assignment",
    "title": "7  Homework 2",
    "section": "7.3 Problem 3.6 (Python Assignment)",
    "text": "7.3 Problem 3.6 (Python Assignment)\n\nUsing the synthetic data model in Section A8.1 for the homoskedastic case with \\(\\mu_0 = (0,\\dots,0)\\), \\(\\mu_1=(1,\\dots,1)\\), \\(P(Y=0)=P(Y=1)\\), and \\(k=d\\) (independent features), generate a large number (e.g., \\(M=1000\\)) of training data sets for each sample size \\(n=20\\) to \\(n=100\\), in steps of \\(10\\), with \\(d=2,5,8\\), and \\(\\sigma=1\\). Obtain an approximation of the expected classification error \\(E[\\epsilon_n]\\) of the nearest centroid classifier in each case by averaging \\(\\epsilon_n\\), computed using the exact formula (3.13), over the \\(M\\) synthetic training data sets. Plot \\(E[\\epsilon_n]\\) as a function of the sample size, for \\(d=2,5,8\\) (join the individual points with lines to obtain a smooth curve). Explain what you see.\n\n\nThe formula in Braga-Neto (2020, 56, Eq. 3.13)\n\\(\\epsilon_n = \\frac{1}{2}\\left(\\Phi\\left(\\frac{a_{n}^{T}\\hat{\\mu}_0 + b_n}{\\|a_n\\|}\\right) + \\Phi\\left(-\\frac{a_{n}^{T}\\hat{\\mu}_1 + b_n}{\\|a_n\\|}\\right) \\right)\\)\n\n\\(\\mu_0 = (0,\\dots, 0)\\); \\(\\hat{\\mu}_0 = \\frac{1}{N_0}\\sum^{n}_{i=1}X_i I_{Y_i=0}\\)\n\\(\\mu_1 = (1,\\dots,1)\\); \\(\\hat{\\mu}_1 = \\frac{1}{N_1}\\sum^{n}_{i=1}X_i I_{Y_i=1}\\)\n\\(a_n = \\hat{\\mu}_1 - \\hat{\\mu}_0\\)\n\\(b_n = -\\frac{(\\hat{\\mu}_1 - \\hat{\\mu}_0)^{T}(\\hat{\\mu}_1 + \\hat{\\mu}_0)}{2}\\)1\n\nAs shown in Figure 7.1, the error rate converges to optimal error as the sample size increases.\n\n\ndef norml(v):\n    return LA.norm(v, 2)\n\ndef get_an(hm0,hm1):\n    return hm1 - hm0\n\ndef get_bn(hm0,hm1):\n    return -float((hm1 - hm0).T @ (hm1+hm0))/2\n\ndef epsilon(hmu0, hmu1, mu0, mu1,p0=0.5):\n    p1 = 1-p0\n    an = get_an(hmu0, hmu1)\n    bn = get_bn(hmu0, hmu1)\n    epsilon0 = st.norm.cdf( (float(an.T* mu0) + bn)/norml(an))\n    epsilon1 = st.norm.cdf(- (float(an.T*mu1)+ bn)/norml(an))\n    return p0*epsilon0 + p1*epsilon1\n\nclass GaussianDataGen:\n    def __init__(self, n, d, s=1, mu=0):\n        self.n = n\n        self.d = d\n        self.mu = np.matrix(np.ones(d) * mu).T\n        self.s = s\n        self.cov = self.get_cov()\n\n    def get_cov(self):\n        return np.identity(self.d) * self.s\n    \n    def sample(self):\n        data = np.random.normal(self.mu[0][0], self.s, size= (self.d, self.n))\n        hmuV = np.mean(data, axis=1)\n        return np.matrix(hmuV).T\n\ndef cal_eps(dg0, dg1, p0=0.5):\n    hmuV0 = dg0.sample()\n    hmuV1 = dg1.sample()\n    mu0 = np.matrix(np.zeros(dg0.d)).T\n    mu1 = np.matrix(np.ones(dg1.d)).T\n    return epsilon(hmuV0, hmuV1, mu0, mu1,p0=0.5)\ncal_eps_func = np.vectorize(cal_eps)\n\ndef exp_try_nd(n, d, s=1,M=1000):\n    gX0 = GaussianDataGen(n=n, d=d, s= s,mu=0)\n    gX1 = GaussianDataGen(n=n, d=d, s= s, mu=1)\n    #eps = cal_eps_func([gX0]*M, gX1)\n    eps = [cal_eps_func(gX0, gX1) for i in range(0,M)]\n    return np.mean(eps)\nexp_try_nd_func = np.vectorize(exp_try_nd)\n\ndef bayes_ncc(mu0, mu1):\n    return st.norm.cdf(- norml(mu1-mu0)/2)\n\nM = 1000\nns = np.arange(20,100, 10)\ns = 1\ndres = {2:[],5:[],8:[]}\n\nfor k in dres.keys():\n    dres[k]= exp_try_nd_func(ns,k)\n\n# Optimal error\nopts = [bayes_ncc(np.zeros(k), np.ones(k)) for k in dres.keys()]\n\nfig, ax = plt.subplots()\nsts = [\"-\", \"--\", \":\"]\nfor (i, k) in enumerate(dres.keys()):\n    ax.plot(ns, dres[k], 'o',label=\"d={}\".format(k))\n    ax.axhline(y= opts[i], label=\"Optimal d = {}\".format(k), color='k', linestyle=sts[i])\n\nax.set_xlabel(\"n\")\nax.set_ylabel(\"$E[\\\\epsilon_n]$\")\nax.legend();\n\n\n\n\nFigure 7.1: Error of En"
  },
  {
    "objectID": "hw/hw2.html#problem-4.2",
    "href": "hw/hw2.html#problem-4.2",
    "title": "7  Homework 2",
    "section": "7.4 Problem 4.2",
    "text": "7.4 Problem 4.2\n\nA common method to extend binary classification rules to \\(K\\) classes, \\(K>2\\), is the one-vs-one approach, in which \\(\\frac{K(K-1)}{2}\\) classifiers are trained between all pairs of classes2, and a majority vote of assigned labels is taken.\n\n\n7.4.1 (a)\n\nFormulate a multiclass version of parametric plug-in classification using the one-vs-one approach.\n\nLet \\(\\psi^{*}_{i,j}\\) be a one-one classifiers that \\(i\\neq j\\), and \\(\\{(i,j)| i\\in [1,k], j \\in [1,k], i\\neq j\\}\\). For \\(K\\) classes, there are \\(K(K-1)\\) classifiers; for each classifier \\(\\psi^{*}_{i,j}\\) and \\(x\\in R^d\\),\n\\[\\begin{equation}\n    \\psi^{*}_{ij,n} =\n    \\begin{cases}\n        1, & D_{ij, n}(x) > k_{ij,n}\\\\\n        0, & \\text{otherwise}\n    \\end{cases}\n\\end{equation}\\]\nwhere\n\n\\(D_{ij,n}(x) = \\ln \\frac{p(x|\\theta_{i,n})}{p(x|\\theta_{j,n})}\\)\n\\(k_{ij,n} = \\ln\\frac{P(Y=j)}{P(Y=i)}\\)\nNoted that feature-label distribution is expressed via a familty of PDF \\(\\{p(x|\\theta_i) | \\theta \\in \\Theta \\subseteq R^m\\}\\), for \\(i=1,\\dots,K\\).\n\\(\\psi^{*}_{ij,n} = 1\\otimes \\psi^{*}_{ji,n}\\). These two are similar classifiers with inverted outcome.\n\nLet \\(\\psi^{*}_{i,n} = \\sum_{j\\neq i} \\psi^{*}_{ij,n}\\), and the one-vs-one classifier is\n\\[\\psi^{*}_{n}(x) = \\arg\\max_{k=1,\\dots,K} \\psi^{*}_{k,n} \\tag{7.1}\\]\n\n\n7.4.2 (b)\n\nShow that if the threshold \\(k_{ij,n}\\) between classes \\(i\\) and \\(j\\) is given by \\(\\ln\\frac{\\hat{c}_j}{\\hat{c}_i}\\), then the one-vs-one parametric classification rule is equivalent to the simple decision. \\[\\psi_{n}(x) = \\arg\\max_{k=1,...,K} \\hat{c}_{k} p(x|\\theta_{k,n}), x\\in R^d\\] (For simplicity, you may ignore the possibility of ties.)\n\n\\[\\begin{align}\n    \\ln \\frac{ p(x|\\theta_{i,n})}{p(x|\\theta_{j,n})} &> k_{ij,n} = \\ln\\frac{\\hat{c_j}}{\\hat{c_i}}\\\\\n    \\ln p(x|\\theta_{i,n}) - \\ln p(x|\\theta_{j,n}) &> \\ln \\hat{c}_j - \\ln \\hat{c}_{i}\\\\\n    \\hat{c}_i p(x|\\theta_{i,n}) &> \\hat{c}_j p(x|\\theta_{j,n})\n\\end{align}\\]\n\\[\\begin{align}\n    \\psi^{*}_{ij, n}%\n     &= \\begin{cases}\n        1, & \\hat{c}_i p(x|\\theta_{i,n}) > \\hat{c}_j p(x|\\theta_{j,n})\\\\\n        0, & otherwise\n    \\end{cases}\\\\\n    &= I_{\\hat{c}_i p(x|\\theta_{i,n}) > \\hat{c}_j p(x|\\theta_{j,n})}\n\\end{align}\\]\nThen,\n\\[\\begin{align}\n    \\psi^{*}_{i,n}%\n    &=\\sum_{j\\neq i} \\psi^{*}_{ij,n}\\\\\n    &= \\sum_{j\\neq i}  I_{\\hat{c}_i p(x|\\theta_{i,n}) > \\hat{c}_j p(x|\\theta_{j,n})}\n\\end{align}\\]\n\\[\\begin{align}\n    \\psi^{*}_{n}(x)%\n    &= \\arg\\max_{k=1,\\dots,K}\\psi^{*}_{k,n}\\\\\n    &= \\arg\\max_{k=1,\\dots,K}\\sum_{j\\neq i}\\psi^{*}_{kj,n}\\\\\n    &= \\arg\\max_{k=1,\\dots,K} \\sum_{j\\neq i}  I_{\\hat{c}_k p(x|\\theta_{k,n}) > \\hat{c}_j p(x|\\theta_{j,n})}\\\\\n\\end{align}\\]\nLet \\(\\psi^{*}_{n}(x) =\\kappa\\), that means \\(\\psi^{*}_{\\kappa,n}\\) is the maximum among \\(\\{\\psi^{*}_{j,n}|j= (1,\\dots, K)\\}\\). Assume there is an \\(s\\neq \\kappa\\) s.t. \\(\\hat{c}_s p(x|\\theta_{s,n}) > \\hat{c}_\\kappa p(x|\\theta_{\\kappa,n}) >\\) the rests. Thus, \\(I_{\\hat{c}_s p(x|\\theta_{s,n}) > \\hat{c}_\\kappa p(x|\\theta_{\\kappa,n})} = 1\\)\n\\[\\psi^{*}_{s,n} = \\sum_{j\\neq s}  I_{\\hat{c}_s p(x|\\theta_{s,n}) > \\hat{c}_j p(x|\\theta_{j,n})} = \\underbrace{\\sum_{j\\neq s; j\\neq \\kappa}  I_{\\hat{c}_s p(x|\\theta_{s,n}) > \\hat{c}_j p(x|\\theta_{j,n})}}_{=a}+\\underbrace{I_{\\hat{c}_s p(x|\\theta_{s,n}) > \\hat{c}_\\kappa p(x|\\theta_{\\kappa,n})}}_{=1}\\]\n\\[\\psi^{*}_{\\kappa,n} = \\sum_{j\\neq \\kappa} I_{\\hat{c}_\\kappa p(x|\\theta_{\\kappa,n}) > \\hat{c}_j p(x|\\theta_{j,n})} = \\underbrace{\\sum_{j\\neq \\kappa; j\\neq s}I_{\\hat{c}_\\kappa p(x|\\theta_{\\kappa,n}) > \\hat{c}_j p(x|\\theta_{j,n})}}_{=a} + \\underbrace{I_{\\hat{c}_\\kappa p(x|\\theta_{\\kappa,n}) > \\hat{c}_s p(x|\\theta_{s,n})}}_{=0}\\]\nwhere \\(a\\) is a nonnegative number. That means\n\\[\\psi^{*}_{\\kappa,n} < \\psi^{*}_{s,n} \\tag{7.2}\\]\n\\(\\psi^{*}_{\\kappa,n}\\) is not the maximum. Equation 7.2 is contradict to the statement that \\(\\psi^{*}_{n}(x)=\\kappa\\). In conclusion, \\(\\hat{c}_k p(x|\\theta_{\\kappa}, n)\\) is the maximum if \\(\\psi_n(x) = k\\).\n\n\n7.4.3 (c)\n\nApplying the approach in items (a) and (b), formulate a multiclass version of Gaussian discriminant analysis. In the case of multiclass NMC, with all thresholds equal to zero, how does the decision boundary look like?\n\nFor Gaussian discriminant analyis, the discriminant is defined as\n\\[\\hat{D}^{*}_{ij}(x) = \\frac{1}{2}(x-\\hat{\\mu}_i)^{T}\\hat{\\Sigma}^{-1}_{i}(x-\\hat{\\mu}_i)-\\frac{1}{2}(x-\\hat{\\mu}_j)^{T}\\hat{\\Sigma}^{-1}_{j}(x-\\hat{\\mu}_j) + \\frac{1}{2}\\ln\\frac{\\det(\\hat{\\Sigma}_i)}{\\det(\\hat{\\Sigma}_j)}\\]\n\\[\\begin{equation}\n    \\psi^{*}_{ij, n}(x) = I_{\\hat{D}^{*}_{ij}(x) > 0}\n\\end{equation}\\]\n\\[\\psi^{*}_{n} = \\arg\\max_{k=1,\\dots,K} \\hat{c}_k p(x|\\hat{\\mu}_{k,n}, \\hat{\\Sigma}_{k,n}) = \\arg\\max_{k=1,\\dots,K} Normal(x; \\hat{\\mu}_{k,n}, \\hat{\\Sigma}_{k,n})\\]\nFor NMC case, suppose there are 3 classes with \\(d=2\\).\n\n\\(K=3\\)\nNumber of classifiers: \\(\\frac{3\\cdot 2}{2}=3\\)\n\n\\(\\psi^{*}_{1,2}, \\psi^{*}_{1,3}, \\psi^{*}_{2,3}\\)\n\n\n\\[\\begin{equation}\n    boundary =\n    \\begin{cases}\n        \\hat{\\Sigma}^{-1}(\\hat{\\mu}_1 - \\hat{\\mu}_2)\\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix} +(\\hat{\\mu}_2 - \\hat{\\mu}_1)^T \\hat{\\Sigma}^{-1}\\left(\\frac{\\hat{\\mu}_1+\\hat{\\mu}_2}{2}\\right) = 0\\\\\n        \\hat{\\Sigma}^{-1}(\\hat{\\mu}_1 - \\hat{\\mu}_3)\\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix} +(\\hat{\\mu}_3 - \\hat{\\mu}_1)^T \\hat{\\Sigma}^{-1}\\left(\\frac{\\hat{\\mu}_1+\\hat{\\mu}_3}{2}\\right) = 0\\\\\n        \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_3)\\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix} +(\\hat{\\mu}_3 - \\hat{\\mu}_2)^T \\hat{\\Sigma}^{-1}\\left(\\frac{\\hat{\\mu}_2+\\hat{\\mu}_3}{2}\\right) = 0\\\\\n    \\end{cases}\n\\end{equation}\\]\n\n\n\nFigure 7.2: Homoskedastic cases"
  },
  {
    "objectID": "hw/hw2.html#problem-4.3",
    "href": "hw/hw2.html#problem-4.3",
    "title": "7  Homework 2",
    "section": "7.5 Problem 4.3",
    "text": "7.5 Problem 4.3\n\nUnder the general Gaussian model \\(p(x|Y=0)\\sim \\mathcal{N}_d(\\mu_0, \\sum_0)\\) and \\(p(x|Y=1)\\sim \\mathcal{N}_d(\\mu_1, \\sum_1)\\), the classification error \\(\\epsilon_n = P(\\psi_n(X)\\neq Y| S_n)\\) of any linear classifier in the form\n\\[\\begin{equation}\n   \\psi_{n}(x) =\n   \\begin{cases}\n       1,& a_{n}^{T}x + b_n > 0,\\\\\n       0,& \\text{otherwise}\n   \\end{cases}\n\\end{equation}\\]\n(examples discussed so far include LDA and its variants, and the logistic classifier) can be readily computed in terms of \\(\\Phi\\) (the CDF of a standard normal random variable), the classifier parameters \\(a_n\\) and \\(b_n\\), and the distributional parameters \\(c=P(Y=1)\\), \\(\\mu_0\\), \\(\\mu_1\\), \\(\\Sigma_0\\), and \\(\\Sigma_1\\).\n\n\n7.5.1 (a)\n\nShow that\n\\[\\epsilon_n = (1-c)\\Phi\\left( \\frac{a_{n}^{T}\\mu_0 + b_n}{\\sqrt{a_{n}^{T}\\Sigma_0 a_n}} \\right) + c \\Phi\\left( -\\frac{a^{T}_{n}\\mu_1 + b_n}{\\sqrt{a_{n}^{T}\\Sigma_1 a_n}}\\right)\\]\nHint: the discriminant \\(a^{T}_{n}x+b_n\\) has a simple Gaussian distribution in each class.\n\nFrom Braga-Neto (2020, Eq. 2.34),\n\\[\\epsilon^{*} = \\underbrace{P(Y=0)}_{=1-c}\\epsilon^{0}[\\psi^*] + \\underbrace{P(Y=1)}_{=c}\\epsilon^{1}[\\psi^*]\\]\n\\[\\begin{align}\n\\epsilon^{0}[\\psi^{*}]%\n&= P(a_{n}^{T}x+b_n > 0 | Y=0)\\\\\n\\end{align}\\]\nUse the affine property of Gaussian distribution described in Braga-Neto (2020) [pp. 307. G4]3.\n\n\\(a^{T}_{n}x+b_n | Y=0 \\sim N(a^{T}_{n}\\mu_0+b_n, \\underbrace{a^{T}_{n}\\Sigma_{0}a_{n}}_{\\sigma^2})\\)\n\n\\[\\begin{align}\n\\epsilon^{0}[\\psi^{*}]%\n&= 1 - P(a^{T}_{n}x+b_n \\leq 0 | Y=0)\\\\\n&= 1 - \\Phi\\left(\\frac{0 - (a^{T}_{n}\\mu_0 + b_n)}{\\sqrt{a^{T}_{n}\\Sigma_{0}a_{n}}}\\right)\\\\\n&= 1 - \\Phi\\left(-\\frac{a^{T}_{n}\\mu_0 + b_n}{\\sqrt{a^{T}_{n}\\Sigma_{0}a_{n}}}\\right)\\\\\n&= \\Phi\\left(\\frac{a^{T}_{n}\\mu_0 + b_n}{\\sqrt{a^{T}_{n}\\Sigma_{0}a_{n}}}\\right)\n\\end{align}\\]\nSimilarly,\n\\[\\begin{align}\n    \\epsilon^{1}(\\psi^{*})%\n    &= P(a_{n}^{T}x+b_n < 0 | Y=1)\\\\\n    &= \\Phi\\left(\\frac{0 - (a^{T}_{n}\\mu_1 + b_n)}{\\sqrt{a^{T}_{n}\\Sigma_1 a_n}}\\right)\\\\\n    &= \\Phi\\left(-\\frac{a^{T}_{n}\\mu_1 + b_n}{\\sqrt{a^{T}_{n}\\Sigma_1 a_n}}\\right)\n\\end{align}\\]\nCombining together,\n\\[\\begin{align}\n    \\epsilon^{*}%\n     &= \\underbrace{P(Y=0)}_{=1-c}\\epsilon^{0}[\\psi^*] + \\underbrace{P(Y=1)}_{=c}\\epsilon^{1}[\\psi^*]\\\\\n     &= (1-c)\\epsilon^{0}[\\psi^*] + c\\epsilon^{1}[\\psi^*]\\\\\n     &= (1-c)\\Phi\\left(\\frac{a^{T}_{n}\\mu_0 + b_n}{\\sqrt{a^{T}_{n}\\Sigma_{0}a_{n}}}\\right) + c\\Phi\\left(-\\frac{a^{T}_{n}\\mu_1 + b_n}{\\sqrt{a^{T}_{n}\\Sigma_1 a_n}}\\right)\n\\end{align}\\]\n\n\n7.5.2 (b)\n\nCompute the errors of the NMC, LDA, and DLDA classifiers in Example 4.2 if \\(c=1/2\\), \\[\\begin{equation*}\n\\mu_0 =\n\\begin{bmatrix}\n2\\\\\n3\n\\end{bmatrix},\n\\mu_1 =\n\\begin{bmatrix}\n6\\\\\n5\n\\end{bmatrix},\n\\Sigma_0 =\n\\begin{bmatrix}\n1 & 1\\\\\n1 & 2\n\\end{bmatrix},\n\\text{ and }\n\\Sigma_1 =\n\\begin{bmatrix}\n4 & 0\\\\\n0 & 1\n\\end{bmatrix}\n\\end{equation*}\\] Which classifier does the best?\n\nAs shown in Table 7.1, NMC has the lowest Bayes error.\n\ndef epsilon_general(an, bn, c, mu0, mu1, sig0, sig1):\n\n    e0 = st.norm.cdf(\\\n        float(an.T @ mu0 + bn)/\\\n        np.sqrt(float(an.T @ sig0 @ an)))\n    e1 = st.norm.cdf(\\\n        -float(an.T @ mu1 + bn)/\\\n        np.sqrt(float(an.T @ sig1 @an)) )\n\n    return (1-c)*e0 + c*e1\n\ntruth = {\n    \"c\": 0.5,\n    \"mu0\": np.matrix([[2],[3]]),\n    \"mu1\": np.matrix([[6],[5]]),\n    \"sig0\": np.matrix([[1,1],[1,2]]),\n    \"sig1\": np.matrix([[4,0],[0,1]]),\n}\n\nmeth = {\n    \"NMC\":{\n        \"an\": np.matrix([[4],[2]]),\n        \"bn\": -24\n    },\n    \"LDA\":{\n        \"an\": 3/7*np.matrix([[5], [3]]),\n        \"bn\": -96/7\n    },\n    \"DLDA\":{\n        \"an\": 2/5*np.matrix([[6],[5]]),\n        \"bn\": -88/5\n    }\n}\n\nberrors = np.zeros(len(meth.keys()))\n\nfor (i,k) in enumerate(meth.keys()):\n    berrors[i] = epsilon_general(**meth[k], **truth)\n\npd.DataFrame({\"Method\": list(meth.keys()),\\\n    \"Bayes Error\": berrors}).sort_values(\\\n    [\"Bayes Error\"], ascending=[1])\n\n\n\n\n\nTable 7.1:  Bayes Errors of NMC, LDA and DLDA \n  \n    \n      \n      Method\n      Bayes Error\n    \n  \n  \n    \n      0\n      NMC\n      0.084775\n    \n    \n      1\n      LDA\n      0.085298\n    \n    \n      2\n      DLDA\n      0.087606"
  },
  {
    "objectID": "hw/hw2.html#problem-4.4",
    "href": "hw/hw2.html#problem-4.4",
    "title": "7  Homework 2",
    "section": "7.6 Problem 4.4",
    "text": "7.6 Problem 4.4\n\nEven in the Gaussian case, the classification error of quadratic classifiers in general require numerical integration for its computation. In some special simple cases, however, it is possible to obtain exact solutions. Assume a two-dimensional Gaussian problem with \\(P(Y=1)=\\frac{1}{2}\\), \\(\\mu_0=\\mu_1 = 0\\), \\(\\Sigma_0=\\sigma_{0}^{2}I_2\\), and \\(\\Sigma_1 = \\sigma^{2}_{1}I_2\\). For definiteness, assume that \\(\\sigma_0 < \\sigma_1\\).\n\n\n7.6.1 (a)\n\nShow that the Bayes classifier is given by \\[\\begin{equation}\n\\psi^{*}(x) =\n\\begin{cases}\n1, &\\|x\\| > r^{*},\\\\\n0, &\\text{ otherwise },\n\\end{cases}\n\\quad \\text{ where } r^{*} = \\sqrt{2\\left(\\frac{1}{\\sigma_{0}^{2}} - \\frac{1}{\\sigma_{1}^{2}}\\right)^{-1}\\ln\\frac{\\sigma^{2}_{1}}{\\sigma^{2}_{0}}}\n\\end{equation}\\] In particular, the optimal decision boundary is a circle of radius \\(r^{*}\\).\n\nThe inverted \\(\\Sigma_1\\) and \\(\\Sigma_2\\) are4\n\\[\\begin{align}\n    \\Sigma_0 &= \\sigma_{0}^2 I_2 = \\begin{bmatrix}\n        \\sigma_{0}^2 & 0 \\\\\n        0 & \\sigma_{0}^2\n    \\end{bmatrix}\\\\\n    \\Sigma_{0}^{-1} &= \\frac{1}{\\sigma_{0}^{4}} \\begin{bmatrix}\n        \\sigma_{0}^2 & 0 \\\\\n        0 & \\sigma_{0}^2\n    \\end{bmatrix} = \\sigma_{0}^{-2}\\begin{bmatrix}\n            1 & 0\\\\\n            0 & 1\n        \\end{bmatrix} = \\sigma^{-2}_{0}I_2\\\\\n    \\Sigma^{-1}_{1} &= \\sigma^{-2}_{1}I_2\n\\end{align}\\]\nUse the derivation in Braga-Neto (2020, 74),\n\\[\\begin{equation}\n    A_n = \\begin{bmatrix}\n        a_{11} & a_{12}\\\\\n        a_{12} & a_{22}\n    \\end{bmatrix} = \\frac{-1}{2} \\Sigma_{1}^{-1} - \\Sigma_{0}^{-1} = \\frac{-1}{2}(\\sigma_{1}^{-2} - \\sigma_{0}^{-2}) \\begin{bmatrix}\n        1 & 0\\\\\n        0 & 1\n    \\end{bmatrix}\n\\end{equation}\\]\n\\[\\begin{align}\n    b_n &= \\begin{bmatrix}\n        b_{n,1}\\\\\n        b_{n,2}\n    \\end{bmatrix}\n    = \\Sigma_{1}^{-1}\\underbrace{\\mu_1}_{=0} - \\Sigma_{0}^{-1}\\underbrace{\\mu_{0}}_{=0}\\\\\n        &= \\begin{bmatrix}\n            0\\\\\n            0\n        \\end{bmatrix}\n\\end{align}\\]\n\\[c = -\\frac{1}{2}\\ln\\frac{|\\Sigma_1|}{|\\Sigma_0|} = \\frac{-1}{2}\\ln\\frac{\\sigma_{1}^{4}}{\\sigma_{0}^{4}} = -\\ln \\frac{\\sigma_{1}^2}{\\sigma_{0}^2}\\]\nAccording to Braga-Neto (2020, Eq. 4.26), the 2-dimensional QDA decision boundary is\n\\[\\begin{align}\n    D(x) = a_{11}x^{2}_1 + 2 a_{12}x_1x_2 + a_{22}x^{2}_{2} + b_1 x_1 + b_2 x_2 + c &= 0\\\\\n    a_{11}(x_{1}^{2} + x_{2}^{2}) &= \\ln \\frac{\\sigma_{1}^2}{\\sigma_{0}^2}\\\\\n    x^{2}_{1} + x^{2}_{2} &= 2(\\frac{1}{\\sigma^{2}_{0}} - \\frac{1}{\\sigma^{2}_{1}})^{-1}\\ln\\frac{\\sigma_{1}^2}{\\sigma_{0}^2}\\\\\n    r^{*} = \\sqrt{x^{2}_{1} + x^{2}_{2}} &= \\sqrt{2(\\frac{1}{\\sigma^{2}_{0}} - \\frac{1}{\\sigma^{2}_{1}})^{-1}\\ln\\frac{\\sigma_{1}^2}{\\sigma_{0}^2}}\n\\end{align}\\]\nNoted that \\(\\left(\\frac{1}{\\sigma^{2}_{0}} - \\frac{1}{\\sigma^{2}_{1}}\\right) > 0\\) because \\(\\sigma_0 < \\sigma_1\\)\nFor any point \\(\\|x_j\\| > r^{*}\\), the discriminant (\\(D\\)) is larger than \\(0\\), and \\(\\psi^{*}(x_j) = 1\\).\n\n\n7.6.2 (b)\n\nShow that the corresponding Bayes error is given by \\[\\epsilon^{*} = \\frac{1}{2} - \\frac{1}{2}(\\frac{\\sigma^{2}_{1}}{\\sigma^{2}_{0}} - 1)e^{-(1-\\frac{\\sigma^{2}_{0}}{\\sigma^{2}_{1}})^{-1}\\ln \\frac{\\sigma^{2}_{1}}{\\sigma^{2}_{0}}}\\] In particular, the Bayes error is a function only of the ratio of variances \\(\\frac{\\sigma^{2}_{1}}{\\sigma^{2}_{0}}\\), and \\(\\epsilon^{*}\\rightarrow 0\\) as \\(\\frac{\\sigma^{2}_{1}}{\\sigma^{2}_{0}} \\rightarrow \\infty\\).\nHint: use polar coordinates to solve the required integrals analytically.\n\nPart I: Definition of errors\n\\[\\begin{align}\n    \\epsilon^{0}[\\psi^{*}]\n    &= P(D^{*}(X)>k^{*}|Y=0)\\\\\n    &= P(\\|x\\|>r^{*} | Y=0)\n\\end{align}\\]\n\\[\\begin{align}\n    \\epsilon^{1}[\\psi^{*}]\n    &= P(D^{*}(X)\\leq k^{*}|Y=1)\\\\\n    &= P(\\|x\\|\\leq r^{*} | Y=1)\n\\end{align}\\]\nPart II: PDF of 2D Gaussian\n\\[\\begin{align}\np(x = \\begin{bmatrix}\n        x_1\\\\\n        x_2\n    \\end{bmatrix}\n    )%\n    &= \\frac{1}{\\sqrt{(2\\pi)^2 \\sigma^4}} \\exp(-\\frac{1}{2}x^T \\Sigma^{-1}x)\\\\\n    &= \\frac{1}{\\sqrt{(2\\pi)^2 \\sigma^4}} \\exp(-\\frac{1}{2}\\frac{x_{1}^{2}+x_{2}^{2}}{\\sigma^2})\\\\\n    &= \\frac{1}{2\\pi \\sigma^2}\\exp(-\\frac{x_{1}^{2} + x_{2}^{2}}{2\\sigma^2})\\\\\n\\end{align}\\]\nUse the polar coordination, \\(x_{1} = r\\cos\\theta\\) and \\(x_{2} = r\\sin\\theta\\). \\(x_{1}^{2} + x_{2}^{2} = r^2\\). We can transform 2D gaussian into polar coordination:\n\\[p(r,\\theta) = \\frac{1}{2\\pi \\sigma^2}\\exp(-\\frac{r^2}{2\\sigma^2})\\]\nPart III: Integration\n\\[\\begin{align}\n    \\epsilon^{0}[\\psi^{*}]%\n    &= \\int_{\\theta=0}^{\\theta=2\\pi}\\int_{r=r^{*}}^{\\infty}   \\frac{1}{2\\pi \\sigma_{0}^2}\\exp(-\\frac{r^2}{2\\sigma_{0}^2})rdrd\\theta\\\\\n    &= \\frac{1}{2\\pi \\sigma_{0}^2}\\int_{\\theta=0}^{\\theta=2\\pi}\\int_{r=r^{*}}^{\\infty}   \\exp(-\\frac{r^2}{2\\sigma_{0}^2})rdrd\\theta\\\\\n    &= \\frac{1}{2\\pi\\sigma_{0}^2}\\int_{\\theta=0}^{\\theta=2\\pi} \\sigma_{0}^{2}\\exp(-\\frac{r_{*}^{2}}{2\\sigma_{0}^{2}})d\\theta\\\\\n    &= \\exp(-\\frac{r_{*}^2}{2\\sigma_{0}^{2}})\\\\\n    &= \\exp\\left(- \\frac{1}{\\sigma_{0}^{2}(\\sigma_{0}^{-2} - \\sigma_{1}^{-2})} \\ln\\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\\\\\n    &= \\exp\\left(\\frac{-1}{(1-\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})}\\ln \\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\\\\\n    &= \\exp\\left(-(1-\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})^{-1}\\ln \\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\n\\end{align}\\]\n\\[\\begin{align}\n    \\epsilon^{1}[\\psi^*] &= \\int^{\\theta=2\\pi}_{\\theta=0}\\int_{r=0}^{r=r^*}  \\frac{1}{2\\pi \\sigma_{1}^2}\\exp(-\\frac{r^2}{2\\sigma_{1}^2})rdrd\\theta\\\\\n    &= 1 - \\exp(- \\frac{r_{*}^{2}}{2\\sigma_{1}^{2}})\\\\\n    &= 1 - \\exp\\left(- \\frac{1}{\\sigma_{1}^{2}(\\sigma_{0}^{-2} - \\sigma_{1}^{-2})} \\ln\\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\\\\\n    &= 1 - \\exp\\left(- \\frac{1}{(\\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}} - 1)} \\ln\\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\\\\\n    &= 1 - \\exp\\left(- \\frac{\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}}}{(1 - \\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})} \\ln\\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\\\\\n    &= 1 - \\exp\\left(- \\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}}(1 - \\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})^{-1} \\ln\\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\n\\end{align}\\]\nPart IV: Combining together\n\\[\\begin{align}\n    \\epsilon^{*}%\n    &= P(Y=0)\\epsilon^{0}[\\psi^{*}] + P(Y=1)\\epsilon^{1}[\\psi^{*}]\\\\\n    &= \\frac{1}{2}\\epsilon^{0} + \\frac{1}{2}\\epsilon^{1}\\\\\n    &= \\frac{1}{2} \\exp\\left(-(1-\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})^{-1}\\ln \\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right) + \\frac{1}{2} - \\frac{1}{2} \\exp\\left(- \\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}}(1 - \\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})^{-1} \\ln\\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\\\\\n    &= \\frac{1}{2} \\exp\\left(-(1-\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})^{-1}\\ln \\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right) + \\frac{1}{2}\\\\\n    &- \\frac{1}{2} \\exp\\left(- (\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}}-1)(1 - \\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})^{-1} \\ln\\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)  \\exp\\left(-(1-\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})^{-1}\\ln \\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\\\\\n    &= \\frac{1}{2} + \\frac{1}{2}\\left[ 1 - \\exp\\left(- \\underbrace{(\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}}-1)(1 - \\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})^{-1})}_{=-1} \\ln\\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right) \\right] \\exp\\left(-(1-\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})^{-1}\\ln \\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\\\\\n    &= \\frac{1}{2} + \\frac{1}{2}\\left[ 1 - \\exp\\left(\\ln\\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right) \\right] \\exp\\left(-(1-\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})^{-1}\\ln \\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\\\\\n    &= \\frac{1}{2} + \\frac{1}{2}\\left[ 1 - \\frac{\\sigma^{2}_{1}}{\\sigma^{2}_{0}} \\right] \\exp\\left(-(1-\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})^{-1}\\ln \\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\\\\\n    &= \\frac{1}{2} - \\frac{1}{2}\\left( \\frac{\\sigma^{2}_{1}}{\\sigma^{2}_{0}} -1 \\right) \\exp\\left(-(1-\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}})^{-1}\\ln \\frac{\\sigma_{1}^{2}}{\\sigma_{0}^{2}}\\right)\\\\\n\\end{align}\\]\n\n\n7.6.3 (c)\n\nCompare the optimal classifier to the QDA classifier in Braga-Neto (2020, Example 4.3). Compute the error of the QDA classifier and compare to the Bayes error. (Given \\(\\sigma_{0}^{2}=2\\) and \\(\\sigma_{1}^{2} = 8\\))5\n\nPart I: Optimal Error of Example 4.3\n\ndef berror_two(sig0, sig1):\n    assert sig1 > sig0 \n    rat = sig1/sig0\n    return 0.5 - 0.5*(rat-1)*np.exp(-((1-rat**-1)**-1)*np.log(rat))\n\n\npd.DataFrame({\"Optimal Error\": [berror_two(2, 8)]})\n\n\n\n\n\n  \n    \n      \n      Optimal Error\n    \n  \n  \n    \n      0\n      0.263765\n    \n  \n\n\n\n\nPart II: QDA Error\nUse the result in Problem 4.4 (b) and let \\(\\hat{r}\\) be the boundary of the QDA in Braga-Neto (2020, Example 4.3):\n\n\\(\\epsilon^0\\) is 6\n\n\\[\\begin{align}\n    \\epsilon^{0}%\n    &= \\int_{\\theta=0}^{\\theta=2\\pi}\\int_{r=\\hat{r}}^{\\infty}   \\frac{1}{2\\pi \\hat{\\sigma}_{0}^2}\\exp(-\\frac{r^2}{2\\hat{\\sigma}_{0}^2})rdrd\\theta\\\\\n    &= \\exp(-\\frac{\\hat{r}^{2}}{2\\hat{\\sigma}^{2}_{0}})\\\\\n    &= \\exp\\left(-\\frac{\\frac{32}{9}\\ln2}{2\\cdot \\frac{2}{3}}\\right)\\\\\n    &\\approx 0.157\n\\end{align}\\]\n\n\\(\\epsilon^1\\) is 7 \\[\\begin{align}\n  \\epsilon^{1}%\n  &= 1 - \\exp\\left(-\\frac{\\hat{r}^2}{2\\hat{\\sigma}^2_{1}}\\right)\\\\\n  &= 1 - \\exp\\left(- \\frac{\\frac{32}{9}\\ln2}{2\\cdot \\frac{8}{3}}\\right)\\\\\n  &\\approx 0.370\n\\end{align}\\]\n\nSince \\(k_n =0\\) is assumed, the error of LDA is8\n\\[\\begin{align}\n\\epsilon_{LDA}%\n&= \\frac{1}{2}(\\epsilon_{LDA}^{0} + \\epsilon_{LDA}^{1})\\\\\n&= \\frac{1}{2}(0.157 + 0.370)\\\\\n&= \\underline{0.264}\n\\end{align}\\]\nConclusion\nThe QDA error is larger than the optimal error."
  },
  {
    "objectID": "hw/hw2.html#problem-4.8-python-assignment",
    "href": "hw/hw2.html#problem-4.8-python-assignment",
    "title": "7  Homework 2",
    "section": "7.7 Problem 4.8 (Python Assignment)",
    "text": "7.7 Problem 4.8 (Python Assignment)\n\nApply linear discriminant analysis to the stacking fault energy (SFE) dataset (see Braga-Neto (2020, sec. A8.4)), already mentioned in Braga-Neto (2020, ch. 1). Categorize the SFE values into two classes, low (SFE \\(\\leq 35\\)) and high (SFE \\(\\geq 45\\)), excluding the middle values.\n\n\n7.7.1 (a)\n\nApply the preprocessing steps in c01_matex.py to obtain a data matrix of dimensions \\(123 (\\text{number of sample points}) \\times 7 (\\text{number of features})\\), as described in Braga-Neto (2020, sec. 1.8.2). Define low (SFE \\(\\leq 35\\)) and high (SFE \\(\\geq 45\\)) labels for the data. Pick the first \\(50\\%\\) of the sampe point s to be the training data and the remaining \\(50\\%\\) to be test data9.\n\n\n# Setting\ndef get_SFE_low(df):\n    df_ = df[df[\"SFE\"]<=35]\n    return df_.drop(\"SFE\", axis=1)\ndef get_SFE_high(df):\n    df_ = df[df[\"SFE\"]>=45]\n    return df_.drop(\"SFE\", axis=1)\n# Load data\nSFE_data = pd.read_table(\"data/Stacking_Fault_Energy_Dataset.txt\")\n# pre-process the data\nf_org = SFE_data.columns[:-1]                # original features\nn_org = SFE_data.shape[0]                    # original number of training points\np_org = np.sum(SFE_data.iloc[:,:-1]>0)/n_org # fraction of nonzero components for each feature\nf_drp = f_org[p_org<0.6]                         # features with less than 60% nonzero components\nSFE1  = SFE_data.drop(f_drp,axis=1)          # drop those features\ns_min = SFE1.min(axis=1)                     \nSFE2  = SFE1[s_min!=0]                       # drop sample points with any zero values  \nSFE   = SFE2[(SFE2.SFE<35)|(SFE2.SFE>45)]    # drop sample points with middle responses\nY = SFE > 40\ntrain, test, ytrain, ytest  = train_test_split(SFE, Y, test_size=0.5, shuffle=False)\n\n\nprint(SFE.shape)\nSFE.head(4)\n\n(123, 8)\n\n\n\n\n\n\nTable 7.2:  Filtered data \n  \n    \n      \n      C\n      N\n      Ni\n      Fe\n      Mn\n      Si\n      Cr\n      SFE\n    \n  \n  \n    \n      0\n      0.004\n      0.003\n      15.6\n      64.317\n      0.03\n      0.02\n      17.5\n      51.6\n    \n    \n      1\n      0.020\n      0.009\n      15.6\n      64.188\n      0.03\n      0.03\n      17.6\n      54.6\n    \n    \n      2\n      0.020\n      0.002\n      14.0\n      66.409\n      0.03\n      0.01\n      17.1\n      50.3\n    \n    \n      3\n      0.005\n      0.001\n      15.6\n      63.866\n      0.19\n      0.01\n      17.7\n      52.8\n    \n  \n\n\n\n\n\n\nprint(train.shape)\ntrain.head(4)\n\n(61, 8)\n\n\n\n\n\n\nTable 7.3:  Train data \n  \n    \n      \n      C\n      N\n      Ni\n      Fe\n      Mn\n      Si\n      Cr\n      SFE\n    \n  \n  \n    \n      0\n      0.004\n      0.003\n      15.6\n      64.317\n      0.03\n      0.02\n      17.5\n      51.6\n    \n    \n      1\n      0.020\n      0.009\n      15.6\n      64.188\n      0.03\n      0.03\n      17.6\n      54.6\n    \n    \n      2\n      0.020\n      0.002\n      14.0\n      66.409\n      0.03\n      0.01\n      17.1\n      50.3\n    \n    \n      3\n      0.005\n      0.001\n      15.6\n      63.866\n      0.19\n      0.01\n      17.7\n      52.8\n    \n  \n\n\n\n\n\n\nprint(test.shape)\ntest.head(4)\n\n(62, 8)\n\n\n\n\n\n\nTable 7.4:  Test data \n  \n    \n      \n      C\n      N\n      Ni\n      Fe\n      Mn\n      Si\n      Cr\n      SFE\n    \n  \n  \n    \n      295\n      0.07\n      0.40\n      16.13\n      54.818\n      9.64\n      0.45\n      18.48\n      65.0\n    \n    \n      296\n      0.07\n      0.54\n      16.13\n      54.678\n      9.64\n      0.45\n      18.48\n      53.0\n    \n    \n      297\n      0.04\n      0.04\n      9.00\n      70.920\n      1.20\n      0.40\n      18.20\n      30.4\n    \n    \n      298\n      0.04\n      0.04\n      9.00\n      70.920\n      1.20\n      0.40\n      18.20\n      25.7\n    \n  \n\n\n\n\n\n\n\n7.7.2 (b)\n\nUsing the function ttest_ind from the scipy.stats module, apply Welch’s two-sample t-test on the training data, and produce a table with the predictors, T statistic, and p-value, ordered with largest absolute T statistics at the top.\n\n\nFor calculate independent t-test, we need to use equal_var=False for two different sample sizes10.\n\n\ndf_low = get_SFE_low(train)\ndf_high = get_SFE_high(train)\n\nttest = st.ttest_ind(df_low, df_high, equal_var=False)\n\ntdf = pd.DataFrame({\n    \"Features\": df_low.keys(),\n    \"T-statistics\": ttest.statistic,\n    \"P-Values\": ttest.pvalue\n})\n\ntdf = tdf.sort_values([\"T-statistics\"], ascending=[0], key=abs)\ntdf \n\n\n\n\n\nTable 7.5:  Results of T-test analysis \n  \n    \n      \n      Features\n      T-statistics\n      P-Values\n    \n  \n  \n    \n      2\n      Ni\n      -10.020338\n      9.325882e-14\n    \n    \n      3\n      Fe\n      6.050509\n      1.327194e-07\n    \n    \n      0\n      C\n      1.912194\n      6.242173e-02\n    \n    \n      5\n      Si\n      1.092405\n      2.816877e-01\n    \n    \n      1\n      N\n      -0.889981\n      3.789871e-01\n    \n    \n      4\n      Mn\n      -0.198507\n      8.436217e-01\n    \n    \n      6\n      Cr\n      0.038242\n      9.696653e-01\n    \n  \n\n\n\n\n\n\n\n7.7.3 (c)\n\nPick the top two predictors and design an LDA classifier. (This is an example of filter feature selection, to be discussed in Chapter 9.). Plot the training data with the superimposed LDA decision boundary. Plot the testing data with the superimposed previously-obtained LDA decision boundary. Estimate the classification error rate on the training and test data. What do you observe?\n\nBoth train and test data has higher error rate compared to the result in Table 7.6. Because the train data (Figure 7.3, Figure 7.4) is inseparable with single boundary. This implies that we need extra informative feature to make this two classes separatable.\n\nfeatures = list(tdf[\"Features\"][0:2])\nvar1, var2 = features\n\n\ndef get_data_with_features(data, features):\n    X = data[features].values\n    Y = data.SFE > 40\n    return X, Y\n\ndef get_loss(X, Y, clf):\n    loss = sk.metrics.zero_one_loss(Y, clf.predict(X))\n    return loss\n\nX, Y = get_data_with_features(train, features)\nX_test, Y_test = get_data_with_features(test, features)\n\nclf = LDA(priors=(0.5,0.5))\nclf.fit(X,Y.astype(int))\na = clf.coef_[0]\nb = clf.intercept_[0];\n\n# Error\nerr_train = get_loss(X,Y, clf)\nerr_test = get_loss(X_test, Y_test, clf)\n\n\ndef plot_swe_predict(ax, X, Y, clf, title=\"\"):\n    ax.scatter(X[~Y,0],X[~Y,1],c='blue',s=32,label='Low SFE')\n    ax.scatter(X[Y,0],X[Y,1],c='orange',s=32,label='High SFE')\n    left,right = ax.get_xlim()\n    bottom,top = ax.get_ylim()\n    ax.plot([left,right],[-left*a[0]/a[1]-b/a[1],-right*a[0]/a[1]-b/a[1]],'k',linewidth=2)\n    ax.set_title(title)\n    ax.set_xlim(left,right)\n    ax.set_ylim(bottom,top)\n    ax.set_xlabel(var1)\n    ax.set_ylabel(var2)\n    ax.legend();\n\nfig43tr, ax43tr = plt.subplots()\nax43tr = plot_swe_predict(ax43tr, X, Y, clf, title=\"Train error {}\".format(err_train));\n\n\n\n\nFigure 7.3: Train data with LDA\n\n\n\n\n\nfig43t, ax43t = plt.subplots()\nax43t = plot_swe_predict(ax43t, X_test, Y_test, clf, title=\"Test error {}\".format(err_test))\n\n\n\n\nFigure 7.4: Test data with LDA\n\n\n\n\n\n\n7.7.4 (d)\n\nRepeat for the top three, and five predictors. Estimate the errors on the training and testing data (there is no need to plot the classifiers). What can you observe?\n\nAs shown in Table 7.6, the error rate does not decrease as more features get involved. This is because the sign of t-statistics are mixing together, and deviate the boundary to the optimal solution (see Table 7.5). The LDA approach may not appropriate for features with inversing T-statistics.\n\nn_features = [2,3,4,5]\nerr_trains = np.zeros(len(n_features))\nerr_tests = np.zeros(len(n_features))\nfor (j,i) in enumerate(n_features):\n    fs = list(tdf[\"Features\"][0:i])\n    X, Y = get_data_with_features(train, fs)\n    X_test, Y_test = get_data_with_features(test, fs)\n\n    clf = LDA(priors=(0.5,0.5))\n    clf.fit(X,Y.astype(int))\n\n    # Error\n    err_trains[j] = get_loss(X,Y, clf)\n    err_tests[j] = get_loss(X_test, Y_test, clf)\n\npd.DataFrame({\n    \"Number of Features\": n_features,\n    \"Training Error Rate\": err_trains,\n    \"Testing Error Rate\": err_tests\n})\n\n\n\n\n\nTable 7.6:  Surveying different number of features \n  \n    \n      \n      Number of Features\n      Training Error Rate\n      Testing Error Rate\n    \n  \n  \n    \n      0\n      2\n      0.114754\n      0.145161\n    \n    \n      1\n      3\n      0.081967\n      0.145161\n    \n    \n      2\n      4\n      0.081967\n      0.225806\n    \n    \n      3\n      5\n      0.081967\n      0.193548\n    \n  \n\n\n\n\n\n\n\n\n\nardianumam. 2017. “Understanding Multivariate Gaussian, Gaussian Properties and Gaussian Mixture Model.” Ardian Umam Blog.\n\n\nBraga-Neto, Ulisses. 2020. Fundamentals of Pattern Recognition and Machine Learning. Springer."
  },
  {
    "objectID": "hw/hw3.html#homework-description",
    "href": "hw/hw3.html#homework-description",
    "title": "8  Homework 3",
    "section": "8.1 Homework Description",
    "text": "8.1 Homework Description\n\n\nCourse: ECEN649, Fall2022\n\nProblems from the book:\n5.1\n5.2\n5.6 (a,b)\n5.10 (a,b,c)\nChallenge (not graded):\n5.4\n5.6 (c,d)\n\n\nDeadline: Oct. 26th, 11:59 pm"
  },
  {
    "objectID": "hw/hw3.html#computational-enviromnent-setup",
    "href": "hw/hw3.html#computational-enviromnent-setup",
    "title": "8  Homework 3",
    "section": "8.2 Computational Enviromnent Setup",
    "text": "8.2 Computational Enviromnent Setup\n\n8.2.1 Third-party libraries\n\n%matplotlib inline\nimport sys\nimport matplotlib\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport sklearn as sk\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal as mvn\nfrom sklearn.neighbors import KernelDensity as KD\nfrom matplotlib.colors import ListedColormap\n# Fix random state for reproducibility\nnp.random.seed(1978081)\n# Matplotlib setting\nplt.rcParams['text.usetex'] = True\nmatplotlib.rcParams['figure.dpi']= 300\n\n\n\n8.2.2 Version\n\nprint(sys.version)\nprint(matplotlib.__version__)\nprint(sp.__version__)\nprint(np.__version__)\nprint(pd.__version__)\nprint(sk.__version__)\n\n3.8.14 (default, Sep  6 2022, 23:26:50) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]\n3.3.1\n1.5.2\n1.19.1\n1.1.1\n1.1.2"
  },
  {
    "objectID": "hw/hw3.html#problem-5.1",
    "href": "hw/hw3.html#problem-5.1",
    "title": "8  Homework 3",
    "section": "8.3 Problem 5.1",
    "text": "8.3 Problem 5.1\n\nConsider that an experimenter wants to use A 2-D cubic histogram classification rule, with square cells with side length \\(h_n\\), and achieve consistency as the sample size \\(n\\) increases, for any possible distribution of the data. If the experimenter lets \\(h_n\\) decrease as \\(h_n = \\frac{1}{\\sqrt{n}}\\), would they be guaranteed to achieve consistency and why? If not, how would they need to modify the rate of decrease of \\(h_n\\) to achieve consistency?\n\nUse Braga-Neto (2020, Theorem 5.6).\nTest of consistency\n\n\\(d = 2\\)\n\\(V_n = h_{n}^{2} = \\frac{1}{n}\\)\n\\(h_n \\rightarrow 0\\), \\(V_n \\rightarrow 0\\)\n\\(nV_n = 1\\) is not approaching to infinity as \\(n\\rightarrow \\infty\\)\nThus, the consistency is not guranteed.\n\nModification\n\nLet \\(h_n = \\frac{1}{n^{a}}\\)\n\\(V_n = \\frac{1}{n^{2p}}\\), let \\(p>0\\)\n\\(nV_n = \\frac{1}{n^{2p-1}}\\), \\(\\lim_{n\\to \\infty} nV_n \\to \\infty\\). \\(2p-1<0\\)\n\n\\(0<p<\\frac{1}{2}\\)\n\nThe universal consistence of the cubic histogram rule is guaranteed."
  },
  {
    "objectID": "hw/hw3.html#problem-5.2",
    "href": "hw/hw3.html#problem-5.2",
    "title": "8  Homework 3",
    "section": "8.4 Problem 5.2",
    "text": "8.4 Problem 5.2\n\nConsider that an experimenter wants to use the kNN classification rule and achieve consistency as the sample size \\(n\\) increases. In each of the following alternatives, answer whether the experimenter is successful and why.\n\n\n8.4.1 (a)\n\nThe experimenter does not know the distribution of \\((X,Y)\\) and lets \\(k\\) increase as \\(k=\\sqrt{n}\\).\n\nUse Braga-Neto (2020, Theorem 5.7)\n\n\\(k=\\sqrt{n}\\)\n\\(\\lim_{n\\to \\infty} k = \\infty\\)\n\\(\\lim_{n\\to\\infty} \\frac{k}{n} = \\lim_{n\\to\\infty}\\frac{1}{\\sqrt{n}} = 0\\)\nThe kNN rule is universally consistent.\n\n\n\n8.4.2 (b)\n\nThe experimenter does not know the distribution but knows that \\(\\epsilon^{*} = 0\\) and keeps \\(k\\) fixed, \\(k=3\\).\n\nBecause \\(k\\) is fiexed and independent of \\(n\\), the approach is not universally consistent. However, since \\(\\epsilon^{*}=0\\), this approach is consistent."
  },
  {
    "objectID": "hw/hw3.html#problem-5.6",
    "href": "hw/hw3.html#problem-5.6",
    "title": "8  Homework 3",
    "section": "8.5 Problem 5.6",
    "text": "8.5 Problem 5.6\n\nAssume that the feature \\(X\\) in a classification problem is a real number in the interval \\([0,1]\\). Assume that the classes are equally likely, with \\(p(x|Y=0) = 2xI_{\\{0\\leq x\\leq 1\\}}\\) and \\(p(x|Y=1)= 2(1-x)I_{\\{0\\leq x\\leq 1\\}}\\).\n\n\n8.5.1 (a)\n\nFind the Bayes error \\(\\epsilon^*\\).\n\nBecase the two classes are equally likely, \\(p(Y=0)=p(Y=1)=0.5\\). \\[\\epsilon^{*}=E[\\min(\\eta(x), 1-\\eta(x))]\\]\n\\[\\begin{align}\n    \\eta(x) &= p(Y=1|x)\\\\\n            &= \\frac{p(x|Y=1)p(Y=1)}{p(x)}\\\\\n            &= \\frac{p(x|Y=1)p(Y=1)}{p(x|Y=0)p(Y=0)+p(x|Y=1)p(Y=1)}\\\\\n            &= \\frac{2(1-x)\\cdot 0.5}{2x\\cdot 0.5 + 2(1-x)\\cdot 0.5}\\\\\n            &= \\frac{2(1-x)}{2x + 2-2x}\\\\\n            &= 1 - x\\\\\n1 - \\eta(x) &= x\n\\end{align}\\]\n\\(p(x) = p(x|Y=0)p(Y=0)+p(x|Y=1)p(Y=1) = 2x\\cdot 0.5 + 2(1-x)\\cdot 0.5 = 1\\)\n\\[\\begin{align}\n    \\epsilon^{*} &=E[\\min(\\eta(x), 1-\\eta(x))]\\\\\n                 &= E[\\min(1-x, x)]\\\\\n                 &= \\int_{0}^{1}\\min(\\eta(x), 1-\\eta(x))p(x)dx\\\\\n                 &= \\int_{0}^{1}\\min(\\eta(x), 1-\\eta(x))dx\\\\\n                 &= \\int_{0}^{\\frac{1}{2}} x dx + \\int_{\\frac{1}{2}}^{1} (1-x)dx\\\\\n                 &= 0.25\n\\end{align}\\]\n\n\n8.5.2 (b)\n\nFind the asymptotic error rate \\(\\epsilon_{NN}\\) for the NN classification rule.\n\nUse Cover-Hart Theorem (Braga-Neto 2020, Theorem 5.1).\n\\[\\epsilon_{NN} = \\lim_{n\\to \\infty} E[\\epsilon_n] = E[2\\eta(X)(1-\\eta(X))]\\]\nUse the result from Problem 5.6(a).\n\\[\\begin{align}\n    \\eta(x) &= 1-x\\\\\n    1-\\eta(x) &= x\n\\end{align}\\]\n\\[\\begin{align}\n    \\epsilon_{NN} = \\lim_{n\\to \\infty} E[\\epsilon_n] &= E[2\\eta(X)(1-\\eta(X))]\\\\\n    &= E[2(1-x)x]\\\\\n    &= 2E[x - x^2]\\\\\n    &= 2\\left( \\int^{1}_{0}xp(x)dx - \\int^{1}_{0}x^{2}p(x)dx \\right)\\\\\n    &= 2\\left(\\int^{1}_{0}xdx - \\int^{1}_{0}x^{2}dx \\right)\\\\\n    &= 2\\left((\\frac{1}{2}x^2)^{2}_{1} - (\\frac{1}{3}x^2)^{1}_{0} \\right)\\\\\n    &= 2(\\frac{1}{2} - \\frac{1}{3})\\\\\n    &= 2(\\frac{3 - 2}{6})\\\\\n    &= \\frac{1}{3}\n\\end{align}\\]"
  },
  {
    "objectID": "hw/hw3.html#problem-5.10-python-assignment",
    "href": "hw/hw3.html#problem-5.10-python-assignment",
    "title": "8  Homework 3",
    "section": "8.6 Problem 5.10 (Python Assignment)",
    "text": "8.6 Problem 5.10 (Python Assignment)\n\n8.6.1 (a)\n\nModify the code in c05_kernel.py (modified in appendix) to obtain plots for \\(h = 0.1, 0.3, 0.5, 1, 2, 5\\)1 and \\(n = 50, 100, 250, 500\\) per class. Plot the classifiers over the range \\([-3, 9] \\times [-3, 9]\\) in order to visualize the entire data and reduce the marker size from 12 to 8 to facilitate visualization. Which classifiers are closest to the optimal classifier? How do you explain this in terms of underfitting/overfitting? See the coding hint in part (a) of Problem 5.8.\n\nSince the optimal boundary is a straight line between two centroids. Those classifiers close to optimal decision tend to have large sample size.\nThe bandwidth (h) can have influences on the underfitting/overfitting. Small h may get overfitting; on the other hand, large h may get underfitting.\n\ndef plot_kd(ax, x0, y0, x1, y1, Z):\n    cmap_light = ListedColormap([\"#FFE0C0\",\"#B7FAFF\"])\n    plt.rc(\"xtick\",labelsize=16)\n    plt.rc(\"ytick\",labelsize=16)\n    ax.plot(x0,y0,\".r\",markersize=8) # class 0\n    ax.plot(x1,y1,\".b\",markersize=8) # class 1\n    ax.set_xlim([-3,9])\n    ax.set_ylim([-3,9])\n    ax.pcolormesh(xx,yy,Z,cmap=cmap_light, shading=\"nearest\")\n    ax.contour(xx,yy,Z,colors=\"black\",linewidths=0.5)\n    plt.close()\n    return ax\n\n\nmm0 = np.array([2,2])\nmm1 = np.array([4,4])\nSig0 = 4*np.identity(2)\nSig1 = 4*np.identity(2)\nNs = np.array([50, 100, 250, 500])\n#Ns = [50]\nhs = np.array([0.1,0.3,0.5,1, 2, 5])\n#hs = [0.1]\nXs = [[mvn.rvs(mm0, Sig0, n), mvn.rvs(mm1,Sig1,n)] for n in Ns]\n\nclf0s = [[KD() for i in range(0, len(hs))] for j in range(0, len(Ns))]\nclf1s = [[KD() for i in range(0, len(hs))] for j in range(0, len(Ns))]\n\n# plotting\nx_min,x_max = (-3,9)\ny_min,y_max = (-3,9)\ns = 0.1 #0.01 # mesh step size\nxx,yy = np.meshgrid(np.arange(x_min,x_max,s),np.arange(y_min,y_max,s))\nfig, axs = plt.subplots(len(Ns), len(hs), figsize=(30,20), dpi=150)\n\nfor (i, X) in enumerate(Xs):\n    x0,y0 = np.split(X[0],2,1)\n    x1,y1 = np.split(X[1],2,1)\n    y = np.concatenate((np.zeros(Ns[i]),np.ones(Ns[i])))\n    for (j, h) in enumerate(hs):\n        clf0s[i][j] = KD(bandwidth=h)\n        clf0s[i][j].fit(X[0])\n        clf1s[i][j] = KD(bandwidth=h)\n        clf1s[i][j].fit(X[1])\n        \n        Z0 = clf0s[i][j].score_samples(np.c_[xx.ravel(), yy.ravel()])\n        Z1 = clf1s[i][j].score_samples(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z0<=Z1\n        Z = Z.reshape(xx.shape)\n        plot_kd(axs[i][j], x0, y0, x1, y1, Z);\n        axs[i][j].set_title(\"N={},h={}\".format(Ns[i],h))\n        plt.close()\nfig.savefig(\"img/c05_kernel.png\",bbox_inches=\"tight\",facecolor=\"white\");\n\n\n\n\n8.6.2 (b)\n\nCompute test set errors for each classifier in part (a), using the same procedure as in part (b) of Problem 5.8. Generate a table containing each classifier plot in part (a) with its test set error rate. Which combinations of sample size and kernel bandwidth produce the top 5 smallest error rates?\n\nBayes error\n\\[\\begin{align}\n\\delta &= \\sqrt{(\\mu_1 - \\mu_0)^T \\Sigma^{-1} (\\mu_1-\\mu_0)}\n\\end{align}\\]\n\\[\\epsilon = \\Phi(-\\frac{\\delta}{2}) \\approx 0.23975\\]\n\nmu1 = np.matrix([[4],[4]])\nmu0 = np.matrix([[2,],[2]])\nsig = np.matrix([[4,0], [0,4]])\ndelta = np.sqrt( (mu1-mu0).T @ np.linalg.inv(sig) @ (mu1-mu0))[0][0]\nerror = st.norm.cdf(-delta/2)\n\npd.DataFrame({\"Bayes Error\":[error]})\n\n\n\n\n\n  \n    \n      \n      Bayes Error\n    \n  \n  \n    \n      0\n      [[0.23975006109347669]]\n    \n  \n\n\n\n\nBest Five\n\n0.269 (1,4) N=50, h=1\n0.271 (1,6) N=50, h=5\n0.273 (1,5) N=50, h=2\n0.274 (3,4) N=250, h=1\n0.276 (3,6) N=250, h=5\n\n\ndef measure_test_error(clf0, clf1, xxs, yys, ys):\n    Z0 = clf0.score_samples(np.c_[xxs, yys])\n    Z1 = clf1.score_samples(np.c_[xxs, yys])\n    Z = Z0<=Z1\n    return np.count_nonzero(Z != ys.astype(bool)) / len(ys)\n\nnt = 500\nX_test = [mvn.rvs(mm0, Sig0, nt), mvn.rvs(mm1,Sig1,nt)] \nx0,y0 = np.split(X_test[0],2,1)\nx1,y1 = np.split(X_test[1],2,1)\nys = np.concatenate((np.zeros(nt),np.ones(nt)))\nerrs = np.zeros((len(Ns), len(hs)))\n\nfigt, axts = plt.subplots(len(Ns), len(hs), figsize=(30,20), dpi=150)\n\nfor i in range(0, len(Ns)):\n    xxs = np.concatenate((x0, x1))\n    yys = np.concatenate((y0, y1))\n    for (j, h) in enumerate(hs):\n        errs[i][j]= measure_test_error(clf0s[i][j], clf1s[i][j], xxs, yys, ys)\n\n        axts[i][j].set_title(axs[i][j].get_title() + \"Test Error: {}\\n Bayes Error: {}\".format(errs[i][j], 2))\n        plt.close()\n        \n        Z0 = clf0s[i][j].score_samples(np.c_[xx.ravel(), yy.ravel()])\n        Z1 = clf1s[i][j].score_samples(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z0<=Z1\n        Z = Z.reshape(xx.shape)\n        plot_kd(axts[i][j], x0, y0, x1, y1, Z);\n\nprint(errs)\n\nfigt.savefig(\"img/c05_kernel_test.png\",bbox_inches=\"tight\",facecolor=\"white\");\n\n[[0.339 0.318 0.279 0.269 0.273 0.271]\n [0.349 0.313 0.292 0.281 0.279 0.278]\n [0.351 0.296 0.279 0.274 0.278 0.276]\n [0.335 0.289 0.283 0.285 0.282 0.282]]\n\n\n\n\n\n8.6.3 (c)\n\nCompute expected error rates for the Gaussian kernel classification rule in part (a), using the same procedure as in part (c) of Problem 5.8. Since error computation is faster here, a larger value \\(R = 200\\) can be used, for better estimation of the expected error rates. Which kernel bandwidth should be used for each sample size?\n\n\nerrs = np.zeros((len(Ns), len(hs)))\nR = 200\nfor jj in range(0, R):\n    nt = 500\n    X_test = [mvn.rvs(mm0, Sig0, nt), mvn.rvs(mm1,Sig1,nt)] \n    x0,y0 = np.split(X_test[0],2,1)\n    x1,y1 = np.split(X_test[1],2,1)\n    ys = np.concatenate((np.zeros(nt),np.ones(nt)))\n\n    xxs = np.concatenate((x0, x1))\n    yys = np.concatenate((y0, y1))\n    for i in range(0, len(Ns)):\n        for (j, h) in enumerate(hs):\n            errs[i][j]+= measure_test_error(clf0s[i][j], clf1s[i][j], xxs, yys, ys)\n\nerrs = errs/R\nbest_hi = np.argmin(errs, axis=1)\nprint(errs)\nprint(best_hi)\n\npd.DataFrame({\"Sample Size\": Ns, \"Best H\": hs[best_hi ]})\n\n[[0.309245 0.29467  0.26515  0.250625 0.24554  0.24833 ]\n [0.325025 0.2838   0.255735 0.2419   0.240805 0.240665]\n [0.33395  0.276555 0.25415  0.245395 0.23972  0.23971 ]\n [0.31032  0.25389  0.246315 0.240215 0.240195 0.239245]]\n[4 5 5 5]\n\n\n\n\n\n\n  \n    \n      \n      Sample Size\n      Best H\n    \n  \n  \n    \n      0\n      50\n      2.0\n    \n    \n      1\n      100\n      5.0\n    \n    \n      2\n      250\n      5.0\n    \n    \n      3\n      500\n      5.0"
  },
  {
    "objectID": "hw/hw3.html#appendix",
    "href": "hw/hw3.html#appendix",
    "title": "8  Homework 3",
    "section": "8.7 Appendix",
    "text": "8.7 Appendix\n\n8.7.1 Revised c05_kernel.py2\n\"\"\"\nFoundations of Pattern Recognition and Machine Learning\nChapter 5 Figure 5.5\nAuthor: Ulisses Braga-Neto\nPlot kernel classifiers\n\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal as mvn\nfrom sklearn.neighbors import KernelDensity as KD\nfrom matplotlib.colors import ListedColormap\n# Fix random state for reproducibility\nnp.random.seed(1978081)\nmm0 = np.array([2,2])\nmm1= np.array([4,4])\nSig0 = 4*np.identity(2)\nSig1 = 4*np.identity(2)\nN = 50 # number of points in each class\nX0 = mvn.rvs(mm0,Sig0,N)\nx0,y0 = np.split(X0,2,1)\nX1 = mvn.rvs(mm1,Sig1,N)\nx1,y1 = np.split(X1,2,1)\nX = np.concatenate((X0,X1),axis=0)\ny = np.concatenate((np.zeros(N),np.ones(N)))\ncmap_light = ListedColormap([\"#FFE0C0\",\"#B7FAFF\"])\ns = .01  # mesh step size\nx_min,x_max = (-0.5,6.5)\ny_min,y_max = (-0.5,6.5)\nfor h in [0.1,0.3,0.5,1]:\n    clf0 = KD(bandwidth=h)\n    clf0.fit(X0)\n    clf1 = KD(bandwidth=h)\n    clf1.fit(X1)\n    xx,yy = np.meshgrid(np.arange(x_min,x_max,s),np.arange(y_min,y_max,s))\n    Z0 = clf0.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z1 = clf1.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z0<=Z1\n    Z = Z.reshape(xx.shape)\n    fig,ax=plt.subplots(figsize=(8,8),dpi=150)\n    plt.rc(\"xtick\",labelsize=16)\n    plt.rc(\"ytick\",labelsize=16)\n    plt.plot(x0,y0,\".r\",markersize=16) # class 0\n    plt.plot(x1,y1,\".b\",markersize=16) # class 1\n    plt.xlim([-0.18,6.18])\n    plt.ylim([-0.18,6.18])\n    plt.pcolormesh(xx,yy,Z,cmap=cmap_light)\n    ax.contour(xx,yy,Z,colors=\"black\",linewidths=0.5)\n    plt.show()\n    fig.savefig(\"c05_kernel\"+str(int(10*h))+\".png\",bbox_inches=\"tight\",facecolor=\"white\")\n\n\n\n\nBraga-Neto, Ulisses. 2020. Fundamentals of Pattern Recognition and Machine Learning. Springer."
  },
  {
    "objectID": "hw/hw4.html#homework-description",
    "href": "hw/hw4.html#homework-description",
    "title": "9  Homework 4",
    "section": "9.1 Homework Description",
    "text": "9.1 Homework Description\n\nCourse: ECEN649, Fall2022\nDeadline: 2022/11/16, 11:59 pm\n\n\nProblems from the Book\n6.3\n6.5\n6.7\n7.1\n7.10\n6.12 (coding assignment)\nProblems 6.3-6.5 are worth 10 points each, Problem 7.10 and the coding assignment are worth 20 points each."
  },
  {
    "objectID": "hw/hw4.html#computational-environment",
    "href": "hw/hw4.html#computational-environment",
    "title": "9  Homework 4",
    "section": "9.2 Computational Environment",
    "text": "9.2 Computational Environment\n\n9.2.1 Libraries\n\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport sys\n\n\n\n9.2.2 Versions\n\nprint(np.__version__)\nprint(tf.__version__)\nprint (sys.version)\nprint(sys.executable)\n\n1.23.4\n2.10.0\n3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]\n/Users/stevenchiu/miniconda/bin/python"
  },
  {
    "objectID": "hw/hw4.html#problem-6.3",
    "href": "hw/hw4.html#problem-6.3",
    "title": "9  Homework 4",
    "section": "9.3 Problem 6.3",
    "text": "9.3 Problem 6.3\n\nShow that the decision regions produced by a neural network with \\(k\\) threshold sigmoids in the first hidden layer, no matter what nonlinearities are used in succeeding layers, are equal to the intersection of \\(k\\) half-spaces, i.e., the decision boundary is piecewise linear\nHint: All neurons in the first hidden layer are perceptrons and the output of the layer is a binary vector.\n\nLet \\(\\bar{O}\\) be the \\(k\\) output of first hidden layer, and there are \\(2^k\\) types of binary vectors \\([O_1, \\dots, O_k]\\). Therefore, before the next layer, the input is determinstic in \\(2^k\\) combination in \\(R^k\\) space.\nFor each data point \\(x\\in R^d\\) where \\(d\\) is the feature space. the output of first layer is\n\\[O(x)_{i} = I_{g_{i}(x)}(x), \\quad i = 1,\\cdots, k \\tag{9.1}\\]\nwhere \\(g_{i}(\\cdot)\\) is the perceptron function of neuron \\(i\\). Thus, any point \\(x\\) belong to one type of \\([I_{g_{1}(x)}(x), \\dots, I_{g_{k}(x)}(x)]\\). For each \\(O_i\\), the space forms a half-space with \\(\\{x: g_i(x) > 0\\}\\), and there are \\(k\\) half space in total."
  },
  {
    "objectID": "hw/hw4.html#problem-6.5",
    "href": "hw/hw4.html#problem-6.5",
    "title": "9  Homework 4",
    "section": "9.4 Problem 6.5",
    "text": "9.4 Problem 6.5\n\nFor the VGG16 CNN architecture:\n\n\n\n\nFigure 9.1: VGG16\n\n\n\n9.4.1 (a)\n\nDetermine the number of filters used in each convolution layer.\n\n\nConv-1: 64 filters (pre-depth: 3)\nConv-2: 128 filters (pre-depth: 64)\nConv-3: 256 filters (pre-depth: 128)\nConv-4: 512 filters (pre-depth: 256)\nConv-5: 512 filters (pre-depth: 512)\n\nThere are total\n\nrs = np.array([3, 64, 128, 256, 512])\nt_filters = np.array([64, 128, 256, 512, 512])\nnp.sum(t_filters)\n\n1472\n\n\nfilters.\n\n\n9.4.2 (b)\n\nBased on the fact that all filters are of size \\(3\\times 3\\times r\\), where \\(r\\) is the depth of the previous layer, determine the total number of convolution weights in the entire network.\n\n\nCONV1 = (3*3*3)*64 + (3*3*64)*64 \nCONV1\n\n38592\n\n\n\nCONV2 = (3*3*64)*128 + (3*3*128)*128\nCONV2\n\n221184\n\n\n\nCONV3 = (3*3*128)*256 + (3*3*256)*256 + (3*3*256)*256\nCONV3\n\n1474560\n\n\n\nCONV4 = (3*3*256)*512 + (3*3*512)*512 + (3*3*512)*512\nCONV4\n\n5898240\n\n\n\nCONV5 = (3*3*512)*512 *3\nCONV5\n\n7077888\n\n\n\nfc1 = 512 * 7 * 7 * 4096\nfc1\n\n102760448\n\n\n\nfc2 = 4096 * 4096\nfc2\n\n16777216\n\n\n\nfc3 = 4096 * 1000\nfc3\n\n4096000\n\n\n\n\n9.4.3 (c)\n\nAdd the weights used in the fully-connected layers to obtain the total number of weights used by VGG16.\n\nTotal of weights\n\ntotal = np.sum([CONV1, CONV2, CONV3, CONV4, CONV5, fc1, fc2, fc3])\ntotal \n\n138344128"
  },
  {
    "objectID": "hw/hw4.html#problem-6.7",
    "href": "hw/hw4.html#problem-6.7",
    "title": "9  Homework 4",
    "section": "9.5 Problem 6.7",
    "text": "9.5 Problem 6.7\n\nConsider the training data set given in the figure below.\n\n\n\n\n\n\n\n9.5.1 (a)\n\nBy inspection, find the coefficients of the linear SVM hyperplane \\(a_1 x_1 + a_2 x_2 + a_0 = 0\\) and plot it. What is the value of the margin? Say as much as you can about the values of the Lagrange multipliers associated with each of the points.\n\n\n\n\nFigure 9.2: SVM boundry\n\n\nThe boundary passes by \\(\\frac{1}{2}((3,3) + (3,2)) = (3, 2.5)\\) and \\(\\frac{1}{2} ((3,4) + (4,3))=(3.5, 3.5)\\)\n\n\\(a_1 = 2.5 - 3.5 = -1\\)\n\\(a_2 = 3.5 - 3 = 0.5\\)\n\\(a_0 = 3\\cdot 3.5 - 3.5 \\cdot 2.5 = 1.75\\)\nThe boundary is \\[-x_1 + 0.5 x_2 + 1.75 = 0\\]\n\nIn Figure 9.2, there are \\(6\\) support vectors that are \\(\\lambda_2\\) to \\(\\lambda_7\\). The KKT conditions1 state that\n\\[\\begin{align}\n    \\lambda_i = 0 &\\Rightarrow y_i E_i \\leq 0\\\\\n    0 < \\lambda_i < C &\\Rightarrow y_i E_i = 0\\\\\n    \\lambda_i = C &\\Rightarrow y_i E_i \\geq 0\n\\end{align}\\]\n\nLagrange multipliers\n\n\\(\\lambda_1 = 0\\)\n\\(\\lambda_2 \\in (0, C)\\)\n\\(\\lambda_3 \\in (0, C)\\)\n\\(\\lambda_4 \\in (0, C)\\)\n\\(\\lambda_5 \\in (0, C)\\)\n\\(\\lambda_6 \\in (0, C)\\)\n\\(\\lambda_7 \\in (0, C)\\)\n\\(\\lambda_8 = 0\\)\n\n\nwhere \\(C\\) is the pentalty term.\n\n\n9.5.2 (b)\n\nApply the CART rule, using the misclassification impurity, and stop after finding one splitting node (this is the “1R” or “stump” rule). If ther eis a tie between best splits, pick one that makes at most one error in each class. Plot this classifier as a decision boundary superimposed on the training data and also as a binary decision tree showing the splitting and leaf nodes.\n\n\n\n\n\n\n\n\nDecision boundary\n\n\n\n\n\n\n\nflowchart TD\n  A[x1 <= 3.5] \n  A --> |yes| D[1]\n  A --> |no| E[0]\n\n\n\n\n\n\n\n\n\nFigure 9.3: Apply CART rule\n\n\nwhere \\(\\bullet\\) labelled as \\(1\\); \\(\\circ\\) labelled as \\(0\\).\n\n\n9.5.3 (c)\n\nHow do you compare the classifiers in (a) and (b) ? Which one is more likely to have a smaller classification error in this problem?\n\n\nSVM of (a) yields smaller classification error than (b) because it allow any slope of decision boundary."
  },
  {
    "objectID": "hw/hw4.html#problem-7.1",
    "href": "hw/hw4.html#problem-7.1",
    "title": "9  Homework 4",
    "section": "9.6 Problem 7.1",
    "text": "9.6 Problem 7.1\n\nSuppose that the classification error \\(\\epsilon_n\\) and an error estimator \\(\\hat{\\epsilon}_n\\) are jointly Gaussian, such \\[\\epsilon_n \\sim N(\\epsilon^* + \\frac{1}{n}, \\frac{1}{n^2}), \\hat{\\epsilon}_n \\sim N(\\epsilon^* - \\frac{1}{n}, \\frac{1}{n^2}), Cov(\\epsilon_n, \\hat{\\epsilon}_n) = \\frac{1}{2n^2}\\] where \\(\\epsilon^*\\) is the Bayes error. Find the bias, deviation variance, RMS, correlation coefficient and tail probabilities \\(P(\\hat{\\epsilon}_n - \\epsilon_n < - \\tau)\\) and \\(P(\\hat{\\epsilon}_n - \\epsilon_n > \\tau)\\) of \\(\\hat{\\epsilon}_n\\). Is this estimator optimistically or pessimistically biased? Does performance improve as sample size increases? Is the estimator consistent?\n\n\n9.6.1 Bias\nUse Eq. 7.3 (Braga-Neto 2020, 154),\n\\[Bias(\\hat{\\epsilon}_n) = E[\\hat{\\epsilon}_n] - E[\\epsilon_n]\\]\n\n\\(E[\\hat{\\epsilon}_n] = \\epsilon^{*} - \\frac{1}{n}\\)\n\\(E[\\epsilon_n] = \\epsilon^* + \\frac{1}{n}\\)\n\nThus,\n\\[Bias(\\hat{\\epsilon}_n) = \\frac{-2}{n} < 0\\]\nThis estimator is optimisitcally biased.\n\n\n9.6.2 Deviation variance\nUse Eq. 7.4 (Braga-Neto 2020, 154),\n\\[Var_{dev}(\\hat{\\epsilon}_n) = Var(\\hat{\\epsilon}_n, \\epsilon_n) = Var(\\hat{\\epsilon}_n) + Var(\\epsilon_n) - 2 Cov(\\epsilon_n, \\hat{\\epsilon}_n)\\]\n\n\\(Var(\\hat{\\epsilon}_n) = \\frac{1}{n^2}\\)\n\\(Var(\\epsilon_n) = \\frac{1}{n^2}\\)\n\\(Cov(\\epsilon_n, \\hat{\\epsilon}_n) = \\frac{1}{2n^2}\\)\n\nThus,\n\\[Var_{dev}(\\hat{\\epsilon}_n) = \\frac{1}{n^2} + \\frac{1}{n^2} - 2\\frac{1}{2n^2} = \\frac{1}{n^2}\\]\nThe deviation variance reduces as sample size increases.\n\n\n9.6.3 Root mean-square error\nUse Eq. 7.5 (Braga-Neto 2020, 154),\n\\[RMS(\\hat{\\epsilon}_n) = \\sqrt{E[(\\hat{\\epsilon}_n  - \\epsilon_n)^2]} = \\sqrt{Bias(\\hat{\\epsilon}_n)^2 + Var_{dev}(\\hat{\\epsilon}_n)}\\]\nApply previous results,\n\\[RMS(\\hat{\\epsilon}_n) = \\sqrt{\\frac{4}{n^2} + \\frac{1}{n^2}} = \\frac{\\sqrt{5}}{n}\\]\n\n\n9.6.4 Correlation coefficient\nUse the pearson correlation coefficient2\n\\[\\rho_{X,Y} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\\]\n\n\\(Cov(\\epsilon_n, \\hat{\\epsilon}_n) = \\frac{1}{2n^2}\\)\n\\(\\sigma_{\\epsilon_n} = \\frac{1}{n}\\)\n\\(\\sigma_{\\hat{\\epsilon}_n} = \\frac{1}{n}\\)\n\n\\[\\rho_{\\epsilon_n, \\hat{\\epsilon}_n} =\\frac{1}{2}\\]\nCorrelation coefficient is a constant and independent from sample size.\n\n\n9.6.5 Tail probabilities\nUse Eq. 7.6 (Braga-Neto 2020, 154),\n\\[P(|\\hat{\\epsilon}_n - \\epsilon_n| \\geq \\tau) = P(\\hat{\\epsilon}_n - \\epsilon_n \\geq \\tau) + P(\\hat{\\epsilon}_n - \\epsilon_n \\leq -\\tau), \\quad \\text{for } \\tau > 0\\]\nThe normal difference distribution3 of \\(\\hat{\\epsilon}_n - \\epsilon_n\\)\n\\[\\hat{\\epsilon}_n - \\epsilon_n \\sim N(\\frac{-2}{n}, \\frac{2}{n^2}) = N(\\mu, \\sigma^2)\\]\nThat \\(\\Delta\\epsilon_n = \\hat{\\epsilon}_n - \\epsilon_n\\)\n\\[\\begin{align}\n    P(\\Delta\\epsilon_n \\leq -\\tau)%\n    &= P(\\frac{\\Delta\\epsilon_n - \\mu}{\\sigma} \\leq \\frac{-\\tau - \\mu}{\\sigma})\\\\\n    &= \\Phi(\\frac{-\\tau - \\mu}{\\sigma})\\\\\n    &= \\Phi(\\frac{-\\tau + 2/n}{\\sqrt{2}/n})\\\\\n    &= \\Phi(\\frac{-n\\tau + 2}{\\sqrt{2}})\\\\\n\\end{align}\\]\n\\[\\begin{align}\n    P(\\Delta\\epsilon_n \\geq \\tau)%\n    &= P(\\frac{\\Delta\\epsilon_n - \\mu}{\\sigma} \\geq \\frac{\\tau - \\mu}{\\sigma})\\\\\n    &= 1 - P(\\frac{\\Delta\\epsilon_n - \\mu}{\\sigma} < \\frac{\\tau - \\mu}{\\sigma})\\\\\n    &= 1 - \\Phi(\\frac{\\tau - \\mu}{\\sigma})\\\\\n    &= 1 - \\Phi(\\frac{n\\tau - 2}{\\sqrt{2}})\n\\end{align}\\]\nThus, when \\(n\\to \\infty\\)\n\\[\\begin{align}\n    \\lim_{n\\to \\infty}  P(\\Delta\\epsilon_n \\leq -\\tau) = 0\\\\\n    \\lim_{n\\to \\infty} P(\\Delta\\epsilon_n \\geq \\tau) = 0\n\\end{align}\\]\nThis can be conluced to \\[\\lim_{n\\to \\infty} P(|\\hat{\\epsilon}_n - \\epsilon_n| \\geq \\tau) = 0\\]\nThe estimator is consistent."
  },
  {
    "objectID": "hw/hw4.html#problem-7.10",
    "href": "hw/hw4.html#problem-7.10",
    "title": "9  Homework 4",
    "section": "9.7 Problem 7.10",
    "text": "9.7 Problem 7.10\n\nThis problem illustrates the very poor (even paradoxical) performance of cross-validation with very small sample sizes. Consider the resubstitution and leave-one-out estimators \\(\\hat{\\epsilon}^{r}_{n}\\) and \\(\\hat{\\epsilon}^{l}_n\\) for the 3NN classification rule, with a sample of size \\(n=4\\) from a mixture of two equally-likely Gaussian populations \\(\\Pi_0 \\sim N_d(\\mu_0, \\Sigma)\\) and \\(\\Pi_1 \\sim N_d(\\mu_1, \\Sigma)\\). Assume that \\(\\mu_0\\) and \\(\\mu_1\\) are far enough apart to make \\(\\delta = \\sqrt{(\\mu_1 - \\mu_0)^T \\Sigma^{-1} (\\mu_1 = \\mu_0)}\\gg 0\\) (in which case the Bayes error is \\(\\epsilon_{\\text{bay}} = \\Phi(-\\frac{\\delta}{2})\\approx 0\\)).\n\n\n9.7.1 (a)\n\nFor a sample \\(S_n\\) with \\(N_0 = N_1 = 2\\), which occurs \\(P(N_0 = 2) = {4\\choose 2}2^{-4} = 37.5\\%\\) of the time, show that \\(\\epsilon_n \\approx 0\\) but \\(\\hat{\\epsilon}^{l}_{n}=1\\)\n\nIf \\(N_0=N_1=2\\), the leave-one-out method removes one of the data point. The remaining points will have the majority label and have opposite label to the removed point (Figure 9.4). This flipping causes \\(\\hat{\\epsilon}^l = 1\\).\nSince two Gaussian population are far away from each other. The decidsion boundary is in the middle of two means, and there is little overlap between two distribution. Thus, when \\(\\delta \\gg 0\\), \\(\\epsilon_n \\approx 0\\).\n\n\n\n\n\nFigure 9.4: Two separated gaussian process in hyperplan with N0=N1=2\n\n\n\n\n\n\n9.7.2 (b)\n\nShow that \\(E[\\epsilon_n] \\approx \\frac{5}{16} = 0.3125\\), but \\(E[\\hat{\\epsilon}^{l}_{n}] = 0.5\\), so that \\(\\text{Bias}(\\hat{\\epsilon}^{l}_{n}) \\approx \\frac{3}{16} = 0.1875\\), and the leave-one-out estimator is far from unbiased.\n\nGiven two label have equal occurrences,\n\n\\(P(N_0 = 0) = {4 \\choose 0}2^{-4} = 1\\cdot 2^{-4}\\)\n\n\\((N_0, N_1) = (0, 4)\\)\n\\(\\epsilon_n = \\frac{1}{2}\\) (always predicting \\(N_1\\))\n\\(\\hat{\\epsilon}_{n}^{l} = 0\\)\n\n\\(P(N_0 = 1) = {4 \\choose 1}2^{-4} = 4\\cdot 2^{-4}\\)\n\n\\((N_0, N_1) = (1, 3)\\)\n\\(\\epsilon_n = \\frac{1}{2}\\)\n\\(\\hat{\\epsilon}_{n}^{l} = \\frac{1}{4}\\)\n\n\\(P(N_0 = 2) = {4 \\choose 2}2^{-4} = 6\\cdot 2^{-4}\\)\n\n\\((N_0, N_1) = (2, 2)\\)\n\\(\\epsilon_n = 0\\)\n\\(\\hat{\\epsilon}_{n}^{l} = 1\\) (flipped)\n\n\\(P(N_0 = 3) = {4 \\choose 3}2^{-4} = 4\\cdot 2^{-4}\\)\n\n\\((N_0, N_1) = (3, 1)\\)\n\\(\\epsilon_n = \\frac{1}{2}\\)\n\\(\\hat{\\epsilon}_{n}^{l} = \\frac{1}{4}\\)\n\n\\(P(N_0 = 4) = {4 \\choose 4}2^{-4} = 1\\cdot 2^{-4}\\)\n\n\\((N_0, N_1) = (4, 0)\\)\n\\(\\epsilon_n = \\frac{1}{2}\\)\n\\(\\hat{\\epsilon}_{n}^{l} = 0\\)\n\n\n\\[E[\\epsilon_n] = \\frac{1}{2}\\frac{1}{16} + \\frac{1}{2}\\frac{4}{16}+ 0 + \\frac{1}{2}\\frac{4}{16} + \\frac{1}{2}\\frac{1}{16}=\\frac{5}{16}\n\\]\n\\[E[\\hat{\\epsilon}_{n}^{l}] = 0 + \\frac{1}{4}\\frac{4}{16} + 1\\frac{6}{16} + \\frac{1}{4}\\frac{4}{16} + 0 = \\frac{8}{16} = \\frac{1}{2}\\]\n\n\n9.7.3 (c)\n\nShow that \\(Var_d(\\hat{\\epsilon}^{l}_n) \\approx \\frac{103}{256} \\approx 0.402\\), which corresponds to a standard deviation of \\(\\sqrt{0.402} = 0.634\\). The leave-one-out estimator is therefore highly-biased and highly-variable in this case.\n\n\\[\\begin{align}\n    Var_d(\\hat{\\epsilon}^{l}_n)%\n    &= E[(\\hat{\\epsilon}^{l}_n - \\epsilon_n)^2] - (E[\\hat{\\epsilon}^{l}_n - \\epsilon_n])^2\\\\\n    &= (0 -\\frac{1}{2})^2 \\frac{1}{16} + (\\frac{1}{4} - \\frac{1}{2})^2 \\frac{4}{16}\\\\\n    &+ (1-0)^2 \\frac{6}{16} + (\\frac{1}{2} - \\frac{1}{4})^2 \\frac{4}{16} + (0-\\frac{1}{2})^2 \\frac{1}{16} - (\\frac{3}{16})^2\\\\\n    &= 2(\\frac{1}{2})^2 \\frac{1}{16} + 2(\\frac{1}{4})^2 \\frac{4}{16}+ \\frac{6}{16} - (\\frac{3}{16})^2\\\\\n    &= \\frac{14}{32} - (\\frac{3}{16})^2 = \\frac{103}{256}\n\\end{align}\\]\n\n\n9.7.4 (d)\n\nConsider the correlation coefficient of an error estimator \\(\\hat{\\epsilon}_n\\) with the true error \\(\\epsilon_n\\): \\[\\rho(\\epsilon_n, \\hat{\\epsilon}_n) = \\frac{Cov(\\epsilon_n, \\hat{\\epsilon}_n)}{Std(\\epsilon_n)Std(\\hat{\\epsilon}_n)}\\] Show that \\(\\rho(\\epsilon_n, \\hat{\\epsilon}^{l}_{n} \\approx 0.98)\\), i.e., the leave-one-out estimator is almost perfectly negatively correlated with the true error.\n\n\\[\\begin{align}\n    Var(\\hat{\\epsilon}_{n}^{l}) &= E[\\epsilon^{2}_n] - E[\\epsilon_n]^2\\\\\n    &= \\frac{1}{16}\\frac{4}{16} + \\frac{6}{16} + \\frac{1}{16}\\frac{4}{16} = \\frac{4 + 96 + 4}{256} - \\frac{1}{4} = \\frac{40}{256} = \\frac{5}{32}\n\\end{align}\\]\n\\[\\begin{align}\n    Var(\\epsilon_n)%\n    &= E[(\\hat{\\epsilon}_{n}^{l})^2] - (E[\\hat{\\epsilon}_{n}^{l}])^2\\\\\n    &= \\frac{1}{4}\\frac{1}{16} + \\frac{1}{4}\\frac{4}{16}\\\\\n    &+ \\frac{1}{4}\\frac{4}{16} + \\frac{1}{4}\\frac{1}{16} - (\\frac{5}{16})^2\\\\\n    &= \\frac{10}{64} - \\frac{25}{256} = \\frac{15}{256}\n\\end{align}\\]\nUse the previous results,\n\\[\\begin{align}\n    Cov(\\epsilon_n, \\hat{\\epsilon}_{n}^{l})%\n    &= E[\\epsilon_n \\hat{\\epsilon}_{n}^{l}] - E[\\epsilon_n ]E[\\hat{\\epsilon}_{n}^{l}]\\\\\n    &= (0 + \\frac{1}{2}\\frac{1}{4}\\frac{4}{16} + 0 + \\frac{1}{2}\\frac{1}{4}\\frac{4}{16}) - \\frac{5}{16}\\frac{1}{2}\\\\\n    &= \\frac{1}{16} - \\frac{5}{32} = \\frac{-3}{32} \\approx -0.98\n\\end{align}\\]\nWe can derive the correlation coefficient:\n\\[\\rho(\\epsilon_n, \\hat{\\epsilon}_{n}^{l}) = \\frac{-3/32}{\\sqrt{\\frac{5}{32}}\\sqrt{\\frac{15}{256}}}\\]\n\n\n9.7.5 (e)\n\nFor comparison, show that, although \\(E[\\hat{\\epsilon}^{r}_{n}] = \\frac{1}{8} = 0.125\\), so that \\(\\text{Bias}(\\hat{\\epsilon}^{r}_{n}) \\approx \\frac{-3}{16} = -0.1875\\), which is exactly the negative of the bias of leave-one-out, we have \\(Var_d(\\hat{\\epsilon}^{r}_{n}) \\approx \\frac{7}{256} \\approx 0.027\\), for a standard deviation of \\(\\frac{\\sqrt{7}}{16} \\approx 0.165\\), which is several times smaller than the leave-one-out variance, and \\(\\rho(\\epsilon_n, \\hat{\\epsilon}^{r}_{n}) \\approx \\sqrt{\\frac{3}{5}} \\approx 0.775\\), showing that the resubstitution estimator is highly positively correlated with the true error.\n\nThe resubstitution error estimator uses 3 nearest neighbors, and no point is removed:\n\n\\(P(N_0 = 0) = {4 \\choose 0}2^{-4} = 1\\cdot 2^{-4}\\)\n\n\\((N_0, N_1) = (0, 4)\\)\n\\(\\epsilon_n = \\frac{1}{2}\\)\n\\(\\hat{\\epsilon}_{n}^{r} = 0\\)\n\n\\(P(N_0 = 1) = {4 \\choose 1}2^{-4} = 4\\cdot 2^{-4}\\)\n\n\\((N_0, N_1) = (1, 3)\\)\n\\(\\epsilon_n = \\frac{1}{2}\\)\n\\(\\hat{\\epsilon}_{n}^{r} = \\frac{1}{4}\\)\n\n\\(P(N_0 = 2) = {4 \\choose 2}2^{-4} = 6\\cdot 2^{-4}\\)\n\n\\((N_0, N_1) = (2, 2)\\)\n\\(\\epsilon_n = 0\\)\n\\(\\hat{\\epsilon}_{n}^{r} = 0\\)\n\n\\(P(N_0 = 3) = {4 \\choose 3}2^{-4} = 4\\cdot 2^{-4}\\)\n\n\\((N_0, N_1) = (3, 1)\\)\n\\(\\epsilon_n = \\frac{1}{2}\\)\n\\(\\hat{\\epsilon}_{n}^{r} = \\frac{1}{4}\\)\n\n\\(P(N_0 = 4) = {4 \\choose 4}2^{-4} = 1\\cdot 2^{-4}\\)\n\n\\((N_0, N_1) = (4, 0)\\)\n\\(\\epsilon_n = \\frac{1}{2}\\)\n\\(\\hat{\\epsilon}_{n}^{r} = 0\\)\n\n\nThe resubstitution estimator is positively correlated with the true error.\n\n\n\n\nBraga-Neto, Ulisses. 2020. Fundamentals of Pattern Recognition and Machine Learning. Springer."
  },
  {
    "objectID": "hw/hw4_p6-12_right_cv.html",
    "href": "hw/hw4_p6-12_right_cv.html",
    "title": "10  Homework4: Problem 6.12",
    "section": "",
    "text": "10.0.1 Libraries\n\nimport tensorflow as tf\nimport numpy as np\nimport PIL\nimport cv2\nimport os\nimport sklearn\nimport pandas as pd\nimport pickle\nimport platform\nfrom tqdm.notebook import tqdm\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn import preprocessing\nfrom sklearn import svm\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats as st\n\n\n\n10.0.2 Computational Environment\n\nphysical_devices = tf.config.list_physical_devices('GPU')\nmy_system = platform.uname()\nprint(physical_devices)\nprint(f\"System: {my_system.system}\")\nprint(f\"Node Name: {my_system.node}\")\nprint(f\"Release: {my_system.release}\")\nprint(f\"Version: {my_system.version}\")\nprint(f\"Machine: {my_system.machine}\")\nprint(f\"Processor: {my_system.processor}\")\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nSystem: Darwin\nNode Name: client-10-229-179-166.tamulink.tamu.edu\nRelease: 21.5.0\nVersion: Darwin Kernel Version 21.5.0: Tue Apr 26 21:08:29 PDT 2022; root:xnu-8020.121.3~4/RELEASE_ARM64_T8101\nMachine: arm64\nProcessor: i386\n\n\n\n\n10.0.3 Helper function\n\ndef load_image(path, width=484, preprocess_input=tf.keras.applications.vgg16.preprocess_input):\n    \"\"\"\n    Load and Preprocessing image\n    \"\"\"\n    img = tf.keras.utils.load_img(path)\n    x = tf.keras.utils.img_to_array(img)\n    x = x[0:width,:,:]\n    x = np.expand_dims(x, axis=0)\n    return tf.keras.applications.vgg16.preprocess_input(x)\n\n\n\n10.0.4 Data inspectation\n\ndpath = os.path.join(\"data\", \"CMU-UHCS_Dataset\")\npic_path = os.path.join(dpath, \"images\")\ndf_micro = pd.read_csv( os.path.join(dpath, \"micrograph.csv\"))\ndf_micro = df_micro[[\"path\", \"primary_microconstituent\"]]\n\nfor i in range(0, len(df_micro)):\n    img_ph = os.path.join(pic_path,df_micro.iloc[i][0])\n    assert os.path.exists(img_ph)\n    df_micro.iloc[i][0] = img_ph\ndf_micro2 = df_micro.copy()\nCLS_rm = [\"pearlite+widmanstatten\", \"martensite\", \"pearlite+spheroidite\"] #(type, sample size)\n\n\nfor c in CLS_rm:\n    df_micro.drop(df_micro[df_micro[\"primary_microconstituent\"] == c].index, inplace=True)\n\n\n# labels\nname_lbs = df_micro[\"primary_microconstituent\"].unique()\nle = preprocessing.LabelEncoder()\nle.fit(name_lbs)\nlist(le.classes_)\n\n['network', 'pearlite', 'spheroidite', 'spheroidite+widmanstatten']\n\n\n\ndlabel = le.transform(df_micro[\"primary_microconstituent\"])\ndf_micro.insert(2, \"label\", dlabel)\ndf_micro\n\n\n\n\n\n  \n    \n      \n      path\n      primary_microconstituent\n      label\n    \n  \n  \n    \n      0\n      data/CMU-UHCS_Dataset/images/micrograph1.tif\n      pearlite\n      1\n    \n    \n      1\n      data/CMU-UHCS_Dataset/images/micrograph2.tif\n      spheroidite\n      2\n    \n    \n      3\n      data/CMU-UHCS_Dataset/images/micrograph5.tif\n      pearlite\n      1\n    \n    \n      4\n      data/CMU-UHCS_Dataset/images/micrograph6.tif\n      spheroidite\n      2\n    \n    \n      5\n      data/CMU-UHCS_Dataset/images/micrograph7.tif\n      spheroidite+widmanstatten\n      3\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      955\n      data/CMU-UHCS_Dataset/images/micrograph1722.tif\n      spheroidite\n      2\n    \n    \n      957\n      data/CMU-UHCS_Dataset/images/micrograph1726.tif\n      spheroidite+widmanstatten\n      3\n    \n    \n      958\n      data/CMU-UHCS_Dataset/images/micrograph1730.png\n      spheroidite\n      2\n    \n    \n      959\n      data/CMU-UHCS_Dataset/images/micrograph1731.tif\n      pearlite\n      1\n    \n    \n      960\n      data/CMU-UHCS_Dataset/images/micrograph1732.tif\n      pearlite\n      1\n    \n  \n\n791 rows × 3 columns\n\n\n\n\n\n10.0.5 Data Processing\n\n# Train-test split\ndf_test = df_micro.copy()\ndf_train = pd.DataFrame(columns = df_micro.keys())\n\nsplit_info = [(\"spheroidite\", 100),\\\n              (\"network\", 100),\\\n              (\"pearlite\", 100),\\\n              (\"spheroidite+widmanstatten\", 60)] #(type, sample size)\n\n\n\nfor ln in split_info:\n    label, n = ln\n    id_train = df_micro[df_micro[\"primary_microconstituent\"] == label][0:n].index\n    df_test.drop(id_train, axis=0, inplace=True)\n    df_train = pd.concat([df_train, df_micro.loc[id_train]])\n\n\ndf_train\n\n\n\n\n\n  \n    \n      \n      path\n      primary_microconstituent\n      label\n    \n  \n  \n    \n      1\n      data/CMU-UHCS_Dataset/images/micrograph2.tif\n      spheroidite\n      2\n    \n    \n      4\n      data/CMU-UHCS_Dataset/images/micrograph6.tif\n      spheroidite\n      2\n    \n    \n      8\n      data/CMU-UHCS_Dataset/images/micrograph10.png\n      spheroidite\n      2\n    \n    \n      9\n      data/CMU-UHCS_Dataset/images/micrograph11.tif\n      spheroidite\n      2\n    \n    \n      20\n      data/CMU-UHCS_Dataset/images/micrograph29.tif\n      spheroidite\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      596\n      data/CMU-UHCS_Dataset/images/micrograph1093.tif\n      spheroidite+widmanstatten\n      3\n    \n    \n      618\n      data/CMU-UHCS_Dataset/images/micrograph1129.tif\n      spheroidite+widmanstatten\n      3\n    \n    \n      631\n      data/CMU-UHCS_Dataset/images/micrograph1156.tif\n      spheroidite+widmanstatten\n      3\n    \n    \n      672\n      data/CMU-UHCS_Dataset/images/micrograph1218.tif\n      spheroidite+widmanstatten\n      3\n    \n    \n      673\n      data/CMU-UHCS_Dataset/images/micrograph1219.tif\n      spheroidite+widmanstatten\n      3\n    \n  \n\n360 rows × 3 columns\n\n\n\n\ndf_test\n\n\n\n\n\n  \n    \n      \n      path\n      primary_microconstituent\n      label\n    \n  \n  \n    \n      237\n      data/CMU-UHCS_Dataset/images/micrograph436.png\n      spheroidite\n      2\n    \n    \n      238\n      data/CMU-UHCS_Dataset/images/micrograph437.tif\n      spheroidite\n      2\n    \n    \n      239\n      data/CMU-UHCS_Dataset/images/micrograph440.png\n      spheroidite\n      2\n    \n    \n      241\n      data/CMU-UHCS_Dataset/images/micrograph442.tif\n      spheroidite\n      2\n    \n    \n      242\n      data/CMU-UHCS_Dataset/images/micrograph443.tif\n      spheroidite\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      955\n      data/CMU-UHCS_Dataset/images/micrograph1722.tif\n      spheroidite\n      2\n    \n    \n      957\n      data/CMU-UHCS_Dataset/images/micrograph1726.tif\n      spheroidite+widmanstatten\n      3\n    \n    \n      958\n      data/CMU-UHCS_Dataset/images/micrograph1730.png\n      spheroidite\n      2\n    \n    \n      959\n      data/CMU-UHCS_Dataset/images/micrograph1731.tif\n      pearlite\n      1\n    \n    \n      960\n      data/CMU-UHCS_Dataset/images/micrograph1732.tif\n      pearlite\n      1\n    \n  \n\n431 rows × 3 columns\n\n\n\n\n\n10.0.6 Feature Extraction\n\n# VGG16\n\nbase_model = tf.keras.applications.vgg16.VGG16(\n    include_top=False,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n\nbase_model.summary()\n\nModel: \"vgg16\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, None, None, 3)]   0         \n                                                                 \n block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n                                                                 \n block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n                                                                 \n block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n                                                                 \n block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n                                                                 \n block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n                                                                 \n=================================================================\nTotal params: 14,714,688\nTrainable params: 14,714,688\nNon-trainable params: 0\n_________________________________________________________________\n\n\nUse five layers\n\nout_layer_ns = [\"block{}_pool\".format(i) for i in range(1,6)]\nout_layer_ns\n\n['block1_pool', 'block2_pool', 'block3_pool', 'block4_pool', 'block5_pool']\n\n\n\n# Construct 5 models for feature extraction\nextmodel = dict(zip(out_layer_ns, [tf.keras.Model(\n    inputs= base_model.input,\n    outputs=base_model.get_layer(bk_name).output\n) for bk_name in out_layer_ns]))\n\nextmodel\n\n{'block1_pool': <keras.engine.functional.Functional at 0x2a7861070>,\n 'block2_pool': <keras.engine.functional.Functional at 0x16f7c2070>,\n 'block3_pool': <keras.engine.functional.Functional at 0x2a785cd90>,\n 'block4_pool': <keras.engine.functional.Functional at 0x2a7855280>,\n 'block5_pool': <keras.engine.functional.Functional at 0x2a7847190>}\n\n\n\n# Display output dimensions\nout_shapes = [extmodel[m].output_shape[-1] for m in extmodel.keys()]\nout_shapes\n\n[64, 128, 256, 512, 512]\n\n\n\n# Initiate feature maps for testing and training\nfs_train = [np.zeros((df_train.shape[0], n_f)) for n_f in out_shapes]\nfs_test = [np.zeros((df_test.shape[0], n_f)) for n_f in out_shapes]\n\nfeatures_train = dict(zip(out_layer_ns, fs_train))\nfeatures_test = dict(zip(out_layer_ns, fs_test))\n\nfeatures_train\n\n{'block1_pool': array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]),\n 'block2_pool': array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]),\n 'block3_pool': array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]),\n 'block4_pool': array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]),\n 'block5_pool': array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]])}\n\n\n\n# Feature extraction with VGG16\n#save file\npaths =  dict(zip([\"train\", \"test\"],\\\n    [os.path.join(dpath, \"feature_{}.pkl\".format(n))\\\n     for n in [\"train\", \"test\"]]))\n#if os.path.exists(os.path.join(dpath, \"feature_train.pkl\")) == False:\nfor m in tqdm(extmodel.keys()):\n    for i, df in enumerate([df_train, df_test]):\n        for j, ph in tqdm(enumerate(df[\"path\"])):\n            x = load_image(ph)\n            xb = extmodel[m].predict(x, verbose = 0) # silence output\n            F = np.mean(xb,axis=(0,1,2))\n            # Save features\n            if i ==0:\n                features_train[m][j, :] = F\n            else:\n                features_test[m][j, :] = F\n## Create new files\nf_train = open(paths[\"train\"], \"wb\")\nf_test = open(paths[\"test\"], \"wb\")\n## Write\npickle.dump(features_train, f_train)\npickle.dump(features_test, f_test)\n## Close files\nf_train.close()\nf_test.close()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.0.7 SVM\n\n# load data\nftn = open(paths[\"train\"], \"rb\")\nftt = open(paths[\"test\"], \"rb\")\nfeatn = pickle.load(ftn) # train feature\nfeatt = pickle.load(ftt) # test feature\nftn.close()\nftt.close()\n\n# label\nltrain = df_train[[\"primary_microconstituent\", \"label\"]].reset_index()\nltest = df_test[[\"primary_microconstituent\", \"label\"]].reset_index()\n\n\nltrain\n\n\n\n\n\n  \n    \n      \n      index\n      primary_microconstituent\n      label\n    \n  \n  \n    \n      0\n      1\n      spheroidite\n      2\n    \n    \n      1\n      4\n      spheroidite\n      2\n    \n    \n      2\n      8\n      spheroidite\n      2\n    \n    \n      3\n      9\n      spheroidite\n      2\n    \n    \n      4\n      20\n      spheroidite\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      596\n      spheroidite+widmanstatten\n      3\n    \n    \n      356\n      618\n      spheroidite+widmanstatten\n      3\n    \n    \n      357\n      631\n      spheroidite+widmanstatten\n      3\n    \n    \n      358\n      672\n      spheroidite+widmanstatten\n      3\n    \n    \n      359\n      673\n      spheroidite+widmanstatten\n      3\n    \n  \n\n360 rows × 3 columns\n\n\n\n\n10.0.7.1 One-to-One SVM\n\nclass One2OneSVM:\n    def __init__(self, n_class=4):\n        self.n_class = n_class\n        self.clfs_list = [svm.SVC(kernel=\"rbf\", C=1., gamma=\"auto\")\\\n                          for i in range(0,self.n_class*self.n_class)]\n        \n        self.clfs = [[self.clfs_list[self.n_class*i + j]\\\n                     for i in range(0,self.n_class)]\\\n                     for j in range(0,self.n_class)]\n        self.cv = np.zeros((self.n_class,self.n_class))\n    def train(self, ltrain, feature, fold=10):\n        # traversal all features\n        for i in range(0, self.n_class-1):\n            lis = ltrain[ltrain[\"label\"] == i].index.to_numpy()\n            for j in range(i+1, self.n_class):\n                ljs = ltrain[ltrain[\"label\"] == j].index.to_numpy()\n                # Data\n                X = np.concatenate(\\\n                  (feature[lis,:],\\\n                   feature[ljs,:]), axis=0)\n                Y = np.concatenate((np.ones(len(lis))*i,np.ones(len(ljs))*j))\n                # Train SVM\n                scores = sklearn.model_selection.cross_val_score\\\n                        (self.clfs[i][j], X, Y, cv=fold)\n                self.clfs[i][j].fit(X,Y)\n                self.cv[i][j] = 1 - np.max(scores)\n                \n    def test_1v1_error(self, ltest, feature):\n        # traversal all features\n        errM = np.zeros((self.n_class, self.n_class))\n        for i in range(0, self.n_class-1):\n            lis = ltest[ltest[\"label\"] == i].index.to_numpy()\n            for j in range(i+1, self.n_class):\n                ljs = ltest[ltest[\"label\"] == j].index.to_numpy()\n                # Data\n                X = np.concatenate(\\\n                  (feature[lis,:],\\\n                   feature[ljs,:]), axis=0)\n                Y = np.concatenate((np.ones(len(lis))*i,np.ones(len(ljs))*j))\n                # Train SVM\n                y_pred = self.clfs[i][j].predict(X)\n                errM[i,j] = error(Y, y_pred)\n        return errM\n        \n    def predict(self, feature):\n        predM = np.zeros(( int(self.n_class * (self.n_class -1)/2) , feature.shape[0]))\n        c = 0\n        for i in range(0, self.n_class-1):\n            for j in range(i+1, self.n_class):\n                predM[c,:] = self.clfs[i][j].predict(feature)\n                c += 1\n        return st.mode(predM, axis=0, keepdims=True).mode[0,:] #majority voting\n\ndef error(ans, pred):\n    assert len(ans) == len(pred)\n    return (ans != pred).sum()/float(ans.size)\n\n\n\n\n10.0.8 (a)\n\nThe convolution layer used and the cross-validated error estimate for each of the six pairwise two-label classifiers\n\nThe cross-validation error of pairwise two-label classifiers given convolution layer is shown in the following table.\n\n\n10.0.9 (b)\n\nSeparate test error rates on the unused micrographs of each of the four categories, for the pairwise two-label classifiers and the multilabel one-vs-one voting classifier described previously. For the pairwise classifiers use only the test micrographs with the two labels used to train the classifier. For the multilabel classifier, use the test micrographs with the corresponding four labels.\n\nThe empirical test error with unseen dataset is shown in the following table. The pairwise classifier was tested by the labels same as their training set. On the other hand, the multilable classifier is tested with fully four corresonding four labels.\n\ndef df_cv(m, clf, info=\"\"):\n    var1 = []\n    var2 = []\n    cvs = []\n    errs = []\n    for i in range(0, m.shape[0]-1):\n        for j in range(i+1, m.shape[0]):\n            var1.append(i)\n            var2.append(j)\n            cvs.append(clf.cv[i,j])\n            errs.append(m[i,j])\n    infos = [info] * len(errs)\n    return pd.DataFrame({\"Info\": infos, \"Label 1\": var1, \"Label 2\": var2, \"Test error\": errs,\"Cross Validation Error\": cvs})\n\n\n10.0.9.1 Pair-wise classifier\nThe final block performs best in cross validatoin score.\n\ndf_errors = []\nfor b in out_layer_ns:\n    clf1 = One2OneSVM()\n    clf1.train(ltrain, features_train[b])\n    errs = clf1.test_1v1_error(ltest, features_test[b])\n    df_errors.append(df_cv(errs, clf1, b))\n    \nres_error = pd.concat(df_errors)\nres_error\n\n\n\n\n\n  \n    \n      \n      Info\n      Label 1\n      Label 2\n      Test error\n      Cross Validation Error\n    \n  \n  \n    \n      0\n      block1_pool\n      0\n      1\n      0.823529\n      0.500\n    \n    \n      1\n      block1_pool\n      0\n      2\n      0.290155\n      0.450\n    \n    \n      2\n      block1_pool\n      0\n      3\n      0.157895\n      0.375\n    \n    \n      3\n      block1_pool\n      1\n      2\n      0.906040\n      0.500\n    \n    \n      4\n      block1_pool\n      1\n      3\n      0.466667\n      0.375\n    \n    \n      5\n      block1_pool\n      2\n      3\n      0.071186\n      0.375\n    \n    \n      0\n      block2_pool\n      0\n      1\n      0.823529\n      0.350\n    \n    \n      1\n      block2_pool\n      0\n      2\n      0.709845\n      0.350\n    \n    \n      2\n      block2_pool\n      0\n      3\n      0.157895\n      0.375\n    \n    \n      3\n      block2_pool\n      1\n      2\n      0.919463\n      0.500\n    \n    \n      4\n      block2_pool\n      1\n      3\n      0.466667\n      0.375\n    \n    \n      5\n      block2_pool\n      2\n      3\n      0.071186\n      0.375\n    \n    \n      0\n      block3_pool\n      0\n      1\n      0.823529\n      0.400\n    \n    \n      1\n      block3_pool\n      0\n      2\n      0.290155\n      0.400\n    \n    \n      2\n      block3_pool\n      0\n      3\n      0.157895\n      0.375\n    \n    \n      3\n      block3_pool\n      1\n      2\n      0.080537\n      0.450\n    \n    \n      4\n      block3_pool\n      1\n      3\n      0.466667\n      0.375\n    \n    \n      5\n      block3_pool\n      2\n      3\n      0.071186\n      0.375\n    \n    \n      0\n      block4_pool\n      0\n      1\n      0.823529\n      0.500\n    \n    \n      1\n      block4_pool\n      0\n      2\n      0.290155\n      0.450\n    \n    \n      2\n      block4_pool\n      0\n      3\n      0.157895\n      0.375\n    \n    \n      3\n      block4_pool\n      1\n      2\n      0.080537\n      0.500\n    \n    \n      4\n      block4_pool\n      1\n      3\n      0.466667\n      0.375\n    \n    \n      5\n      block4_pool\n      2\n      3\n      0.071186\n      0.375\n    \n    \n      0\n      block5_pool\n      0\n      1\n      0.073529\n      0.000\n    \n    \n      1\n      block5_pool\n      0\n      2\n      0.033679\n      0.000\n    \n    \n      2\n      block5_pool\n      0\n      3\n      0.060150\n      0.000\n    \n    \n      3\n      block5_pool\n      1\n      2\n      0.000000\n      0.000\n    \n    \n      4\n      block5_pool\n      1\n      3\n      0.088889\n      0.000\n    \n    \n      5\n      block5_pool\n      2\n      3\n      0.061017\n      0.125\n    \n  \n\n\n\n\n\n\n10.0.9.2 Multiple one-vs-one classifier\n\n# Multiclass one-vs-one\ndfm_errors = []\nfor b in out_layer_ns:\n    clf = OneVsOneClassifier(svm.SVC(kernel=\"rbf\", C=1., gamma=\"auto\").fit(features_train[b],\\\n          ltrain[\"label\"].to_numpy(int)))\n    clf.fit(features_train[b],\\\n          ltrain[\"label\"].to_numpy(int))\n    y_predm = clf.predict(features_test[b])\n    dfm_errors.append(error(y_predm, ltest[\"label\"].to_numpy()))\n\n# Display result\nres_multi1v1 = pd.DataFrame({\"Multiple One-Vs-One Classifier\": out_layer_ns, \"Test Error\": dfm_errors})\nres_multi1v1\n\n\n\n\n\n  \n    \n      \n      Multiple One-Vs-One Classifier\n      Test Error\n    \n  \n  \n    \n      0\n      block1_pool\n      0.935035\n    \n    \n      1\n      block2_pool\n      0.944316\n    \n    \n      2\n      block3_pool\n      0.364269\n    \n    \n      3\n      block4_pool\n      0.364269\n    \n    \n      4\n      block5_pool\n      0.071926\n    \n  \n\n\n\n\n\n\n\n10.0.10 (c)\n\nFor the mixed pearlite + spheroidite test micrographs, apply the trained pairwise classifier for pearlite vs. spheroidite and the multilabel voting classifier. Print the predicted labels by these two classifiers side by side (one row for each test micrograph). Comment your results\n\nThe pairwise SVM classifier performs better than Multiclass one-to-one classifier. Because the pairwise SVM is specialized for the binary problem and not be interfered with other classification setting.\n\nltestm = ltest[(ltest[\"primary_microconstituent\"] == \"pearlite\") |\\\n      (ltest[\"primary_microconstituent\"] == \"spheroidite\")]\nfeature_m = features_test[\"block5_pool\"][ltestm.index.to_numpy(), :]\nl = le.transform([\"pearlite\", \"spheroidite\"])\n\npred_pairs = clf1.clfs[l[0]][l[1]].predict(feature_m)\npred_multi = clf.predict(feature_m)\n\nres_ps = pd.DataFrame({\"Test Label\": le.inverse_transform(ltestm[\"label\"]),\\\n              \"Pairwise (pearlite vs. spheroidite)\": le.inverse_transform(pred_pairs.astype(int)),\\\n              \"Multi-OnevsOne\": le.inverse_transform(pred_multi)})\n\nprint(res_ps.to_string())\n\n      Test Label Pairwise (pearlite vs. spheroidite)             Multi-OnevsOne\n0    spheroidite                         spheroidite                spheroidite\n1    spheroidite                         spheroidite                spheroidite\n2    spheroidite                         spheroidite                spheroidite\n3    spheroidite                         spheroidite                spheroidite\n4    spheroidite                         spheroidite                spheroidite\n5    spheroidite                         spheroidite                spheroidite\n6    spheroidite                         spheroidite                spheroidite\n7    spheroidite                         spheroidite                spheroidite\n8    spheroidite                         spheroidite                spheroidite\n9    spheroidite                         spheroidite                spheroidite\n10   spheroidite                         spheroidite                spheroidite\n11   spheroidite                         spheroidite                spheroidite\n12   spheroidite                         spheroidite                spheroidite\n13   spheroidite                         spheroidite                spheroidite\n14   spheroidite                         spheroidite                spheroidite\n15   spheroidite                         spheroidite                spheroidite\n16   spheroidite                         spheroidite                spheroidite\n17   spheroidite                         spheroidite                spheroidite\n18   spheroidite                         spheroidite                spheroidite\n19   spheroidite                         spheroidite                spheroidite\n20   spheroidite                         spheroidite                spheroidite\n21   spheroidite                         spheroidite                spheroidite\n22   spheroidite                         spheroidite                spheroidite\n23   spheroidite                         spheroidite  spheroidite+widmanstatten\n24   spheroidite                         spheroidite                spheroidite\n25   spheroidite                         spheroidite                spheroidite\n26   spheroidite                         spheroidite                spheroidite\n27   spheroidite                         spheroidite  spheroidite+widmanstatten\n28   spheroidite                         spheroidite                spheroidite\n29   spheroidite                         spheroidite                spheroidite\n30   spheroidite                         spheroidite                spheroidite\n31   spheroidite                         spheroidite                spheroidite\n32   spheroidite                         spheroidite                spheroidite\n33   spheroidite                         spheroidite                spheroidite\n34   spheroidite                         spheroidite                spheroidite\n35   spheroidite                         spheroidite                spheroidite\n36   spheroidite                         spheroidite                spheroidite\n37   spheroidite                         spheroidite                spheroidite\n38   spheroidite                         spheroidite                spheroidite\n39   spheroidite                         spheroidite                spheroidite\n40   spheroidite                         spheroidite  spheroidite+widmanstatten\n41   spheroidite                         spheroidite                spheroidite\n42   spheroidite                         spheroidite                spheroidite\n43   spheroidite                         spheroidite                spheroidite\n44   spheroidite                         spheroidite                spheroidite\n45   spheroidite                         spheroidite                spheroidite\n46   spheroidite                         spheroidite                spheroidite\n47   spheroidite                         spheroidite                spheroidite\n48   spheroidite                         spheroidite                spheroidite\n49   spheroidite                         spheroidite                spheroidite\n50   spheroidite                         spheroidite                spheroidite\n51   spheroidite                         spheroidite                spheroidite\n52   spheroidite                         spheroidite                spheroidite\n53   spheroidite                         spheroidite                spheroidite\n54   spheroidite                         spheroidite                spheroidite\n55   spheroidite                         spheroidite                spheroidite\n56   spheroidite                         spheroidite                spheroidite\n57   spheroidite                         spheroidite                spheroidite\n58   spheroidite                         spheroidite                spheroidite\n59   spheroidite                         spheroidite                spheroidite\n60   spheroidite                         spheroidite                spheroidite\n61   spheroidite                         spheroidite                spheroidite\n62   spheroidite                         spheroidite                spheroidite\n63   spheroidite                         spheroidite                spheroidite\n64   spheroidite                         spheroidite                spheroidite\n65   spheroidite                         spheroidite                spheroidite\n66   spheroidite                         spheroidite                spheroidite\n67   spheroidite                         spheroidite                spheroidite\n68   spheroidite                         spheroidite                spheroidite\n69   spheroidite                         spheroidite                spheroidite\n70   spheroidite                         spheroidite                spheroidite\n71   spheroidite                         spheroidite                spheroidite\n72   spheroidite                         spheroidite                spheroidite\n73   spheroidite                         spheroidite                spheroidite\n74   spheroidite                         spheroidite                spheroidite\n75   spheroidite                         spheroidite                spheroidite\n76   spheroidite                         spheroidite                spheroidite\n77   spheroidite                         spheroidite                spheroidite\n78   spheroidite                         spheroidite                spheroidite\n79   spheroidite                         spheroidite                spheroidite\n80   spheroidite                         spheroidite                spheroidite\n81   spheroidite                         spheroidite                spheroidite\n82   spheroidite                         spheroidite                spheroidite\n83   spheroidite                         spheroidite                spheroidite\n84   spheroidite                         spheroidite                spheroidite\n85   spheroidite                         spheroidite                spheroidite\n86   spheroidite                         spheroidite                spheroidite\n87   spheroidite                         spheroidite                spheroidite\n88   spheroidite                         spheroidite                spheroidite\n89   spheroidite                         spheroidite                spheroidite\n90   spheroidite                         spheroidite  spheroidite+widmanstatten\n91   spheroidite                         spheroidite                spheroidite\n92   spheroidite                         spheroidite                spheroidite\n93   spheroidite                         spheroidite                spheroidite\n94   spheroidite                         spheroidite                spheroidite\n95   spheroidite                         spheroidite                spheroidite\n96   spheroidite                         spheroidite                spheroidite\n97   spheroidite                         spheroidite                spheroidite\n98   spheroidite                         spheroidite                spheroidite\n99   spheroidite                         spheroidite                spheroidite\n100  spheroidite                         spheroidite                spheroidite\n101  spheroidite                         spheroidite                spheroidite\n102  spheroidite                         spheroidite                spheroidite\n103  spheroidite                         spheroidite                spheroidite\n104  spheroidite                         spheroidite                spheroidite\n105  spheroidite                         spheroidite                spheroidite\n106  spheroidite                         spheroidite                spheroidite\n107  spheroidite                         spheroidite                spheroidite\n108  spheroidite                         spheroidite                spheroidite\n109  spheroidite                         spheroidite                spheroidite\n110  spheroidite                         spheroidite                spheroidite\n111  spheroidite                         spheroidite                spheroidite\n112  spheroidite                         spheroidite  spheroidite+widmanstatten\n113  spheroidite                         spheroidite                spheroidite\n114  spheroidite                         spheroidite                spheroidite\n115  spheroidite                         spheroidite                spheroidite\n116  spheroidite                         spheroidite                spheroidite\n117  spheroidite                         spheroidite                spheroidite\n118  spheroidite                         spheroidite                spheroidite\n119  spheroidite                         spheroidite                spheroidite\n120  spheroidite                         spheroidite                spheroidite\n121  spheroidite                         spheroidite                spheroidite\n122  spheroidite                         spheroidite                spheroidite\n123  spheroidite                         spheroidite                spheroidite\n124  spheroidite                         spheroidite                spheroidite\n125  spheroidite                         spheroidite                spheroidite\n126  spheroidite                         spheroidite                spheroidite\n127  spheroidite                         spheroidite                spheroidite\n128  spheroidite                         spheroidite                spheroidite\n129  spheroidite                         spheroidite                spheroidite\n130  spheroidite                         spheroidite                spheroidite\n131  spheroidite                         spheroidite                spheroidite\n132  spheroidite                         spheroidite                spheroidite\n133  spheroidite                         spheroidite                spheroidite\n134  spheroidite                         spheroidite                spheroidite\n135  spheroidite                         spheroidite                spheroidite\n136  spheroidite                         spheroidite                spheroidite\n137  spheroidite                         spheroidite                spheroidite\n138  spheroidite                         spheroidite                spheroidite\n139  spheroidite                         spheroidite                spheroidite\n140  spheroidite                         spheroidite                spheroidite\n141  spheroidite                         spheroidite                spheroidite\n142  spheroidite                         spheroidite                spheroidite\n143  spheroidite                         spheroidite                spheroidite\n144  spheroidite                         spheroidite                spheroidite\n145  spheroidite                         spheroidite                spheroidite\n146  spheroidite                         spheroidite                spheroidite\n147  spheroidite                         spheroidite                spheroidite\n148  spheroidite                         spheroidite                spheroidite\n149  spheroidite                         spheroidite                spheroidite\n150  spheroidite                         spheroidite                spheroidite\n151  spheroidite                         spheroidite                spheroidite\n152  spheroidite                         spheroidite                spheroidite\n153  spheroidite                         spheroidite                spheroidite\n154  spheroidite                         spheroidite                spheroidite\n155  spheroidite                         spheroidite                spheroidite\n156  spheroidite                         spheroidite                spheroidite\n157  spheroidite                         spheroidite                spheroidite\n158  spheroidite                         spheroidite                spheroidite\n159  spheroidite                         spheroidite                spheroidite\n160  spheroidite                         spheroidite                spheroidite\n161  spheroidite                         spheroidite                spheroidite\n162  spheroidite                         spheroidite                spheroidite\n163  spheroidite                         spheroidite                spheroidite\n164  spheroidite                         spheroidite                spheroidite\n165  spheroidite                         spheroidite                spheroidite\n166  spheroidite                         spheroidite                    network\n167  spheroidite                         spheroidite                spheroidite\n168  spheroidite                         spheroidite                spheroidite\n169  spheroidite                         spheroidite                spheroidite\n170  spheroidite                         spheroidite                spheroidite\n171  spheroidite                         spheroidite                spheroidite\n172  spheroidite                         spheroidite                spheroidite\n173  spheroidite                         spheroidite                spheroidite\n174  spheroidite                         spheroidite                spheroidite\n175  spheroidite                         spheroidite                spheroidite\n176  spheroidite                         spheroidite                spheroidite\n177  spheroidite                         spheroidite                spheroidite\n178  spheroidite                         spheroidite  spheroidite+widmanstatten\n179  spheroidite                         spheroidite                spheroidite\n180  spheroidite                         spheroidite                spheroidite\n181  spheroidite                         spheroidite                spheroidite\n182  spheroidite                         spheroidite                spheroidite\n183  spheroidite                         spheroidite                spheroidite\n184  spheroidite                         spheroidite                spheroidite\n185  spheroidite                         spheroidite                spheroidite\n186  spheroidite                         spheroidite                spheroidite\n187  spheroidite                         spheroidite                spheroidite\n188  spheroidite                         spheroidite                spheroidite\n189  spheroidite                         spheroidite                spheroidite\n190  spheroidite                         spheroidite  spheroidite+widmanstatten\n191  spheroidite                         spheroidite                spheroidite\n192  spheroidite                         spheroidite                spheroidite\n193  spheroidite                         spheroidite                spheroidite\n194  spheroidite                         spheroidite                spheroidite\n195  spheroidite                         spheroidite                spheroidite\n196  spheroidite                         spheroidite                spheroidite\n197  spheroidite                         spheroidite                spheroidite\n198  spheroidite                         spheroidite                spheroidite\n199  spheroidite                         spheroidite                spheroidite\n200  spheroidite                         spheroidite                spheroidite\n201  spheroidite                         spheroidite                spheroidite\n202  spheroidite                         spheroidite                spheroidite\n203  spheroidite                         spheroidite                spheroidite\n204  spheroidite                         spheroidite                spheroidite\n205  spheroidite                         spheroidite                spheroidite\n206  spheroidite                         spheroidite                spheroidite\n207  spheroidite                         spheroidite                spheroidite\n208  spheroidite                         spheroidite                spheroidite\n209  spheroidite                         spheroidite                spheroidite\n210  spheroidite                         spheroidite                spheroidite\n211  spheroidite                         spheroidite                spheroidite\n212  spheroidite                         spheroidite                spheroidite\n213  spheroidite                         spheroidite                spheroidite\n214  spheroidite                         spheroidite                spheroidite\n215  spheroidite                         spheroidite                spheroidite\n216  spheroidite                         spheroidite                spheroidite\n217  spheroidite                         spheroidite                spheroidite\n218  spheroidite                         spheroidite  spheroidite+widmanstatten\n219     pearlite                            pearlite                   pearlite\n220  spheroidite                         spheroidite                spheroidite\n221  spheroidite                         spheroidite                spheroidite\n222  spheroidite                         spheroidite                spheroidite\n223     pearlite                            pearlite                   pearlite\n224     pearlite                            pearlite                   pearlite\n225  spheroidite                         spheroidite                spheroidite\n226  spheroidite                         spheroidite                spheroidite\n227     pearlite                            pearlite                   pearlite\n228  spheroidite                         spheroidite                spheroidite\n229  spheroidite                         spheroidite                spheroidite\n230  spheroidite                         spheroidite                spheroidite\n231  spheroidite                         spheroidite                spheroidite\n232  spheroidite                         spheroidite                spheroidite\n233  spheroidite                         spheroidite                    network\n234  spheroidite                         spheroidite                spheroidite\n235  spheroidite                         spheroidite  spheroidite+widmanstatten\n236  spheroidite                         spheroidite                spheroidite\n237  spheroidite                         spheroidite                spheroidite\n238     pearlite                            pearlite                   pearlite\n239     pearlite                            pearlite                   pearlite\n240  spheroidite                         spheroidite                spheroidite\n241     pearlite                            pearlite                   pearlite\n242     pearlite                            pearlite                   pearlite\n243  spheroidite                         spheroidite                spheroidite\n244  spheroidite                         spheroidite                spheroidite\n245  spheroidite                         spheroidite                spheroidite\n246  spheroidite                         spheroidite                spheroidite\n247  spheroidite                         spheroidite                spheroidite\n248  spheroidite                         spheroidite                spheroidite\n249  spheroidite                         spheroidite                spheroidite\n250  spheroidite                         spheroidite                spheroidite\n251  spheroidite                         spheroidite                spheroidite\n252     pearlite                            pearlite                   pearlite\n253     pearlite                            pearlite                   pearlite\n254     pearlite                            pearlite                   pearlite\n255     pearlite                            pearlite                   pearlite\n256     pearlite                            pearlite                   pearlite\n257  spheroidite                         spheroidite                spheroidite\n258  spheroidite                         spheroidite                spheroidite\n259  spheroidite                         spheroidite                spheroidite\n260  spheroidite                         spheroidite                spheroidite\n261  spheroidite                         spheroidite                spheroidite\n262  spheroidite                         spheroidite                spheroidite\n263  spheroidite                         spheroidite                spheroidite\n264  spheroidite                         spheroidite                spheroidite\n265  spheroidite                         spheroidite                spheroidite\n266  spheroidite                         spheroidite                spheroidite\n267     pearlite                            pearlite                   pearlite\n268     pearlite                            pearlite                   pearlite\n269  spheroidite                         spheroidite                spheroidite\n270     pearlite                            pearlite                   pearlite\n271  spheroidite                         spheroidite                spheroidite\n272  spheroidite                         spheroidite                spheroidite\n273  spheroidite                         spheroidite                spheroidite\n274     pearlite                            pearlite                   pearlite\n275     pearlite                            pearlite                   pearlite\n276  spheroidite                         spheroidite                spheroidite\n277     pearlite                            pearlite                   pearlite\n278  spheroidite                         spheroidite                spheroidite\n279  spheroidite                         spheroidite                spheroidite\n280  spheroidite                         spheroidite                spheroidite\n281  spheroidite                         spheroidite                spheroidite\n282  spheroidite                         spheroidite                spheroidite\n283  spheroidite                         spheroidite                spheroidite\n284  spheroidite                         spheroidite                spheroidite\n285  spheroidite                         spheroidite                spheroidite\n286     pearlite                            pearlite                   pearlite\n287     pearlite                            pearlite                   pearlite\n288  spheroidite                         spheroidite                spheroidite\n289  spheroidite                         spheroidite                spheroidite\n290  spheroidite                         spheroidite                spheroidite\n291  spheroidite                         spheroidite                spheroidite\n292  spheroidite                         spheroidite                spheroidite\n293     pearlite                            pearlite                   pearlite\n294  spheroidite                         spheroidite                spheroidite\n295  spheroidite                         spheroidite                spheroidite\n296     pearlite                            pearlite                   pearlite\n297     pearlite                            pearlite                   pearlite\n\n\n\n\n10.0.11 (d)\n\nNow apply the multilabel classifier on the pearlite + Widmanst¨atten and martensite micrographs and print the predicted labels. Compare to the results in part (c)\n\nThere is no specific relation for these unseen datasets. The prediction can not extrapolate, and (c) has preferred prediction accuracy and consistency.\n\ndf_micro2 = df_micro2[(df_micro2[\"primary_microconstituent\"] == \"pearlite+widmanstatten\") |\\\n(df_micro2[\"primary_microconstituent\"] == \"martensite\")]\n\n# Encode labels\nle2 = preprocessing.LabelEncoder()\nle2.fit(df_micro2[\"primary_microconstituent\"].unique())\nlist(le2.classes_)\n\n['martensite', 'pearlite+widmanstatten']\n\n\n\ndlabel2 = le2.transform(df_micro2[\"primary_microconstituent\"])\ndf_micro2.insert(2, \"label\", dlabel2)\n\n\ndf_micro2\n\n\n\n\n\n  \n    \n      \n      path\n      primary_microconstituent\n      label\n    \n  \n  \n    \n      15\n      data/CMU-UHCS_Dataset/images/micrograph20.tif\n      martensite\n      0\n    \n    \n      29\n      data/CMU-UHCS_Dataset/images/micrograph41.tif\n      martensite\n      0\n    \n    \n      31\n      data/CMU-UHCS_Dataset/images/micrograph44.tif\n      martensite\n      0\n    \n    \n      63\n      data/CMU-UHCS_Dataset/images/micrograph99.tif\n      martensite\n      0\n    \n    \n      71\n      data/CMU-UHCS_Dataset/images/micrograph114.tif\n      martensite\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      892\n      data/CMU-UHCS_Dataset/images/micrograph1599.tif\n      martensite\n      0\n    \n    \n      936\n      data/CMU-UHCS_Dataset/images/micrograph1684.tif\n      pearlite+widmanstatten\n      1\n    \n    \n      942\n      data/CMU-UHCS_Dataset/images/micrograph1697.tif\n      martensite\n      0\n    \n    \n      944\n      data/CMU-UHCS_Dataset/images/micrograph1700.tif\n      martensite\n      0\n    \n    \n      956\n      data/CMU-UHCS_Dataset/images/micrograph1723.tif\n      martensite\n      0\n    \n  \n\n63 rows × 3 columns\n\n\n\n\n# Feature extraction with VGG16\nif os.path.exists(os.path.join(dpath, \"feature_test2.pkl\")) == False:\n    fs_test2 = np.zeros((df_micro2.shape[0], out_shapes[-1]))\n    m = \"block5_pool\"\n    for j, ph in tqdm(enumerate(df_micro2[\"path\"])):\n        x = load_image(ph)\n        xb = extmodel[m].predict(x, verbose = 0) # silence output\n        F = np.mean(xb,axis=(0,1,2))\n        # Save features\n        fs_test2[j, :] = F\n\n    # Save data\n    ## Create new files\n    fs_test2_p = open(os.path.join(dpath, \"feature_test2.pkl\"), \"wb\")\n    ## Write\n    pickle.dump(fs_test2, fs_test2_p)\n    ## Close files\n    fs_test2_p.close()\n\n\n#load data\nfs_test2_p  = open(os.path.join(dpath, \"feature_test2.pkl\"), \"rb\")\nfs_test2 = pickle.load(fs_test2_p) # train feature\nfs_test2_p .close()\n\n\npred_multi2 = clf.predict(fs_test2)\n\nres_ps2 = pd.DataFrame({\"Test Label\": le2.inverse_transform(df_micro2[\"label\"]),\\\n              \"Multi-OnevsOne\": le.inverse_transform(pred_multi2)})\n\nprint(res_ps2.to_string())\n\n                Test Label             Multi-OnevsOne\n0               martensite                spheroidite\n1               martensite                    network\n2               martensite                   pearlite\n3               martensite                spheroidite\n4               martensite                spheroidite\n5               martensite                    network\n6               martensite                spheroidite\n7   pearlite+widmanstatten                   pearlite\n8               martensite                   pearlite\n9               martensite                spheroidite\n10              martensite                spheroidite\n11  pearlite+widmanstatten                   pearlite\n12              martensite                   pearlite\n13  pearlite+widmanstatten                   pearlite\n14              martensite                   pearlite\n15  pearlite+widmanstatten                spheroidite\n16  pearlite+widmanstatten  spheroidite+widmanstatten\n17  pearlite+widmanstatten                   pearlite\n18              martensite                   pearlite\n19  pearlite+widmanstatten                spheroidite\n20  pearlite+widmanstatten                   pearlite\n21  pearlite+widmanstatten                spheroidite\n22  pearlite+widmanstatten                spheroidite\n23  pearlite+widmanstatten                   pearlite\n24  pearlite+widmanstatten                   pearlite\n25              martensite                   pearlite\n26              martensite                spheroidite\n27              martensite                   pearlite\n28              martensite                spheroidite\n29              martensite                   pearlite\n30              martensite                spheroidite\n31              martensite                   pearlite\n32  pearlite+widmanstatten                   pearlite\n33              martensite                   pearlite\n34              martensite                spheroidite\n35  pearlite+widmanstatten                spheroidite\n36              martensite                spheroidite\n37  pearlite+widmanstatten                spheroidite\n38  pearlite+widmanstatten                   pearlite\n39  pearlite+widmanstatten                   pearlite\n40              martensite                   pearlite\n41              martensite                spheroidite\n42  pearlite+widmanstatten                   pearlite\n43  pearlite+widmanstatten                spheroidite\n44  pearlite+widmanstatten  spheroidite+widmanstatten\n45  pearlite+widmanstatten                   pearlite\n46  pearlite+widmanstatten                   pearlite\n47              martensite                   pearlite\n48  pearlite+widmanstatten                   pearlite\n49              martensite                   pearlite\n50  pearlite+widmanstatten  spheroidite+widmanstatten\n51  pearlite+widmanstatten                   pearlite\n52              martensite                   pearlite\n53  pearlite+widmanstatten                spheroidite\n54              martensite                spheroidite\n55              martensite                spheroidite\n56              martensite                   pearlite\n57              martensite                    network\n58              martensite                spheroidite\n59  pearlite+widmanstatten                   pearlite\n60              martensite                spheroidite\n61              martensite                   pearlite\n62              martensite                spheroidite"
  },
  {
    "objectID": "hw/hw5.html#description",
    "href": "hw/hw5.html#description",
    "title": "11  Homework 5",
    "section": "11.1 Description",
    "text": "11.1 Description\n\nProblems from the Book\n9.8\n11.11\nBoth problems are coding assignments, with starting code provided. Each is worth 40 points."
  },
  {
    "objectID": "hw/hw5.html#problem-9.8",
    "href": "hw/hw5.html#problem-9.8",
    "title": "11  Homework 5",
    "section": "11.2 Problem 9.8",
    "text": "11.2 Problem 9.8\n\nThis assignment concerns the application of PCA to the soft magnetic alloy data set (See section A8.5).\n\n\n11.2.1 (a)\n\nReproduce the plots in Figure 9.5 by running c09_PCA.py\n\n\n\"\"\"\"\nFoundations of Pattern Recognition and Machine Learning\nChapter 9 Figure 9.5\nAuthor: Ulisses Braga-Neto\n\nPCA example using the softt magnetic alloy dataset\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler as ssc\n\n\n# Fix random state for reproducibility\nnp.random.seed(0)\n\nSMA = pd.read_csv('data/Soft_Magnetic_Alloy_Dataset.csv')\n\nfn0 = SMA.columns[0:26]         # all feature names\nfv0 = SMA.values[:,0:26]        # all feature values\nrs0 = SMA['Coercivity (A/m)']   # select response\n\n\n# pre-process the data\nn_orig   = fv0.shape[0]                 # original number of training points\np_orig   = np.sum(fv0>0,axis=0)/n_orig  # fraction of nonzero components for each feature\nnoMS     = p_orig>0.05\nfv1      = fv0[:,noMS]            # drop features with less than 5% nonzero components\nnoNA     = np.invert(np.isnan(rs0))     # find available response values\nSMA_feat = fv1[noNA,:]                  # filtered feature values\nSMA_fnam = fn0[noMS]                    # filtered feature names\nSMA_resp = rs0[noNA]                    # filtered response values\n\nn,d = SMA_feat.shape # filtered data dimensions\n\n# add random perturbation to the features\nsg = 2\nSMA_feat_ns = SMA_feat + np.random.normal(0,sg,[n,d])\nSMA_feat_ns = (SMA_feat_ns + abs(SMA_feat_ns))/2 # clamp values at zero\n\n# standardize data\nSMA_feat_std = ssc().fit_transform(SMA_feat_ns)\n\n# compute PCA\npca = PCA()\npr = pca.fit_transform(SMA_feat_std)\n\n# PCA plots\ndef plot_PCA(X,Y,resp,thrs,nam1,nam2):\n    Ihigh = resp>thrs[1]\n    Imid = (resp>thrs[0])&(resp<=thrs[1])\n    Ilow = resp<=thrs[0]  \n    plt.xlabel(nam1+' principal component',fontsize=16)\n    plt.ylabel(nam2+' principal component',fontsize=16)\n    plt.scatter(X[Ilow],Y[Ilow],c='blue',s=16,marker='o',label='Low')\n    plt.scatter(X[Imid],Y[Imid],c='green',s=16,marker='o',label='Mid')\n    plt.scatter(X[Ihigh],Y[Ihigh],c='red',s=16,marker='o',label='High')\n    plt.xticks(size='medium')\n    plt.yticks(size='medium')\n    plt.legend(fontsize=14,facecolor='white',markerscale=2,markerfirst=False,handletextpad=0)\n    plt.show()\n\nfig=plt.figure(figsize=(8,8))#,dpi=150)\nplot_PCA(pr[:,0],pr[:,1],SMA_resp,[2,8],'First','Second')\nfig=plt.figure(figsize=(8,8))#,dpi=150)\nplot_PCA(pr[:,0],pr[:,2],SMA_resp,[2,8],'First','Third')\nfig=plt.figure(figsize=(8,8))#,dpi=150)\nplot_PCA(pr[:,1],pr[:,2],SMA_resp,[2,8],'Second','Third')\n\n\n\n\n\n\n\n\n\n\n\n\n11.2.2 (b)\n\nPlot the percentage of variance explained by each PC as a function of PC number. This is called the scree plot. Now plot the cumulative percentage of variance explained by the PCs as a function of PC number. How many PCs are needed to explain \\(95\\%\\) of the variance.\nCoding hint: use the attribute explained_variance_ratio_ and the cumsum() method.\n\n\nn = len(pca.explained_variance_ratio_)\n#plt.plot( np.arange(1, n+1) ,np.cumsum(pca.explained_variance_ratio_))\nplt.plot( np.arange(1, n+1) ,pca.explained_variance_ratio_, \"-o\")\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Cumulative ratio of explained variance\")\nplt.title(\"Scree plot\")\n\nText(0.5, 1.0, 'Scree plot')\n\n\n\n\n\n\nnp.cumsum(pca.explained_variance_ratio_)\n\narray([0.20991838, 0.32353309, 0.42210314, 0.51647789, 0.6047665 ,\n       0.68519162, 0.76168501, 0.83369708, 0.89706386, 0.94621316,\n       0.98369719, 1.        ])\n\n\n\nnp.where(np.cumsum(pca.explained_variance_ratio_) > 0.95)[0][0] + 1\n\n11\n\n\n\n\n\n\n\n11 PCs are needed to explain 95% of the variance.\n\n\n\n\n11.2.3 (c)\n\nPrint the loading matrix \\(W\\) (this is the matrix of eigenvectors, ordered by PC number from left to right). The absolute value of the coefficients indicate the relative importance of each original variable (row of \\(W\\)) in the corresponding PC (column of \\(W\\)). 1\n\n\n#for i in range(0,n):\n#    print(\"Component\", i+1,\":\",pca.components_[i].T)\n    \nwd = pd.DataFrame(pca.components_.T * np.sqrt(pca.explained_variance_ratio_), columns = [\"PC{}\".format(i+1) for i in range(0,n)])\nwd.iloc[:,0:6]\n\n\n\n\n\n  \n    \n      \n      PC1\n      PC2\n      PC3\n      PC4\n      PC5\n      PC6\n    \n  \n  \n    \n      0\n      0.217429\n      -0.128414\n      -0.000768\n      -0.013032\n      0.025015\n      -0.018742\n    \n    \n      1\n      -0.231749\n      0.065983\n      -0.014794\n      -0.089591\n      -0.010017\n      0.005178\n    \n    \n      2\n      0.032908\n      0.166780\n      0.078030\n      0.083952\n      -0.009801\n      -0.005831\n    \n    \n      3\n      -0.093151\n      -0.142560\n      -0.098557\n      0.086218\n      0.086567\n      -0.076516\n    \n    \n      4\n      0.198563\n      0.135102\n      -0.001895\n      -0.026140\n      0.032221\n      0.055660\n    \n    \n      5\n      -0.026492\n      0.008521\n      -0.036036\n      0.214858\n      -0.108705\n      -0.008577\n    \n    \n      6\n      -0.017498\n      -0.014458\n      -0.162399\n      -0.104232\n      -0.065114\n      0.042457\n    \n    \n      7\n      0.052591\n      -0.099443\n      0.151379\n      -0.036285\n      -0.161591\n      -0.083255\n    \n    \n      8\n      -0.157455\n      0.021118\n      0.011608\n      0.081410\n      -0.001242\n      0.047744\n    \n    \n      9\n      -0.041245\n      0.059526\n      0.063562\n      -0.019442\n      0.142836\n      -0.178096\n    \n    \n      10\n      -0.010472\n      -0.101413\n      0.114606\n      0.041513\n      0.123522\n      0.164500\n    \n    \n      11\n      -0.171381\n      -0.043990\n      0.121028\n      -0.062770\n      -0.033236\n      0.034567\n    \n  \n\n\n\n\n\nwd.iloc[:,6:]\n\n\n\n\n\n  \n    \n      \n      PC7\n      PC8\n      PC9\n      PC10\n      PC11\n      PC12\n    \n  \n  \n    \n      0\n      -0.015495\n      -0.005038\n      0.051062\n      -0.046767\n      -0.091825\n      -0.070203\n    \n    \n      1\n      -0.023988\n      -0.035185\n      -0.063599\n      0.058694\n      -0.014704\n      -0.085907\n    \n    \n      2\n      -0.096659\n      0.160212\n      -0.048367\n      -0.038402\n      -0.047184\n      -0.010801\n    \n    \n      3\n      -0.098246\n      0.066477\n      -0.030437\n      -0.034888\n      0.084873\n      -0.020465\n    \n    \n      4\n      0.017248\n      -0.016361\n      0.043242\n      -0.027380\n      0.124907\n      -0.045211\n    \n    \n      5\n      0.141188\n      -0.028350\n      -0.034109\n      -0.016461\n      0.001849\n      -0.032101\n    \n    \n      6\n      0.098092\n      0.169961\n      0.031244\n      -0.005649\n      -0.004468\n      -0.001099\n    \n    \n      7\n      -0.013892\n      0.063737\n      0.015561\n      0.073965\n      0.057188\n      -0.012833\n    \n    \n      8\n      -0.047099\n      0.012760\n      0.213021\n      0.033689\n      -0.007880\n      -0.009804\n    \n    \n      9\n      0.134329\n      0.040100\n      0.041069\n      0.012429\n      -0.005979\n      -0.004630\n    \n    \n      10\n      0.070125\n      0.069752\n      -0.044561\n      0.062093\n      0.007589\n      -0.009022\n    \n    \n      11\n      0.038157\n      -0.002185\n      0.012125\n      -0.170754\n      0.018848\n      -0.004422\n    \n  \n\n\n\n\n\n\n11.2.4 (d)\n\nIdentify which two features contribute the most to the discriminating first PC and plot the data using these top two features. What can you conclude about the effect of these two features on the coercivity? This is an application of PCA to feature selection.\n\n\nmost_id = np.abs(wd[\"PC1\"]).argsort()[-2:].to_numpy()\n\n\nmost_id\n\narray([0, 1])\n\n\n\nthrs = [2, 8]\nid_d = {\n    \"high\": SMA_resp>thrs[1],\n    \"mid\": (SMA_resp>thrs[0])&(SMA_resp<=thrs[1]),\n    \"low\": SMA_resp<=thrs[0]  \n}\ncs = [\"blue\", \"green\", \"red\"]\n\nfor i, lb in enumerate([\"low\", \"mid\", \"high\"]):\n    plt.scatter(SMA_feat_std[id_d[lb], most_id[1]], SMA_feat_std[id_d[lb], most_id[0]],\\\n               c= cs[i],s=16,marker='o',label=lb)\nplt.legend(fontsize=14,facecolor='white',markerscale=2,markerfirst=False,handletextpad=0)\nplt.xticks(size='medium');\nplt.yticks(size='medium');\nplt.xlabel(SMA_fnam[most_id[1]])\nplt.ylabel(SMA_fnam[most_id[0]])\n\nText(0, 0.5, 'Fe')\n\n\n\n\n\nRelations 1. High: -1 ~ 2 in feature 1; -2 ~ 2 in feature 0 2. Mid: -1 ~ 0 in feature 1; -1 ~ 2 in feature 0 3. low: Approximately locates around -1 ~ 2 in feature 1, -3 ~ 1 in feature 0\nThere values are refferred to the normalized units."
  },
  {
    "objectID": "hw/hw5.html#problem-11.11",
    "href": "hw/hw5.html#problem-11.11",
    "title": "11  Homework 5",
    "section": "11.3 Problem 11.11",
    "text": "11.3 Problem 11.11\n\nApply linear regression to the stacking fault energy (SFE) data set.\n\n\n11.3.1 (a)\n\nModify c11_SFE.py to fit a univariate linear regression model (with intercept) separately to each of the seven variables remaining after preprocessing (two of these were already done in Example 11.4. List the fitted coefficients, the normalized RSS, and the \\(R^2\\) statsitic for each model.\nWhich one of the seven variables is the best predictor of SFE, according to \\(R^2\\)? Plot the SFE response against each of the seven variables, with regression lines superimposed. How do you interpret these results?\n\n\n\"\"\"\"\nFoundations of Pattern Recognition and Machine Learning\nChapter 11 Figure 11.3\nAuthor: Ulisses Braga-Neto\n\nRegression with a line example with stacking fault energy dataset\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.base import clone\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nplt.style.use('seaborn')\n\nMatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn')\n\n\n\ndef r2(model, x, y):\n    y_pred = model.predict(x)\n    return r2_score(y, y_pred)\n\ndef fitted_coef(model):\n    return model.coef_[0]\n\ndef fitted_int(model):\n    return model.intercept_\n\ndef normalized_RSS(model, xrr, yr):\n    y_pred = model.predict(xrr)\n    rss = np.sum(np.square(y_pred - yr)) / len(y_pred)\n    return rss\n\n\nSFE_orig = pd.read_table('data/Stacking_Fault_Energy_Dataset.txt')\n\n\n# pre-process the data\n\nn_orig = SFE_orig.shape[0]          # original number of rows\np_orig = np.sum(SFE_orig>0)/n_orig  # fraction of nonzero entries for each column\nSFE_colnames = SFE_orig.columns[p_orig>0.6]\nSFE_col = SFE_orig[SFE_colnames]        # throw out columns with fewer than 60% nonzero entries\nm_col = np.prod(SFE_col,axis=1)\nSFE = SFE_col.iloc[np.nonzero(m_col.to_numpy())] # throw out rows that contain any zero entries\n\n\nyr = SFE['SFE']\nmodel = LinearRegression()\nres = {\n    \"feature\": [],\n    \"slope\": [],\n    \"intercept\": [],\n    \"Norm RSS\":[],\n    \"R2\": []\n}\n\nfor feat in SFE.keys()[:-1].to_numpy():\n    xr = np.array(SFE[feat])\n    xrr = xr.reshape((-1,1)) # format xr for Numpy regression code   \n    model.fit(xrr,yr)\n    \n    # Performance\n    res[\"feature\"].append(feat)\n    res[\"slope\"].append(fitted_coef(model))\n    res[\"intercept\"].append(fitted_int(model))\n    res[\"Norm RSS\"].append(normalized_RSS(model, xrr, yr))\n    res[\"R2\"].append(r2(model, xrr, yr))\n    \n    # Plotting\n    fig=plt.figure(figsize=(8,6),dpi=150)\n    plt.xlabel(feat,size=24)\n    plt.ylabel('SFE',size=24)\n    plt.xticks(size=20)\n    plt.yticks(size=20)\n    plt.scatter(xr,yr,s=32,marker='o',facecolor='none',edgecolor='r',linewidth=1.5)\n    ## Plotting regression model\n    plt.plot(xrr,model.predict(xrr),c='green',lw=2)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npd.DataFrame(res)\n\n\n\n\n\n  \n    \n      \n      feature\n      slope\n      intercept\n      Norm RSS\n      R2\n    \n  \n  \n    \n      0\n      C\n      -16.200376\n      39.658087\n      167.480699\n      0.006946\n    \n    \n      1\n      N\n      20.088487\n      37.447183\n      162.756200\n      0.034959\n    \n    \n      2\n      Ni\n      2.335338\n      10.324642\n      84.669120\n      0.497966\n    \n    \n      3\n      Fe\n      -1.440431\n      134.100015\n      100.297936\n      0.405297\n    \n    \n      4\n      Mn\n      0.345602\n      38.236325\n      167.230592\n      0.008429\n    \n    \n      5\n      Si\n      -3.520817\n      39.959897\n      167.129698\n      0.009027\n    \n    \n      6\n      Cr\n      0.521318\n      29.878868\n      167.428848\n      0.007254\n    \n  \n\n\n\n\nAccording to \\(R^2\\), Ni is the best predictor that has positive linear relation between SFE. Also, Fe has high correlation coefficient. The rest of the features performs low correlation between SFE. As we can see in the linear regression plots.\n\n\n11.3.2 (b)\n\nPerform multivariate linear regression with a linear forward wrapper search (for 1 to 5 variables) using the \\(R^2\\) statistic as the search criterion. List the normalized RSS, the \\(R^2\\) statistic, and the adjusted \\(R^2\\) statistic for each model. Which would be the most predictive model according to adjusted \\(R^2\\)? How do you compare these results with those of item (a)?\n\nUse the adjusted \\(R^2\\) formula:\n\\[R^{2}_{adj} = 1 - \\frac{(1-R^2)(n-1)}{n-k-1}\\]\nRef: Sequential forward search https://vitalflux.com/sequential-forward-selection-python-example/\n\ndef adr2(model, x, y):\n    r = r2(model,x,y)\n    N = x.shape[0]\n    p = x.shape[1]\n    return 1 - (1-r)*(N-1)/(N-p-1)\n\nclass SequentialForwardSearch():\n    def __init__(self, clf):\n        self.clf = clone(clf)\n    \n    def search(self, xs, ys):\n        unchosen_ids = list(range(xs.shape[1]))\n        chosen_ids = []\n        self.scores = []\n        self.subset = []\n        self.r2 = []\n        self.rss = []\n        self.adr2 = []\n        \n        while len(unchosen_ids) > 0:\n            scrs = [] #scores record\n            \n            for i in unchosen_ids:\n                subset = chosen_ids + [i]\n                scrs.append( self._score(xs[:, subset], ys))\n            \n            \n            best_i = np.argmax(scrs) #best base\n            chosen_ids.append(unchosen_ids.pop(best_i)) # swap\n            \n            # Rcord\n            self.scores.append(scrs[best_i])\n            \n            # Performance\n            self.clf.fit(xs[:, chosen_ids], ys)\n            self.r2.append(r2(self.clf, xs[:, chosen_ids], ys))\n            self.rss.append(normalized_RSS(self.clf, xs[:, chosen_ids], ys))\n            self.adr2.append(adr2(self.clf, xs[:, chosen_ids], ys))\n            \n        self.subset = chosen_ids\n                \n    def _score(self, xs, ys):\n        self.clf.fit(xs, ys)\n        return r2(self.clf, xs, ys)\n        \n\n\n# data\nxr = SFE.drop(\"SFE\", axis=1).to_numpy()[:, 0:5]\nyr = SFE['SFE']\n\n# Search\nsh = SequentialForwardSearch(LinearRegression())\nsh.search(xr, yr)\nids_sh = sh.subset\n\n## Performance\nfts = [ str([res[\"feature\"][j] for j in sh.subset[0:i+1]]) for i in range(0, len(sh.subset))] #features\npd.DataFrame({\"Feature\": fts, \"R2\": sh.r2, \"Norm RSS\": sh.rss, \"Adjust R2\": sh.adr2})\n\n\n\n\n\n  \n    \n      \n      Feature\n      R2\n      Norm RSS\n      Adjust R2\n    \n  \n  \n    \n      0\n      ['Ni']\n      0.497966\n      84.669120\n      0.495564\n    \n    \n      1\n      ['Ni', 'N']\n      0.553514\n      75.300860\n      0.549221\n    \n    \n      2\n      ['Ni', 'N', 'C']\n      0.572860\n      72.038072\n      0.566670\n    \n    \n      3\n      ['Ni', 'N', 'C', 'Fe']\n      0.579493\n      70.919455\n      0.571328\n    \n    \n      4\n      ['Ni', 'N', 'C', 'Fe', 'Mn']\n      0.579832\n      70.862215\n      0.569584\n    \n  \n\n\n\n\nIf solely depends on \\(R^2\\), model with \\(5\\) is the best option. However, after adding regularization term with adjusted \\(R^2\\), model with \\(4\\) features is the best."
  },
  {
    "objectID": "final_project.html#predicting-stock-market-with-bayesian-neural-network",
    "href": "final_project.html#predicting-stock-market-with-bayesian-neural-network",
    "title": "12  Abstract",
    "section": "12.1 Predicting Stock Market with Bayesian Neural Network",
    "text": "12.1 Predicting Stock Market with Bayesian Neural Network\nThe randomness of the stock market challenge investments to be reliable. Many approaches have been introduced to find the hidden pattern behind the transitions. However, error estimation with the non-parametric method is in the early stage. In this project, we used a Bayesian neural network to predict discrete time-series data with Taken’s embedding theorem. In this project, The Unadjusted Langevin Monte Carlo, Metropolis-adjusted Langevin algorithm (MALA) and Hamiltonian Monte Carlo (HMC) are applied to measure the posterior distribution. We found that Langevin method performed best for the log likelihood optimization. The purpose of this project is to provide a model-free approach with uncertainty quantification that is essential to the investment strategy"
  },
  {
    "objectID": "final_project.html#reports",
    "href": "final_project.html#reports",
    "title": "12  Abstract",
    "section": "12.2 Reports",
    "text": "12.2 Reports\n\nProposal: PDF\nCheckpoint: PDF\nFinal Report: PDF"
  },
  {
    "objectID": "final_project/script/HMC_Copy_of_EXP_fit_time_window.html#fit-sliding-window-of-stock-markets",
    "href": "final_project/script/HMC_Copy_of_EXP_fit_time_window.html#fit-sliding-window-of-stock-markets",
    "title": "13  Predicting Stock Market with Bayesian Nerual Network",
    "section": "13.1 Fit sliding window of stock markets",
    "text": "13.1 Fit sliding window of stock markets"
  },
  {
    "objectID": "final_project/script/HMC_Copy_of_EXP_fit_time_window.html#installing-packages",
    "href": "final_project/script/HMC_Copy_of_EXP_fit_time_window.html#installing-packages",
    "title": "13  Predicting Stock Market with Bayesian Nerual Network",
    "section": "13.2 Installing packages",
    "text": "13.2 Installing packages\n\n!pip install git+https://github.com/deepmind/dm-haiku\n!pip install git+https://github.com/jamesvuc/jax-bayes\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting git+https://github.com/deepmind/dm-haiku\n  Cloning https://github.com/deepmind/dm-haiku to /tmp/pip-req-build-roptddu5\n  Running command git clone -q https://github.com/deepmind/dm-haiku /tmp/pip-req-build-roptddu5\nRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from dm-haiku==0.0.10.dev0) (1.3.0)\nCollecting jmp>=0.0.2\n  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\nRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from dm-haiku==0.0.10.dev0) (1.21.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from dm-haiku==0.0.10.dev0) (0.8.10)\nBuilding wheels for collected packages: dm-haiku\n  Building wheel for dm-haiku (setup.py) ... done\n  Created wheel for dm-haiku: filename=dm_haiku-0.0.10.dev0-py3-none-any.whl size=614395 sha256=25abd727928e5aaeeeafa7caad835333acf006ea394957d67ececb647c2b4ee0\n  Stored in directory: /tmp/pip-ephem-wheel-cache-idhjkii0/wheels/c7/4d/89/b159f184ad7c9e95672c342eafcc176ad92ee0c77f27f3bd23\nSuccessfully built dm-haiku\nInstalling collected packages: jmp, dm-haiku\nSuccessfully installed dm-haiku-0.0.10.dev0 jmp-0.0.2\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting git+https://github.com/jamesvuc/jax-bayes\n  Cloning https://github.com/jamesvuc/jax-bayes to /tmp/pip-req-build-n2yx3bkn\n  Running command git clone -q https://github.com/jamesvuc/jax-bayes /tmp/pip-req-build-n2yx3bkn\nRequirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from jax-bayes==0.1.1) (1.3.0)\nRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from jax-bayes==0.1.1) (1.21.6)\nRequirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.8/dist-packages (from jax-bayes==0.1.1) (3.3.0)\nRequirement already satisfied: protobuf>=3.12.4 in /usr/local/lib/python3.8/dist-packages (from jax-bayes==0.1.1) (3.19.6)\nRequirement already satisfied: scipy>=1.5.2 in /usr/local/lib/python3.8/dist-packages (from jax-bayes==0.1.1) (1.7.3)\nRequirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from jax-bayes==0.1.1) (1.15.0)\nRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.8/dist-packages (from jax-bayes==0.1.1) (4.64.1)\nBuilding wheels for collected packages: jax-bayes\n  Building wheel for jax-bayes (setup.py) ... done\n  Created wheel for jax-bayes: filename=jax_bayes-0.1.1-py3-none-any.whl size=1031680 sha256=9b907924ffc39bb28cbc9fc15f6fa0ae4fe85463be7b70cd798b52802fc3c4d7\n  Stored in directory: /tmp/pip-ephem-wheel-cache-xyinnglr/wheels/3f/7b/9c/326882f09afedfadf20a391de383da7aaea36b633d5e17555f\nSuccessfully built jax-bayes\nInstalling collected packages: jax-bayes\nSuccessfully installed jax-bayes-0.1.1"
  },
  {
    "objectID": "final_project/script/HMC_Copy_of_EXP_fit_time_window.html#downloading-data",
    "href": "final_project/script/HMC_Copy_of_EXP_fit_time_window.html#downloading-data",
    "title": "13  Predicting Stock Market with Bayesian Nerual Network",
    "section": "13.3 Downloading data",
    "text": "13.3 Downloading data\n\n!git clone https://github.com/stevengogogo/Bayesianneuralnet_stockmarket\n\nCloning into 'Bayesianneuralnet_stockmarket'...\nremote: Enumerating objects: 960, done.\nremote: Counting objects: 100% (10/10), done.\nremote: Compressing objects: 100% (8/8), done.\nremote: Total 960 (delta 3), reused 8 (delta 2), pack-reused 950\nReceiving objects: 100% (960/960), 217.92 MiB | 15.57 MiB/s, done.\nResolving deltas: 100% (167/167), done."
  },
  {
    "objectID": "final_project/script/HMC_Copy_of_EXP_fit_time_window.html#training-experiment",
    "href": "final_project/script/HMC_Copy_of_EXP_fit_time_window.html#training-experiment",
    "title": "13  Predicting Stock Market with Bayesian Nerual Network",
    "section": "13.4 Training Experiment",
    "text": "13.4 Training Experiment\n\nimport os\nimport os.path as osp\nimport numpy as np\n# %%\nimport numpy as np\nnp.random.seed(0)\n\nimport haiku as hk\nimport pandas as pd\nimport jax.numpy as jnp\nimport jax\n\nfrom tqdm import tqdm, trange\nfrom matplotlib import pyplot as plt\n\nfrom jax_bayes.utils import confidence_bands\nfrom jax_bayes.mcmc import (\n     langevin_fns,\n    mala_fns,\n     hmc_fns,\n)"
  },
  {
    "objectID": "final_project/script/HMC_Copy_of_EXP_fit_time_window.html#computational-devices",
    "href": "final_project/script/HMC_Copy_of_EXP_fit_time_window.html#computational-devices",
    "title": "13  Predicting Stock Market with Bayesian Nerual Network",
    "section": "13.5 Computational Devices",
    "text": "13.5 Computational Devices\n\njax.devices()\n\n[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]"
  },
  {
    "objectID": "final_project/script/HMC_Copy_of_EXP_fit_time_window.html#data-processing",
    "href": "final_project/script/HMC_Copy_of_EXP_fit_time_window.html#data-processing",
    "title": "13  Predicting Stock Market with Bayesian Nerual Network",
    "section": "13.6 Data processing",
    "text": "13.6 Data processing\nThe original data set is \\(x_t = \\{x_{1}, ..., x_{Total}\\}\\), the training input is a matrix with dimension \\(m \\times s\\) (where m is the capture window, and \\(s\\) is the number of samples). The sample is produced by shifting the original time series with lag of \\(2\\).\n\nTraining set\n\n\\[\\bar{x}_t =\n\\underbrace{\\begin{bmatrix}\n    x_{1+(t-1)T} & \\cdots & x_{m+(t-1)T}\\\\\n    x_{3+(t-1)T} & \\cdots & x_{2m+3+(t-1)T}\\\\\n    \\vdots & \\vdots & \\vdots\n\\end{bmatrix}}_{\\text{m(Capture windows)}}\n\\]\n\\[y_t =\n\\underbrace{\\begin{bmatrix}\n    x_{m+(t-1)T + 1} & \\cdots & x_{m+(t-1)T+ n}\\\\\n    x_{2m+3+(t-1)T + 1} & \\cdots & x_{2m+3+(t-1)T+ n}\\\\\n    \\vdots & \\vdots & \\vdots\n\\end{bmatrix}}_{\\text{n (Prediction Horizons)}}\n\\]\n\n\\(m\\): embedding dimension (predicting horizon)\n\\(T\\): time lag\n\n\ndata_path_base = 'Bayesianneuralnet_stockmarket/code/datasets'\n\ndef get_orig(sig, shift=2):\n    return np.concatenate((sig[0,:].ravel(), sig[1:,-shift:].ravel()))\n    \n\n# horizon\ntimesteps = 5\nsteps_ahead = 5\n\n# load\ntrain = np.loadtxt(open(os.path.join(data_path_base, \"MMM8_train.txt\")))\ntrain.shape\n\n(804, 10)\n\n\n\npd.DataFrame(train)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      0.000554\n      0.003739\n      0.001985\n      0.000000\n      0.002308\n      0.004293\n      0.001846\n      0.004200\n      0.001062\n      0.003970\n    \n    \n      1\n      0.001985\n      0.000000\n      0.002308\n      0.004293\n      0.001846\n      0.004200\n      0.001062\n      0.003970\n      0.007847\n      0.011216\n    \n    \n      2\n      0.002308\n      0.004293\n      0.001846\n      0.004200\n      0.001062\n      0.003970\n      0.007847\n      0.011216\n      0.010524\n      0.010339\n    \n    \n      3\n      0.001846\n      0.004200\n      0.001062\n      0.003970\n      0.007847\n      0.011216\n      0.010524\n      0.010339\n      0.011816\n      0.014355\n    \n    \n      4\n      0.001062\n      0.003970\n      0.007847\n      0.011216\n      0.010524\n      0.010339\n      0.011816\n      0.014355\n      0.019432\n      0.018879\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      799\n      0.719476\n      0.720176\n      0.723407\n      0.705364\n      0.693515\n      0.701704\n      0.694112\n      0.709296\n      0.694166\n      0.692539\n    \n    \n      800\n      0.723407\n      0.705364\n      0.693515\n      0.701704\n      0.694112\n      0.709296\n      0.694166\n      0.692539\n      0.696552\n      0.694491\n    \n    \n      801\n      0.693515\n      0.701704\n      0.694112\n      0.709296\n      0.694166\n      0.692539\n      0.696552\n      0.694491\n      0.676650\n      0.692593\n    \n    \n      802\n      0.694112\n      0.709296\n      0.694166\n      0.692539\n      0.696552\n      0.694491\n      0.676650\n      0.692593\n      0.684730\n      0.697528\n    \n    \n      803\n      0.694166\n      0.692539\n      0.696552\n      0.694491\n      0.676650\n      0.692593\n      0.684730\n      0.697528\n      0.705500\n      0.706259\n    \n  \n\n804 rows × 10 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# original time-series\norig = get_orig(train)\n\npd.DataFrame(orig[0:13])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      0.000554\n    \n    \n      1\n      0.003739\n    \n    \n      2\n      0.001985\n    \n    \n      3\n      0.000000\n    \n    \n      4\n      0.002308\n    \n    \n      5\n      0.004293\n    \n    \n      6\n      0.001846\n    \n    \n      7\n      0.004200\n    \n    \n      8\n      0.001062\n    \n    \n      9\n      0.003970\n    \n    \n      10\n      0.007847\n    \n    \n      11\n      0.011216\n    \n    \n      12\n      0.010524\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt.plot(orig)\nplt.title(\"Original Stock Signal\");\n\n\n\n\n\ntrain.shape\n\n(804, 10)\n\n\n\nx_train = train[:, :timesteps]\ny_train = train[:, timesteps: timesteps + steps_ahead]\nxy_train = (x_train, y_train)\n\n\npd.DataFrame(x_train)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n    \n  \n  \n    \n      0\n      0.000554\n      0.003739\n      0.001985\n      0.000000\n      0.002308\n    \n    \n      1\n      0.001985\n      0.000000\n      0.002308\n      0.004293\n      0.001846\n    \n    \n      2\n      0.002308\n      0.004293\n      0.001846\n      0.004200\n      0.001062\n    \n    \n      3\n      0.001846\n      0.004200\n      0.001062\n      0.003970\n      0.007847\n    \n    \n      4\n      0.001062\n      0.003970\n      0.007847\n      0.011216\n      0.010524\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      799\n      0.719476\n      0.720176\n      0.723407\n      0.705364\n      0.693515\n    \n    \n      800\n      0.723407\n      0.705364\n      0.693515\n      0.701704\n      0.694112\n    \n    \n      801\n      0.693515\n      0.701704\n      0.694112\n      0.709296\n      0.694166\n    \n    \n      802\n      0.694112\n      0.709296\n      0.694166\n      0.692539\n      0.696552\n    \n    \n      803\n      0.694166\n      0.692539\n      0.696552\n      0.694491\n      0.676650\n    \n  \n\n804 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(y_train)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n    \n  \n  \n    \n      0\n      0.004293\n      0.001846\n      0.004200\n      0.001062\n      0.003970\n    \n    \n      1\n      0.004200\n      0.001062\n      0.003970\n      0.007847\n      0.011216\n    \n    \n      2\n      0.003970\n      0.007847\n      0.011216\n      0.010524\n      0.010339\n    \n    \n      3\n      0.011216\n      0.010524\n      0.010339\n      0.011816\n      0.014355\n    \n    \n      4\n      0.010339\n      0.011816\n      0.014355\n      0.019432\n      0.018879\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      799\n      0.701704\n      0.694112\n      0.709296\n      0.694166\n      0.692539\n    \n    \n      800\n      0.709296\n      0.694166\n      0.692539\n      0.696552\n      0.694491\n    \n    \n      801\n      0.692539\n      0.696552\n      0.694491\n      0.676650\n      0.692593\n    \n    \n      802\n      0.694491\n      0.676650\n      0.692593\n      0.684730\n      0.697528\n    \n    \n      803\n      0.692593\n      0.684730\n      0.697528\n      0.705500\n      0.706259\n    \n  \n\n804 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt.plot(get_orig(y_train))\n\n\n\n\n\nx_train.shape\n\n(804, 5)\n\n\n\ny_train.shape\n\n(804, 5)\n\n\n\n## BNN training\n\n#could use any of the samplers modulo hyperparameters\n# sampler_fns = hmc_fns\nsampler_fns = langevin_fns\n#sampler_fns = mala_fns\n\n\n\ndef net_fn(x):\n\n    mlp = hk.Sequential([\n        hk.Linear(128, w_init=hk.initializers.Constant(0), \n                       b_init=hk.initializers.Constant(0)), \n        jnp.tanh, \n        hk.Linear(5,   w_init=hk.initializers.Constant(0), \n                       b_init=hk.initializers.Constant(0))\n        ])\n\n    return mlp(x)\n\n\nlr = 1e-4\nreg = 0.1\nlik_var = 0.5\n\nnet = hk.transform(net_fn)\nkey = jax.random.PRNGKey(0)\n\nsampler_init, sampler_propose, sampler_accept, sampler_update, sampler_get_params = \\\n    sampler_fns(key, num_samples=30, step_size=lr, init_stddev=5.0)\n\n\ndef logprob(params, xy):\n    \"\"\" log posterior, assuming \n    P(params) ~ N(0,eta)\n    P(y|x, params) ~ N(f(x;params), lik_var)\n    \"\"\"\n    x, y = xy\n\n    preds = net.apply(params, None, x)\n    log_prior = - reg * sum(jnp.sum(jnp.square(p)) \n                        for p in jax.tree_leaves(params))\n    log_lik = - jnp.mean(jnp.square(preds - y)) / lik_var\n    return log_lik + log_prior\n\n@jax.jit\ndef sampler_step(i, state, keys, batch):\n    # print(state)\n    # input()\n    params = sampler_get_params(state)\n    logp = lambda params:logprob(params, batch)\n    fx, dx = jax.vmap(jax.value_and_grad(logp))(params)\n\n    fx_prop, dx_prop = fx, dx\n    # fx_prop, prop_state, dx_prop, new_keys = fx, state, dx, keys\n    prop_state, keys = sampler_propose(i, dx, state, keys)\n\n    # for RK-langevin and MALA --- recompute gradients\n    #prop_params = sampler_get_params(prop_state)\n    #fx_prop, dx_prop = jax.vmap(jax.value_and_grad(logp))(prop_params)\n\n    # for HMC\n    prop_state, dx_prop, keys = state, dx, keys\n    for j in range(5): #5 iterations of the leapfrog integrator\n      prop_state, keys = \\\n      sampler_propose(i, dx_prop, prop_state, keys)\n\n      prop_params = sampler_get_params(prop_state)\n      fx_prop, dx_prop = jax.vmap(jax.value_and_grad(logp))(prop_params)\n\n    accept_idxs, keys = sampler_accept(\n        i, fx, fx_prop, dx, state, dx_prop, prop_state, keys\n    )\n    state, keys = sampler_update(\n        i, accept_idxs, dx, state, dx_prop, prop_state, keys\n    )\n\n\n    return state, keys\n\n\n# initialization\nparams = net.init(jax.random.PRNGKey(42), x_train)\nsampler_state, sampler_keys = sampler_init(params)\n\n/usr/local/lib/python3.8/dist-packages/haiku/_src/initializers.py:69: UserWarning: Explicitly requested dtype float64 requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n  return jnp.broadcast_to(jnp.asarray(self.constant), shape).astype(dtype)\n\n\n\nparams['linear']['w'].shape\n\n(5, 128)\n\n\n\n#do the sampling\nniter = 500000\ntrain_logp = np.zeros(niter)\nfor step in trange(niter):\n    # Training log\n    sampler_params = sampler_get_params(sampler_state)\n    logp = lambda params:logprob(params, xy_train)\n    train_logp[step] = jnp.mean(jax.vmap(logp)(sampler_params))\n    \n    sampler_state, sampler_keys = \\\n        sampler_step(step, sampler_state, sampler_keys, xy_train)\n\n\nsampler_params = sampler_get_params(sampler_state)\n\n  0%|          | 0/500000 [00:00<?, ?it/s]FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead.\n  for p in jax.tree_leaves(params))\n  2%|▏         | 8962/500000 [02:23<2:42:49, 50.26it/s]\n\n\n\n# Training log\nftn, axtn = plt.subplots()\naxtn.plot(train_logp[1000:])\naxtn.set_xlabel(\"Iterations\")\naxtn.set_ylabel(\"Log likelihood\")\n#ftn.savefig(\"../img/training_MALA_{}-iter.pdf\".format(niter))\n\n\noutputs = jax.vmap(net.apply, in_axes=(0, None, None))(sampler_params, None, x_train)\noutputs.shape\n\n\npred_lines = np.array([ get_orig(outputs[i,:,:]) for i in range(0, outputs.shape[0])])\npred_lines.shape\n\n\nms = jnp.mean(pred_lines, axis=0)\nss = jnp.std(pred_lines, axis=0)\n\nlower, uper = ms-ss, ms+ss\n\n\nx = jax.device_put(outputs)\n\n\nfmma, axmma = plt.subplots()\naxmma.plot(ms, label=\"Model average\")\naxmma.plot(orig, label=\"Ground truth\")\nfor i in range(outputs.shape[0]):\n        axmma.plot(get_orig(outputs[i,:,:]), color=\"gray\",alpha=0.04)\naxmma.set_xlabel(\"Time\")\naxmma.set_ylabel(\"A.U.\")\naxmma.set_title(\"MMM8 Prediction\")\naxmma.legend()\n#fmma.savefig(\"../img/prediction_MALA_{}-iter.pdf\".format(niter))\n\n\nf, ax = plt.subplots(1)\nfor i in range(outputs.shape[0]):\n        ax.plot(outputs[i,:,0], alpha=0.25)\nax.plot(ms, label=\"Mean\")\nax.plot(y_train)\n\n\nplt.plot(y_train[:,0])"
  },
  {
    "objectID": "final_project/script/EXP_data_visualization.html",
    "href": "final_project/script/EXP_data_visualization.html",
    "title": "14  Dataset before COVID",
    "section": "",
    "text": "import os\nimport os.path as osp\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndata_wo_cov = \"../data/Bayesianneuralnet_stockmarket/code/datasets\"\ndata_wh_cov = \"../data/Bayesianneuralnet_stockmarket/code/cov/data/\"\n\n\ndef get_orig(sig, shift=2):\n    \"\"\"\n    Retrieve original time series\n    \"\"\"\n    return np.concatenate((sig[0,:].ravel(), sig[1:,-shift:].ravel()))\n\ndef get_data(basename, dirname):\n    \"\"\"\n    Load stock matrix data\n    \"\"\"\n    return np.loadtxt(open(os.path.join(dirname, basename)))\n\ndef list_txt(dirname):\n    return [os.path.join(dirname,f) for f in os.listdir(dirname) if \".txt\" in f]\n\ndef fname(path):\n    fn = os.path.basename(path)\n    name = fn.split(\".\")[0]\n    return name\n\ndef plot_stock(filepath, title=\"\"):\n    data = np.loadtxt(filepath)\n    series = get_orig(data)\n    fig, ax = plt.subplots()\n    ax.plot(series)\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"A.U.\")\n    \n    if title==\"\":\n        ax.set_title(os.path.basename(filepath[:-4]))\n    else:\n        ax.set_title(title)\n        \n    return fig, ax\n\n\nfwocs = list_txt(data_wo_cov)\n\nfwocs\n\n['../data/Bayesianneuralnet_stockmarket/code/datasets/CBA.AX_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/DAI.DE_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/DAI.DE_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/MMM8_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/CBA.AX_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/CBA.AX_1_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/DAI.DE_1_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/CBA.AX_1_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/MMM8_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/600118.SS_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/600118.SS_1_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/600118.SS_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/DAI.DE_1_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/datasets/600118.SS_1_train.txt']\n\n\n\nfwhcs = list_txt(data_wh_cov)\n\nfwhcs\n\n['../data/Bayesianneuralnet_stockmarket/code/cov/data/DAI.DE covid_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/600118.SS_covid_half_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/MMM covid_half_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/600118.SS_covid_half_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/DAI.DE covid_half_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/DAI.DE covid_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/CBA.AX covid_half_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/MMM_covid_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/600118.SS covid_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/DAI.DE covid_half_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/CBA.AX covid_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/MMM_covid_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/CBA.AX covid_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/600118.SS covid_train.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/MMM covid_half_test.txt',\n '../data/Bayesianneuralnet_stockmarket/code/cov/data/CBA.AX covid_half_train.txt']\n\n\n\nfwhcs[11]\n\n'../data/Bayesianneuralnet_stockmarket/code/cov/data/MMM_covid_train.txt'\n\n\n\nfor d in [fwocs[3], fwocs[8], fwhcs[7], fwhcs[11]]:\n    f, ax = plot_stock(d)\n    f.savefig(\"../img/{}.pdf\".format(fname(d)), bbox_inches= \"tight\")"
  },
  {
    "objectID": "final_project/script/jax_basic.html#automatic-differentition",
    "href": "final_project/script/jax_basic.html#automatic-differentition",
    "title": "15  Introduction to Jax",
    "section": "15.1 Automatic differentition",
    "text": "15.1 Automatic differentition\n\ngrad()\njax.vjp: reverse-mode vetor-jacobian products\njax.jvp: Forward-mode jacobian-vector products"
  },
  {
    "objectID": "final_project/script/jax_basic.html#index-update",
    "href": "final_project/script/jax_basic.html#index-update",
    "title": "15  Introduction to Jax",
    "section": "15.2 Index update",
    "text": "15.2 Index update\n\nx = jnp.arange(10)\ny = x.at[0].set(0)\n\n\njax_array = jnp.ones((5,6))\nprint(jax_array)\n\n[[1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1.]]\n\n\n\nnew_jax_array = jax_array.at[::2, 3:].add(7.)\nprint(new_jax_array)\n\n[[1. 1. 1. 8. 8. 8.]\n [1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 8. 8. 8.]\n [1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 8. 8. 8.]]"
  },
  {
    "objectID": "final_project/script/jax_basic.html#linear-regression",
    "href": "final_project/script/jax_basic.html#linear-regression",
    "title": "15  Introduction to Jax",
    "section": "15.3 Linear regression",
    "text": "15.3 Linear regression\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nxs = np.random.normal(size=(100,))\nnoise = np.random.normal(scale=0.1, size=(100,))\nys = xs * 3 - 1 + noise\n\nplt.scatter(xs, ys);\n\n\n\n\n\ndef model(theta, x):\n    w, b = theta\n    return w*x + b\n\ndef loss_fn(theta, x, y):\n    prediction = model(theta,x)\n    return jnp.mean((prediction - y)**2)\n\ndef update(theta, x, y, lr=0.1):\n    return theta - lr * jax.grad(loss_fn)(theta, x, y)\n\n\ntheta = jnp.array([1.,1.])\n\nfor _ in range(1000):\n    theta = update(theta, xs, ys)\n\nplt.scatter(xs, ys)\nplt.plot(xs, model(theta, xs))\n\nw, b = theta"
  },
  {
    "objectID": "final_project/script/jax_bnn.html",
    "href": "final_project/script/jax_bnn.html",
    "title": "16  Bayesian Neural Network with Jax",
    "section": "",
    "text": "Resources: https://github.com/jamesvuc/jax-bayes\n\n\n# %%\nimport numpy as np\nnp.random.seed(0)\n\nimport haiku as hk\n\nimport jax.numpy as jnp\nimport jax\n\nfrom tqdm import tqdm, trange\nfrom matplotlib import pyplot as plt\n\nfrom jax_bayes.utils import confidence_bands\nfrom jax_bayes.mcmc import (\n    # langevin_fns,\n    mala_fns,\n    # hmc_fns,\n)\n\n#could use any of the samplers modulo hyperparameters\n# sampler_fns = hmc_fns\n# sampler_fns = langevin_fns\nsampler_fns = mala_fns\n\ndef build_dataset():\n    n_train, n_test, d = 200, 100, 1\n    xlims = [-1.0, 5.0]\n    x_train = np.random.rand(n_train, d) * (xlims[1] - xlims[0]) + xlims[0]\n    x_test = np.random.rand(n_test, d) * (xlims[1] - xlims[0]) + xlims[0]\n\n    target_func = lambda t: (np.log(t + 100.0) * np.sin(1.0 * np.pi*t)) + 0.1 * t\n\n    y_train = target_func(x_train)\n    y_test = target_func(x_test)\n\n    y_train += np.random.randn(*x_train.shape) * (1.0 * (x_train + 2.0)**0.5)\n\n    return (x_train, y_train), (x_test, y_test)\n\ndef net_fn(x):\n\n    mlp = hk.Sequential([\n        hk.Linear(128, w_init=hk.initializers.Constant(0), \n                       b_init=hk.initializers.Constant(0)), \n        jnp.tanh, \n        hk.Linear(1,   w_init=hk.initializers.Constant(0), \n                       b_init=hk.initializers.Constant(0))\n        ])\n\n    return mlp(x)\n\n\n# ======= Setup =======\nxy_train, xy_test = build_dataset()\n(x_train, y_train), (x_test, y_test) = xy_train, xy_test\n\n\nx_train.shape\n\n(200, 1)\n\n\n\ny_train.shape\n\n(200, 1)\n\n\n\nx_test.shape\n\n(100, 1)\n\n\n\n# lr = 1e-3\n# reg = 0.1\n# lik_var = 0.5\n\n# lr = 1e-1\nlr = 1e-4\nreg = 0.1\nlik_var = 0.5\n\nnet = hk.transform(net_fn)\nkey = jax.random.PRNGKey(0)\n\nsampler_init, sampler_propose, sampler_accept, sampler_update, sampler_get_params = \\\n    sampler_fns(key, num_samples=11, step_size=lr, init_stddev=5.0)\n\n\ndef logprob(params, xy):\n    \"\"\" log posterior, assuming \n    P(params) ~ N(0,eta)\n    P(y|x, params) ~ N(f(x;params), lik_var)\n    \"\"\"\n    x, y = xy\n\n    preds = net.apply(params, None, x)\n    log_prior = - reg * sum(jnp.sum(jnp.square(p)) \n                        for p in jax.tree_leaves(params))\n    log_lik = - jnp.mean(jnp.square(preds - y)) / lik_var\n    return log_lik + log_prior\n\n@jax.jit\ndef sampler_step(i, state, keys, batch):\n    # print(state)\n    # input()\n    params = sampler_get_params(state)\n    logp = lambda params:logprob(params, batch)\n    fx, dx = jax.vmap(jax.value_and_grad(logp))(params)\n\n    fx_prop, dx_prop = fx, dx\n    # fx_prop, prop_state, dx_prop, new_keys = fx, state, dx, keys\n    prop_state, keys = sampler_propose(i, dx, state, keys)\n\n    # for RK-langevin and MALA --- recompute gradients\n    prop_params = sampler_get_params(prop_state)\n    fx_prop, dx_prop = jax.vmap(jax.value_and_grad(logp))(prop_params)\n\n    # for HMC\n    # prop_state, dx_prop, keys = state, dx, keys\n    # for j in range(5): #5 iterations of the leapfrog integrator\n    #   prop_state, keys = \\\n    #       sampler_propose(i, dx_prop, prop_state, keys)\n\n    #   prop_params = sampler_get_params(prop_state)\n    #   fx_prop, dx_prop = jax.vmap(jax.value_and_grad(logp))(prop_params)\n\n    accept_idxs, keys = sampler_accept(\n        i, fx, fx_prop, dx, state, dx_prop, prop_state, keys\n    )\n    state, keys = sampler_update(\n        i, accept_idxs, dx, state, dx_prop, prop_state, keys\n    )\n\n\n    return state, keys\n\n\n# ======= Sampling ======\n\n# initialization\nparams = net.init(jax.random.PRNGKey(42), x_train)\n\n\nparams['linear']['w'].shape\n\n(1, 128)\n\n\n\nsampler_state, sampler_keys = sampler_init(params)\n\n\n#do the sampling\nfor step in trange(5000):\n    sampler_state, sampler_keys = \\\n        sampler_step(step, sampler_state, sampler_keys, xy_train)\n\n\nsampler_params = sampler_get_params(sampler_state)\n\n  0%|          | 0/5000 [00:00<?, ?it/s]FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead.\n  for p in jax.tree_leaves(params))\n100%|██████████| 5000/5000 [00:07<00:00, 640.63it/s]\n\n\n\nsampler_params['linear']['w'].shape\n\n(11, 1, 128)\n\n\n\n# ========= Plotting ========\nplot_inputs = np.linspace(-1, 30, num=600).reshape(-1,1)\noutputs = jax.vmap(net.apply, in_axes=(0, None, None))(sampler_params, None, plot_inputs)\n\n\nplot_inputs.shape\n\n(600, 1)\n\n\n\noutputs.shape\n\n(11, 600, 1)\n\n\n\noutputs.squeeze(-1).T.shape\n\n(600, 11)\n\n\n\nlower, upper = confidence_bands(outputs.squeeze(-1).T)\n\nf, ax = plt.subplots(1)\n\nax.plot(x_train.ravel(), y_train.ravel(), 'x', color='green')\nax.plot(x_test.ravel(), y_test.ravel(), 'x', color='red')\nfor i in range(outputs.shape[0]):\n    ax.plot(plot_inputs, outputs[i], alpha=0.25)\nax.plot(plot_inputs, np.mean(outputs[:, :, 0].T, axis=1), color='black', \n        linewidth=1.0)\nax.fill_between(plot_inputs.squeeze(-1), lower, upper, alpha=0.75)\n\nax.set_ylim(-10, 15)\n\nplt.show()"
  },
  {
    "objectID": "ref.html",
    "href": "ref.html",
    "title": "References",
    "section": "",
    "text": "ardianumam. 2017. “Understanding Multivariate\nGaussian, Gaussian Properties and Gaussian\nMixture Model.” Ardian Umam Blog.\n\n\nBraga-Neto, Ulisses. 2020. Fundamentals of Pattern Recognition and\nMachine Learning. Springer."
  }
]