---
title: Homework 1
author: Shao-Ting Chiu (UIN:433002162)
date: today
bibliography: ../ref.bib  
format:
    pdf:
        code-line-numbers: true

jupyter: python3  
execute: 
    echo: true
    freeze: auto 
---

## Homework Description

> Course: ECEN649, Fall2022
> 
> Problems (from Chapter 2 in the book):
> 2.1 , 2.3 (a,b), 2.4, 2.7, 2.9, 2.17 (a,b)
>
> Note: the book is available electronically on the Evans library website.

- Deadline: `Sept. 26th, 11:59 pm`


## Computational Enviromnent Setup

### Third-party libraries
``` {python}
%matplotlib inline
import sys # system information
import matplotlib # plotting
import scipy as st # scientific computing
import pandas as pd # data managing
import numpy as np # numerical comuptation
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
from numpy.linalg import inv, det
# Matplotlib setting
plt.rcParams['text.usetex'] = True
matplotlib.rcParams['figure.dpi']= 300
```

### Version
``` {python}
print(sys.version)
print(matplotlib.__version__)
print(st.__version__)
print(np.__version__)
print(pd.__version__)
```

---

## Problem 2.1

> Suppose that $X$ is a discrete feature vector, with distribution concentrated over a countable set $D=\{x^{1}, x^2, \dots\}$ in $R^{d}$. Derive the discrete versions of (2.3), (2.4), (2.8), (2.9), (2.11), (2.30), (2.34), and (2.36)
> 
> Hint: Note that if $X$ has a discrete distribution, then integration becomes summation, $P(X=x_k)$, for $x_k\in D$, play the role of $p(x)$, and $P(X=x_k | Y=y)$, for $x_k \in D$, play the role of $p(x|Y=y)$, for $y=0,1$.


### (2.3)

> From @braga2020fundamentals [pp. 16]
> \begin{align}
    P(X\in E, Y=0) &= \int_E P(Y=0)p(x|Y=0)dx\\
    P(X\in E, Y=1) &= \int_E P(Y=1)p(x|Y=1)dx\\
> \end{align}

Let $x_k = [x_1,\dots, x_d]$ be the feature vector of $X$ in set $D\in R^d$,

\begin{align}
    P(X\in D, Y=0) &= P(X=[x_1,\dots, x_d], Y=0)\\
                   &= \sum_{X\in D} P(Y=0)P(X=[x_1,\dots, x_d] |Y=0)\\
    P(X\in D, Y=1) &= P(X=[x_1,\dots, x_d], Y=1)\\
                   &= \sum_{X\in D} P(Y=1)P(X=[x_1,\dots, x_d] |Y=1)\\
\end{align}

### (2.4)

> From @braga2020fundamentals [pp.17]

\begin{align}
    P(Y=0|X=x_k) &= \frac{P(Y=0)p(X=x_k | Y=0)}{p(X=x_k)}\\
                 &= \frac{P(Y=0)p(X=x_k |Y=0)}{P(Y=0)p(X=x_k |Y=0) + P(Y=1)p(X=x_k|Y=1)}\\
\end{align}


\begin{align}
    P(Y=1|X=x_k) &= \frac{P(Y=1)p(X=x_k | Y=1)}{p(X=x_k)}\\
                 &= \frac{P(Y=1)p(X=x_k |Y=1)}{P(Y=0)p(X=x_k |Y=0) + P(Y=1)p(X=x_k|Y=1)}\\
\end{align}


### (2.8)

> From @braga2020fundamentals [pp. 18]

$$\epsilon^{0}[\psi] = P(\psi(X)=1 | Y=0) = \sum_{\{x_k|\psi(x)=1\}} p(x_k|Y=0)$$

$$\epsilon^{1}[\psi] = P(\psi(X)=0 | Y=1) = \sum_{\{x_k|\psi(x)=1\}} p(x_k|Y=1)$$

### (2.9)

> From @braga2020fundamentals [pp. 18]

$$\epsilon[\psi] = \sum_{\{x|\psi(x)=1\}} P(Y=0)p(x_k | Y=0) + \sum_{\{x|\psi=0\}}P(Y= 1)p(x_k|Y=1)$$



### (2.11)

> From @braga2020fundamentals [pp. 19]


$$\epsilon[\psi] = E[\epsilon[\psi|X=x_k]] = \sum_{x_{k}\in D} \epsilon[\psi|X=x_k]p(x_k)$$

### (2.30)

> From @braga2020fundamentals [pp. 24].

$$\epsilon^{*} = \sum_{x\in X} \left[ I_{\eta(X=x)\leq 1-\eta(X=x)}\eta(X=x) + I_{\eta(X=x) > 1 - \eta(X=x)(1-\eta(X=x))}\right] p(X=x)$$

### (2.34)

> From @braga2020fundamentals [pp. 25].

\begin{align}
    \epsilon^{*} &= P(Y=0)\epsilon^{0}[\psi^*] + P(Y=1)\epsilon^1 [\psi^*]\\
                 &= \sum_{\{x|P(Y=1)p(x|Y=1)>P(Y=0)p(x|Y=0)\}} P(Y=0)p(x|Y=0) + \sum_{\{x|P(Y=1)p(x|Y=1)\leq P(Y=0)p(x|Y=0)\}} P(Y=1)p(x|Y=1) 
\end{align}

### (2.36)



## Problem 2.3

> This problem seeks to characterize the case $\epsilon^{*}=0$.

### (a) {#sec-23a}

> Prove the "Zero-One Law" for perfect discrimination:
> $$\epsilon^{*} = 0 \Leftrightarrow \eta(X) = 0 \text{ or } 1 \quad \text{with probability } 1.$$ {#eq-2-3-a}


The optimal Bayes classifier is defined in @braga2020fundamentals [pp. 20]. That is 

\begin{equation}
    \psi^{*}(x) = \arg\max_{i} P(Y=i|X=x) = 
    \begin{cases}
        1, & \eta(x) > \frac{1}{2}\\ 
        0, & \text{otherwise}
    \end{cases}
\end{equation}

**Part 1: $\eta(X)=1$**

$$\eta(X) = E[Y|X=x] = P(Y=1|X=x) = 1$$

$$\because \eta(X)=1>\frac{1}{2} \therefore \psi^{*}(x) = 1$$


\begin{align}
    \epsilon^{*} &= \epsilon[\psi^{*}(X) | X=x]\\
    &= I_{\psi^{*}(x) = 0}P(Y=1 | X=x) + I_{\psi^{*}(x) = 1} P(Y=0|X=x)\\ 
    &= \underbrace{I_{\psi^{*}(x) = 0}}_{=0}\underbrace{\eta(X)}_{=1}+ \underbrace{I_{\psi^{*}(x) = 1}}_{=1} \underbrace{(1-\eta(X))}_{=0}\\ 
    &= 0
\end{align}

**Part 2: $\eta(X)=0$**

Similarly,

$$\because \eta(X)=0 \leq \frac{1}{2} \therefore \psi^{*}(x) = 0$$

\begin{align}
    \epsilon^{*} &= \epsilon[\psi^{*}(X) | X=x]\\
    &= I_{\psi^{*}(x) = 0}P(Y=1 | X=x) + I_{\psi^{*}(x) = 1} P(Y=0|X=x)\\ 
    &= \underbrace{I_{\psi^{*}(x) = 0}}_{=1}\underbrace{\eta(X)}_{=0}+ \underbrace{I_{\psi^{*}(x) = 1}}_{=0} \underbrace{(1-\eta(X))}_{=1}\\ 
    &= 0
\end{align}

In conclusion, both cases shows that $\epsilon^{*} = 0$.

### (b)

> Show that
> 
> $$\epsilon^{*} = 0 \Leftrightarrow \text{ there is a function } f \text{ s.t. } Y=f(X) \text{ with probability } 1$$

\begin{equation}
    \eta(X) = Pr(Y=1 | X=x) =
    \begin{cases}
        1, & f(X)=1\\
        0, & f(X)=0
    \end{cases}
\end{equation}

The sceneraio is same as [Problem 3.7 (a)](#sec-23a).

1. Given $\eta(X) = 1$
    - $\epsilon^{*} = 0$
1. Given $\eta(X) = 0$
    - $\epsilon^{*} = 0$

$\epsilon^{*} =0$ for both cases.

## Problem 2.4

>  This problem concerns the extension to the multiple-class case of some of the concepts derived in this chapter. Let  $Y\in \{0,1,\dots,c-1\}$, where $c$ is the number of classes, and let
> $$\eta_{i}(x) = P(Y=i|X=x), \quad i=0,1,\dots, c-1,$$
>
> for each $x\in R^d$. We need to remember that these probabilities are not indpendent, but satisfy $\eta_0(x) + \eta_1(x)+\dots+\eta_{c-1}(x) = 1$, for each $x\in R^{d}$, so that one of the functions is redundant. In the two-class case, this is made explicit by using a single $\eta(x)$, but using the redundant set above proves advantageous in the multiple-class case, as seen below.
>
> Hint:  you should answer the following items in sequence, using the previous answers in the solution of the following ones

### (a)

> Given a classifier $\psi: R^d \rightarrow \{0,1,\dots, c-1\}$, show that its conditional error $P(\psi(X)\neq Y|X=x)$ is given by
>
> $$P(\psi(X)\neq Y|X=x) = 1-\sum^{c-1}_{i=1} I_{\psi(x)=i}\eta_{i}(x) = 1 - \eta_{\psi(x)}(x)$$ {#eq-2-4-a}

Use the "Law of Total Probability" [@braga2020fundamentals, sec. A.53],

$$P(\psi(X) =  Y| X=x) + P(\psi(X)\neq Y| X=x) = 1$$ {#eq-2-4-a-tot}

$\therefore$ We can derive the probability of error via

\begin{align}
    P(\psi(X)\neq Y| X=x) &= 1 - P(\psi(X) =  Y| X=x)\\
    &= 1 - \sum_{i=0}^{c-1}P(\psi(x)=i, Y=i | X=x)\\ 
    &= 1 - \sum_{i=0}^{c-1}I_{\psi(x)=i}P(Y=i | X=x)\\ 
    &= 1 - \sum_{i=0}^{c-1}I_{\psi(x)=i}\eta_i(x)
\end{align}

Combining together, @eq-2-4-a-tot implies @eq-2-4-a.

### (b) {#sec-2-4-b}

> Assuming that $X$ has a density, show that the classification error of $\psi$ is given by
> 
> $$\epsilon = 1 - \sum_{i=0}^{c-1} \int_{\{x|\psi(x)=i\}}\eta_i(x)p(x)dx.$$

Let $\{x|\psi(x) = i\}$ be the set of $\psi(x) = i$ in $X$.

Use the *multiplication rule* [@braga2020fundamentals, sec. A1.3]

\begin{align}
\epsilon &= E[\epsilon[\psi(x) | X=x]]\\ 
&= 1 - \int_{R^d} P(\psi(X) = Y| X=x) p(x) dx\\ 
&= 1 - \sum_{i=0}^{c-1} \int_{R^d} p(\psi(X)=i, Y=i| X=x) p(x) dx\\ 
&= 1 - \sum_{i=0}^{c-1} \int_{R^d} \underbrace{p(\psi(X)=i| X=x)}_{=1 \text{ if } \{x|\psi(x) = i\}; 0, \text{ otherwise.}}p(Y=i|X=x) p(x) dx\\ 
& = 1 - \sum_{i=0}^{c-1} \int_{\{x|\psi(x) = i\}} 1\cdot p(Y=i|X=x) p(x) dx\\ 
& = 1 - \sum_{i=0}^{c-1} \int_{\{x|\psi(x) = i\}} p(Y=i|X=x) p(x) dx
\end{align}





### (c)

> Prove that the Bayes classifier is given by
>
> $$\psi^{*}(x) = \arg\max_{i=0,1,\dots, c-1} \eta_{i}(x), \quad x\in R^d$$ {#eq-2-4-c-q}
>
> Hint: Start by considering the difference between conditional expected errors $P(\psi(X)\neq Y|X=x) - P(\psi^{*}(X)\neq Y|X=x)$.

According to @braga2020fundamentals [pp. 20], a Bayes classifier ($\psi^{*}$) is defined as

$$\psi^{*} = \arg\min_{\psi\in \mathcal{C}} P(\psi(X) \neq Y)$$

over the set $\mathcal{C}$ of all classifiers. We need to show that the error of any $\psi\in \mathcal{C}$ has the conditional error rate:

$$\epsilon[\psi | X=x] \geq \epsilon[\psi^{*} | X=x], \quad \text{ for all } x\in R^d$$ {#eq-2-4-c}

From @eq-2-4-a, classifiers have the error rates:

\begin{align}
P(\psi^{*}(X)\neq |X=x) &= 1 - \sum_{i=1}^{c-1} I_{\psi^{*}(x)=i}\eta_i(x)\\
P(\psi(X)\neq |X=x) &= 1 - \sum_{i=1}^{c-1} I_{\psi(x)=i}\eta_i(x)
\end{align}

Therefore, 

\begin{align}
    P(\psi(X)\neq Y|X=x) - P(\psi^{*}(X)\neq Y|X=x) &= (1 - \sum_{i=1}^{c-1} I_{\psi(x)=i}\eta_i(x)) - (1 - \sum_{i=1}^{c-1} I_{\psi^{*}(x)=i}\eta_i(x))\\
    &= \sum_{i=1}^{c-1} (I_{\psi^{*}(x)=i} - I_{\psi(x)=i})\eta_i(x)
\end{align}

$\because$

- $I_{\psi^{*}(x)=i^{*}} = 1$ when $i^{*}$ satisfies $\eta_{i^{*}}(x) = \max_{i=0,1,\dots,c-1}\eta(x) = \eta_{\max}(x)$
- $I_{\psi(x)=i'}=1$ when $\psi(x) = i'$ for  $i'\in 0,1,\dots, c-1$ 

$\therefore$ 

**if $i^* \neq i'$**

\begin{align}
     P(\psi(X)\neq Y|X=x) - P(\psi^{*}(X)\neq Y|X=x) &= (1-0)\eta_{i^{*}}(x) + (0-1)\eta_{i'}(x)\\
     &= \eta_{i^{*}}(x) - \eta_{i'}(x)\\ 
     &= \eta_{\max}(x) - \eta_{i'}(x)\\ 
     &\geq 0
\end{align}

**if $i^* = i'$**

$$P(\psi(X)\neq Y|X=x) - P(\psi^{*}(X)\neq Y|X=x) = \eta_{i^{*}}(x) - \eta_{i'}(x)=0$$

Therefore, there is no classifier $\psi\in \mathcal{C}$ can have conditional error rate lower than Bayes classifier [@eq-2-4-c-q].

### (d)

> Show that the Bayes error is given by
>
> $$\epsilon^{*} = 1 - E[\max_{i=0,1,\dots, c-1} \eta_{i}(X)]$$

From [Problem 2.4.b](#sec-2-4-b),


- Noted that, $\{x|\psi^{*}(x)=i\} = \emptyset \text{ if } i\neq i^{*}$

\begin{align}
    \epsilon[\psi^{*}] &= E[\epsilon[\psi^{*}(x)|X=x]]\\
    &= 1 - \sum_{i=0}^{c-1} \int_{\{x|\psi^{*}(x)=i\}}\eta_i(x)p(x)dx\\
    &= 1 - \int_{\{x|\psi^{*}(x)=i^*\}}\eta_{\max}(x)p(x)dx\\ 
    &= 1 - E[\eta_{\max}(x)] 
\end{align}


### (e)

> Show that the maximum Bayes error possible is $1-\frac{1}{c}$.

$$\max \epsilon[\psi^{*}] = 1 - \min E[\max_{i=0,1,\dots,c-1} \eta_i(X)]$$ {#eq-2-4-e-1}

also,

given

$$\eta_1(x) = \eta_2(x) = \dots = \eta_{c-1}(x)$$

$$\sum_{i=1}^{c-1} \eta_i(x) = 1$$

we can get that 
$$\min\max\eta(X) = \frac{1}{c}$$ {#eq-2-4-e-2}

Combining @eq-2-4-e-1 and @eq-2-4-e-2 together, the maximum Bayes error is $1-\frac{1}{c}$



## Problem 2.7

> Consider the following univariate Gaussian class-conditional densities:
>
> $$p(x|Y=0) = \frac{1}{\sqrt{2\pi}}\exp(-\frac{(x-3)^2}{2})$$
> $$p(x|Y=1)=\frac{1}{3\sqrt{2\pi}}\exp(-\frac{(x-4)^2}{18})$$
> Assume that the classes are equally likely, i.e., $P(Y=0)=P(Y=1)=\frac{1}{2}$

### (a)

> Draw the densities and determine the Bayes classifier graphically.



### (b)

> Determine the Bayes classifier.

### (c)

> Determine the specificity and sensitivity of the Bayes classifier.
>
> Hint: use the standard Gaussian CDF $\psi(x)$

|Sensitivity|Specificity|
|---|---|
|$1-\epsilon^1[\psi]$|$1-\epsilon^0[\psi]$|
: The definition of sensitivity and specificity from @braga2020fundamentals [pp. 18] {#tbl-spec-def}


### (d)

> Determine the overall Bayes error.

## Problem 2.9

> Obtain the optimal decision boundary in the Gaussian model with $P(Y = 0) = P(Y = 1)$ and
>
> In each case draw the optimal decision boundary, along with the class means and class conditional density contours, indicating the 0- and 1-decision regions.

Since $\Sigma_0 \neq \Sigma_1$ happens in the following subproblems, these are *heteroskedastic cases*. As mentioned in @braga2020fundamentals [sec. 2.5.2]. The Bayes classifier is 

\begin{equation}
    \psi^{*}_{Q}(x) = 
    \begin{cases}
        1, x^T A x + b^T x + c > 0\\ 
        0, \text{ otherwise }
    \end{cases}
\end{equation}

where 

\begin{align}
    A &= \frac{1}{2}(\Sigma^{-1}_{0} - \Sigma^{-1}_{1})\\
    b &= \Sigma^{-1}_{1} \mu_1 - \Sigma^{-1}_{0}\mu_0\\ 
    c &= \frac{1}{2}(\mu^{T}_{0}\Sigma_{0}^{-1}\mu_0 - \mu_{1}^{T}\Sigma^{-1}_{1}\mu_1) + \frac{1}{2}\ln\frac{\det(\Sigma_0)}{\det(\Sigma_1)} + \ln \frac{P(Y=1)}{P(Y=0)}
\end{align}


The following is the Python implementation with NumPy[^numpy] [^plot].

``` {python}
def compose(U,D):
    return U.transpose()@inv(D)@U

def mA(sigma0, sigma1):
    sigma0_inv = inv(sigma0)
    sigma1_inv = inv(sigma1)
    return 0.5*(sigma0_inv - sigma1_inv)

def mb(sigma0, sigma1, mu0, mu1):
    sigma0_inv = inv(sigma0)
    sigma1_inv = inv(sigma1)
    return sigma1_inv@mu1 - sigma0_inv@mu0

def mc(sigma0, sigma1, mu0, mu1, Py):
    return  0.5*(compose(mu0, sigma0) - compose(mu1, sigma1)) +\
        0.5*np.log(det(sigma0)/det(sigma1)) +\
            np.log(Py/(1-Py))

def BayesBound(x, sigma0, sigma1, mu0, mu1, Py):
    xax = x.transpose() @ mA(sigma0, sigma1)@x
    bx = mb(sigma0, sigma1, mu0, mu1).transpose() @ x
    c = mc(sigma0, sigma1, mu0, mu1, Py)
    
    return float(xax + bx + c)

class GaussianBayesClassifier:
    def __init__(self, sigma0, sigma1, mu0, mu1, Py):
        self.sigma0 = sigma0
        self.sigma1 = sigma1
        self.mu0 = mu0
        self.mu1 = mu1
        self.Py = Py

        # Inferred Matrix
        self.mA = mA(sigma0, sigma1)
        self.mb = mb(sigma0, sigma1, mu0, mu1)
        self.mc = mc(sigma0, sigma1, mu0, mu1, Py)

    def BayesBound(self, x):
        return BayesBound(x, self.sigma0, self.sigma1, self.mu0, self.mu1, self.Py)

    def psi(self, x):
        """
        Bayes classification
        """
        pred = 0
        if self.BayesBound(x) > 0:
            pred = 1
        return pred
    
    def plot2D(self, psi_annotates=[[0.3,0.3], [0.7,0.7]]):
        fig, ax = plt.subplots()

        # Create girds
        xlist = np.linspace(-3.5,3.5,30)
        ylist = np.linspace(-3.5,3.5,30)
        X, Y = np.meshgrid(xlist, ylist)
        pos = np.dstack((X,Y))
        
        # Compute Bayes classification
        Z = np.zeros(X.shape)
        for i in range(0, Z.shape[0]):
            for j in range(0, Z.shape[1]):
                x = np.matrix([X[i,j], Y[i,j]]).T 
                Z[i,j] = self.psi(x)
        
        # Compute Gaussia pdf
        rv0 = multivariate_normal(np.array(self.mu0.T)[0], self.sigma0)
        rv1 = multivariate_normal(np.array(self.mu1.T)[0], self.sigma1)
        Z0 = rv0.pdf(pos)
        Z1 = rv1.pdf(pos)

        # Plot contours
        cmap = plt.get_cmap('Set1', 2)
        ax.contour(X,Y,Z, cmap=cmap, levels=[0.9])
        ax.contour(X,Y,Z0, alpha=0.4)
        ax.contour(X,Y,Z1, alpha=0.4)
        ax.plot(self.mu0[0,0], self.mu0[1,0], marker="o", color="k",label="$\mu_0$")
        ax.plot(self.mu1[0,0], self.mu1[1,0], marker="o", color="gray",label="$\mu_1$")
        if psi_annotates==None:
            psi_annotates = [[self.mu0[0,0], self.mu0[1,0]],[self.mu1[0,0], self.mu1[1,0]]]

        # Annotate decisions
        for ann in psi_annotates:
            i = X[int(X.shape[0]*ann[0]), int(X.shape[1]*ann[1])]
            j = Y[int(Y.shape[0]*ann[0]), int(Y.shape[1]*ann[1])]
            x = np.matrix([i, j]).T 
            if self.psi(x) > 0:
                ax.annotate("$\psi^{*}(x) = 1$", (i, j))
            else:
                ax.annotate("$\psi^{*}(x) = 0$", (i, j))
        
        # legend and label settings
        ax.set_xlabel("x")
        ax.set_ylabel("y")
        ax.legend()
```

[^numpy]: There is a well-written documentation for matrix operation: https://numpy.org/doc/stable/user/absolute_beginners.html#creating-matrices
[^plot]: the contour plot for Gaussian process is referred to https://gist.github.com/gwgundersen/90dfa64ca29aa8c3833dbc6b03de44be.

### (a)

> \begin{equation*}
    \mu_0 =(0,0)^T, \mu_1=(2,0)^T, \Sigma_0 = 
    \begin{pmatrix}
        2 & 0 \\
        0 & 1
    \end{pmatrix},
    \Sigma_1 = 
    \begin{pmatrix}
        2 & 0 \\
        0 & 4
    \end{pmatrix}
> \end{equation*}

``` {python}
#| label: fig-2-4-a
#| fig-cap: "Bayes Classifier for 2D Gussian (a)"

bc = GaussianBayesClassifier(np.matrix([[2,0],[0,1]]),\
                             np.matrix([[2,0],[0,4]]),\
                             np.matrix([0,0]).T,\
                             np.matrix([2,0]).T, 0.5)
bc.plot2D()
```


### (b)

> \begin{equation*}
    \mu_0 =(0,0)^T, \mu_1=(2,0)^T, \Sigma_0 = 
    \begin{pmatrix}
        2 & 0 \\
        0 & 1
    \end{pmatrix},
    \Sigma_1 = 
    \begin{pmatrix}
        4 & 0 \\
        0 & 1
    \end{pmatrix}
> \end{equation*}

``` {python}
#| label: fig-2-4-b
#| fig-cap: "Bayes Classifier for 2D Gussian (b)"

bc = GaussianBayesClassifier(np.matrix([[2,0],[0,1]]),\
                             np.matrix([[4,0],[0,1]]),\
                             np.matrix([0,0]).T,np.matrix([2,0]).T,\
                             0.5)
bc.plot2D()
```

### (c)

> \begin{equation*}
    \mu_0 =(0,0)^T, \mu_1=(0,0)^T, \Sigma_0 = 
    \begin{pmatrix}
        1 & 0 \\
        0 & 1
    \end{pmatrix},
    \Sigma_1 = 
    \begin{pmatrix}
        2 & 0 \\
        0 & 2
    \end{pmatrix}
> \end{equation*}

``` {python}
#| label: fig-2-4-c
#| fig-cap: "Bayes Classifier for 2D Gussian (c)"

bc = GaussianBayesClassifier(np.matrix([[1,0],[0,1]]),\
                             np.matrix([[2,0],[0,2]]),\
                             np.matrix([0,0]).T,\
                             np.matrix([0,0]).T,\
                             0.5)
bc.plot2D(psi_annotates= [[0.4,0.3], [0.7,0.7]])
```

### (d)

> \begin{equation*}
    \mu_0 =(0,0)^T, \mu_1=(0,0)^T, \Sigma_0 = 
    \begin{pmatrix}
        2 & 0 \\
        0 & 1
    \end{pmatrix},
    \Sigma_1 = 
    \begin{pmatrix}
        1 & 0 \\
        0 & 2
    \end{pmatrix}
> \end{equation*}

``` {python}
#| label: fig-2-4-d
#| fig-cap: "Bayes Classifier for 2D Gussian (d)"

bc = GaussianBayesClassifier(np.matrix([[2,0],[0,1]]),\
                             np.matrix([[1,0],[0,2]]),\
                             np.matrix([0,0]).T,\
                             np.matrix([0,0]).T,\
                             0.5)
bc.plot2D(psi_annotates= [[0.3,0.4], [0.6,0.7]])
```

---

## Python Assignment: Problem 2.17

> This problem concerns the Gaussian model for synthetic data generation in @braga2020fundamentals [Section A8.1].

### (a)

> Derive a general expression for the Bayes error for the homoskedastic case with $\mu_0 = (0,\dots,0), \mu_1=(1,\dots,1)$, and $P(Y=0)=P(Y=1)$. Your answer should be in terms of $k, \sigma^{2}_{1}, \dots, \sigma^{2}_{k}, l_1, \dots, l_k,$ and $\sigma_{1},\dots, \sigma_k$.
> 
> Hint: Use the fact that 
>
>\begin{equation}
     \begin{bmatrix}
        1 & \sigma & \cdots & \sigma\\ 
        \sigma & 1 & \cdots & \sigma\\ 
        \vdots & \vdots & \ddots & \vdots\\ 
        \sigma & \sigma & \cdots & 1
     \end{bmatrix}^{-1}_{l\times l} = \frac{1}{(1-\sigma)(1+(l-1)\sigma)}
     \begin{bmatrix}
        1+(l-2)\sigma & -\sigma \cdots -\sigma\\ 
        -\sigma & 1+(l-2)\sigma & \cdots -\sigma\\ 
        \vdots & \vdots  & \ddots & \vdots\\ 
        -\sigma & -\sigma & \cdots & 1+(l-2)\sigma
     \end{bmatrix}
>\end{equation}



### (b) 

> Specialize the previous formula for equal-sized blocks $l_1 = \cdots = l_k = l$ with equal correlations $\sigma_1 = \cdots = \sigma_k = \sigma$, and constant variance $\sigma_{1}^{2} = \cdots, \sigma_{k}^{2} = \sigma^2$. Write the resulting formula in terms of $d,l,\sigma$ and $\sigma$.

#### i.

> Using the python function `norm.cdf` in the `scipy.stats` module, plot the Bayes error as a function of $\sigma\in [0.01,3 ]$ for $d=20, l=4$, and four different correlation values $\sigma=0,0.25,0.5,0.75$ (plot one curve for each value). Confirm that the Bayes error increasese monotonically with $\sigma$ from $0$ to $0.5$ for each value of $\sigma$, and that Bayes error for large $\sigma$ is uniformly larger than that for smaller $\sigma$. The latter fact shows that correlation between the features is detrimental to classfification.

#### ii.

> Plot the Bayes error as a function of $d=2,4,6,8,\dots, 40$, with fixed block size $l=4$ and variance $\sigma^2=1$ and $\sigma=0,0.25,0.5,0.75$ (plot one curve for each value). Confirm that the Bayes error decreases monotonically to $0$ with increasing dimensionality, with faster convergence for smaller correlation values.

#### iii.

> Plot the Bayes error as a function of the correlation $\sigma \in [0,1]$ for constant variance $\sigma^2 =2$ and fixed $d=20$ with varying block size $l=1,2,4,10$ (plot one curve for each value). Confirm that the Bayes error increases monotonically with increasing correlation. Notice that the rate of increase is particularly large near $\sigma=0$, which shows that the Bayes error is very sensitive to correlation in the near-independent region.

::: {.content-hidden when-format="html"}

## References

:::
