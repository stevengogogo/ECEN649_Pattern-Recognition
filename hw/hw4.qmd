---
title: Homework 4
author: Shao-Ting Chiu (UIN:433002162)
date: today
bibliography: ../ref.bib  
format:
    pdf:
        code-line-numbers: true
        table-of-contents: true 
        keep-tex: false
        
    html: 
        table-of-contents: true
jupyter: python3  
execute: 
    echo: true
    freeze: auto 
---

## Homework Description
 
- Course: ECEN649, Fall2022
- Deadline: 2022/11/16, 11:59 pm
> Problems from the Book
> 
> 6.3
> 
> 6.5
> 
> 6.7
> 
> 7.1
> 
> 7.10
> 
> 6.12 (coding assignment)
> 
> Problems 6.3-6.5 are worth 10 points each, Problem 7.10 and the coding assignment are worth 20 points each.

---

## Computational Environment

### Libraries
 
``` {python}
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import sys
```

### Versions
``` {python}
print(np.__version__)
print(tf.__version__)
print (sys.version)
print(sys.executable)
```
---

## Problem 6.3

> Show that the decision regions produced by a neural network with $k$ threshold sigmoids in the *first* hidden layer, no matter what nonlinearities are used in succeeding layers, are equal to the intersection of $k$ half-spaces, i.e., the decision boundary is piecewise linear
>
> Hint: All neurons in the first hidden layer are perceptrons and the output of the layer is a binary vector.

Let $\bar{O}$ be the $k$ output of first hidden layer, and there are $2^k$ types of binary vectors $[O_1, \dots, O_k]$.



For each data point $x\in R^d$ where $d$ is the feature space. the output of first layer is 

$$O(x)_{i} = I_{g_{i}(x)}(x), \quad i = 1,\cdots, k$$ {#eq-out}

where $g_{i}(\cdot)$ is the perceptron function of neuron $i$.  Thus, any point $x$ belong to one type of $[I_{g_{1}(x)}(x), \dots, I_{g_{k}(x)}(x)]$. For each $O_i$, the space forms a half-space with $\{x: g_i(x) > 0\}$, and there are $k$ half space in total.


## Problem 6.5

> For the VGG16 CNN architecture:

![VGG16](img/vgg16.png){width=50% #fig-vgg16}

### (a)

> Determine the number of filters used in each convolution layer.

- `Conv-1`: 64 filters (pre-depth: 3)
- `Conv-2`: 128 filters (pre-depth: 64)
- `Conv-3`: 256 filters (pre-depth: 128)
- `Conv-4`: 512 filters (pre-depth: 256)
- `Conv-5`: 512 filters (pre-depth: 512)

There are total 
``` {python}
rs = np.array([3, 64, 128, 256, 512])
t_filters = np.array([64, 128, 256, 512, 512])
np.sum(t_filters)
```

filters.


### (b)

> Based on the fact that all filters are of size $3\times 3\times r$, where $r$ is the depth of the previous layer, determine the total number of convolution weights in the entire network.

``` {python}
CONV1 = (3*3*3)*64 + (3*3*64)*64 
CONV1
```

``` {python}
CONV2 = (3*3*64)*128 + (3*3*128)*128
CONV2
```

``` {python}
CONV3 = (3*3*128)*256 + (3*3*256)*256 + (3*3*256)*256
CONV3
```

``` {python}
CONV4 = (3*3*256)*512 + (3*3*512)*512 + (3*3*512)*512
CONV4
```

``` {python}
CONV5 = (3*3*512)*512 *3
CONV5
```

``` {python}
fc1 = 512 * 7 * 7 * 4096
fc1
```

``` {python}
fc2 = 4096 * 4096
fc2
```

``` {python}
fc3 = 4096 * 1000
fc3
```



[^vgg]: Related https://stackoverflow.com/questions/28232235/how-to-calculate-the-number-of-parameters-of-convolutional-neural-networks.

### (c)

> Add the weights used in the fully-connected layers to obtain the total number of weights used by VGG16.

Total of weights
``` {python}
total = np.sum([CONV1, CONV2, CONV3, CONV4, CONV5, fc1, fc2, fc3])
total 
```

## Problem 6.7

> Consider the training data set given in the figure below.

![](img/p6-7.png){width=50% fig-align="center"}

### (a)

> By inspection, find the coefficients of the linear SVM hyperplane $a_1 x_1 + a_2 x_2 + a_0 = 0$ and plot it. What is the value of the margin? Say as much as you can about the values of the Lagrange multipliers associated with each of the points.



![SVM boundry](img/p6-7_SVM.png){width=30% #fig-svm}

The boundary passes by $\frac{1}{2}((3,3) + (3,2)) = (3, 2.5)$ and $\frac{1}{2} ((3,4) + (4,3))=(3.5, 3.5)$

- $a_1 = 2.5 - 3.5 = -1$

- $a_2 = 3.5 - 3 = 0.5$

- $a_0 = 3\cdot 3.5 - 3.5 \cdot 2.5 = 1.75$

- The boundary is 
    $$-x_1 + 0.5 x_2 + 1.75 = 0$$


In @fig-svm, there are $6$ support vectors that are $\lambda_2$ to $\lambda_7$. The KKT conditions[^kkt] state that 

\begin{align}
    \lambda_i = 0 &\Rightarrow y_i E_i \leq 0\\ 
    0 < \lambda_i < C &\Rightarrow y_i E_i = 0\\ 
    \lambda_i = C &\Rightarrow y_i E_i \geq 0
\end{align}



- Lagrange multipliers
    - $\lambda_1 = 0$
    - $\lambda_2 \in (0, C)$
    - $\lambda_3 \in (0, C)$
    - $\lambda_4 \in (0, C)$
    - $\lambda_5 \in (0, C)$
    - $\lambda_6 \in (0, C)$
    - $\lambda_7 \in (0, C)$
    - $\lambda_8 = 0$

where $C$ is the pentalty term.

[^kkt]: Intro. to SVM: https://article.sciencepublishinggroup.com/html/10.11648.j.acm.s.2017060401.11.html


### (b)

> Apply the CART rule, using the misclassification impurity, and stop after finding one splitting node (this is the "1R" or "stump" rule). If ther eis a tie between best splits, pick one that makes at most one error in each class. Plot this classifier as a decision boundary superimposed on the training data and also as a binary decision tree showing the splitting and leaf nodes.

::: {#fig-elephants layout-ncol=2}

![Decision boundary](img/p6-7_tree.png){width=30% fig-align="center"}

```{mermaid}
%%| echo: false

flowchart TD
  A[x1 <= 3.5] 
  A --> |yes| D[1]
  A --> |no| E[0]
```

Apply CART rule
:::

where $\bullet$ labelled as $1$; $\circ$ labelled as $0$.

### (c)

> How do you compare the classifiers in (a) and (b) ? Which one is more likely to have a smaller classification error in this problem?

- SVM of (a) yields smaller classification error than (b) because it allow any slope of decision boundary.


## Problem 7.1

> Suppose that the classification error $\epsilon_n$ and an error estimator $\hat{\epsilon}_n$ are jointly Gaussian, such 
> $$\epsilon_n \sim N(\epsilon^* + \frac{1}{n}, \frac{1}{n^2}), \hat{\epsilon}_n \sim N(\epsilon^* - \frac{1}{n}, \frac{1}{n^2}), Cov(\epsilon_n, \hat{\epsilon}_n) = \frac{1}{2n^2}$$
> where $\epsilon^*$ is the Bayes error. Find the bias, deviation variance, RMS, correlation coefficient and tail probabilities $P(\hat{\epsilon}_n - \epsilon_n < - \tau)$ and $P(\hat{\epsilon}_n - \epsilon_n > \tau)$ of $\hat{\epsilon}_n$. Is this estimator optimistically or pessimistically biased? Does performance improve as sample size increases? Is the estimator consistent?

### Bias

Use Eq. 7.3 [@braga2020fundamentals, page 154], 

$$Bias(\hat{\epsilon}_n) = E[\hat{\epsilon}_n] - E[\epsilon_n]$$


- $E[\hat{\epsilon}_n] = \epsilon^{*} - \frac{1}{n}$
- $E[\epsilon_n] = \epsilon^* + \frac{1}{n}$

Thus,

$$Bias(\hat{\epsilon}_n) = \frac{-2}{n} < 0$$

This estimator is *optimisitcally biased*.

### Deviation variance

Use Eq. 7.4 [@braga2020fundamentals, page 154], 

$$Var_{dev}(\hat{\epsilon}_n) = Var(\hat{\epsilon}_n, \epsilon_n) = Var(\hat{\epsilon}_n) + Var(\epsilon_n) - 2 Cov(\epsilon_n, \hat{\epsilon}_n)$$

- $Var(\hat{\epsilon}_n) = \frac{1}{n^2}$
- $Var(\epsilon_n) = \frac{1}{n^2}$
- $Cov(\epsilon_n, \hat{\epsilon}_n) = \frac{1}{2n^2}$

Thus,

$$Var_{dev}(\hat{\epsilon}_n) = \frac{1}{n^2} + \frac{1}{n^2} - 2\frac{1}{2n^2} = \frac{1}{n^2}$$

The deviation variance reduces as sample size increases.


### Root mean-square error

Use Eq. 7.5 [@braga2020fundamentals, pp. 154],

$$RMS(\hat{\epsilon}_n) = \sqrt{E[(\hat{\epsilon}_n  - \epsilon_n)^2]} = \sqrt{Bias(\hat{\epsilon}_n)^2 + Var_{dev}(\hat{\epsilon}_n)}$$

Apply previous results, 

$$RMS(\hat{\epsilon}_n) = \sqrt{\frac{4}{n^2} + \frac{1}{n^2}} = \frac{\sqrt{5}}{n}$$

### Correlation coefficient

Use the pearson correlation coefficient[^wiki-r]

$$\rho_{X,Y} = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}$$

- $Cov(\epsilon_n, \hat{\epsilon}_n) = \frac{1}{2n^2}$
- $\sigma_{\epsilon_n} = \frac{1}{n}$
- $\sigma_{\hat{\epsilon}_n} = \frac{1}{n}$

$$\rho_{\epsilon_n, \hat{\epsilon}_n} =\frac{1}{2}$$

Correlation coefficient is a constant and independent from sample size.

[^wiki-r]: Correlation coefficient: https://en.wikipedia.org/wiki/Pearson_correlation_coefficient

### Tail probabilities

Use Eq. 7.6 [@braga2020fundamentals, pp. 154],

$$P(|\hat{\epsilon}_n - \epsilon_n| \geq \tau) = P(\hat{\epsilon}_n - \epsilon_n \geq \tau) + P(\hat{\epsilon}_n - \epsilon_n \leq -\tau), \quad \text{for } \tau > 0$$

The normal difference distribution[^wolf-norm] of $\hat{\epsilon}_n - \epsilon_n$

$$\hat{\epsilon}_n - \epsilon_n \sim N(\frac{-2}{n}, \frac{2}{n^2}) = N(\mu, \sigma^2)$$

That $\Delta\epsilon_n = \hat{\epsilon}_n - \epsilon_n$

\begin{align}
    P(\Delta\epsilon_n \leq -\tau)% 
    &= P(\frac{\Delta\epsilon_n - \mu}{\sigma} \leq \frac{-\tau - \mu}{\sigma})\\
    &= \Phi(\frac{-\tau - \mu}{\sigma})\\ 
    &= \Phi(\frac{-\tau + 2/n}{\sqrt{2}/n})\\ 
    &= \Phi(\frac{-n\tau + 2}{\sqrt{2}})\\ 
\end{align}

\begin{align}
    P(\Delta\epsilon_n \geq \tau)% 
    &= P(\frac{\Delta\epsilon_n - \mu}{\sigma} \geq \frac{\tau - \mu}{\sigma})\\
    &= 1 - P(\frac{\Delta\epsilon_n - \mu}{\sigma} < \frac{\tau - \mu}{\sigma})\\ 
    &= 1 - \Phi(\frac{\tau - \mu}{\sigma})\\ 
    &= 1 - \Phi(\frac{n\tau - 2}{\sqrt{2}})
\end{align}


Thus, when $n\to \infty$

\begin{align}
    \lim_{n\to \infty}  P(\Delta\epsilon_n \leq -\tau) = 0\\ 
    \lim_{n\to \infty} P(\Delta\epsilon_n \geq \tau) = 0
\end{align}

This can be conluced to 
$$\lim_{n\to \infty} P(|\hat{\epsilon}_n - \epsilon_n| \geq \tau) = 0$$

The estimator is *consistent*.

[^wolf-norm]: Normal difference distribution: https://mathworld.wolfram.com/NormalDifferenceDistribution.html



## Problem 7.10

> This problem illustrates the very poor (even paradoxical) performance of cross-validation with very small sample sizes. Consider the resubstitution and leave-one-out estimators $\hat{\epsilon}^{r}_{n}$ and $\hat{\epsilon}^{l}_n$ for the 3NN classification rule, with a sample of size $n=4$ from a mixture of two equally-likely Gaussian populations $\Pi_0 \sim N_d(\mu_0, \Sigma)$ and $\Pi_1 \sim N_d(\mu_1, \Sigma)$. Assume that $\mu_0$ and $\mu_1$ are far enough apart to make $\delta = \sqrt{(\mu_1 - \mu_0)^T \Sigma^{-1} (\mu_1 = \mu_0)}\gg 0$ (in which case the Bayes error is $\epsilon_{\text{bay}} = \Phi(-\frac{\delta}{2})\approx 0$).

### (a)

> For a sample $S_n$ with $N_0 = N_1 = 2$, which occurs $P(N_0 = 2) = {4\choose 2}2^{-4} = 37.5\%$ of the time, show that $\epsilon_n \approx 0$ but $\hat{\epsilon}^{l}_{n}=1$


If $N_0=N_1=2$, the leave-one-out method removes one of the data point. The remaining points will have the majority label and have opposite label to the removed point (@fig-knn). This flipping causes $\hat{\epsilon}^l = 1$.

Since two Gaussian population are far away from each other. The decidsion boundary is in the middle of two means, and there is little overlap between two distribution.  Thus, when $\delta \gg 0$,  $\epsilon_n \approx 0$.


``` {python}
#| echo: false
#| fig-cap: Two separated gaussian process in hyperplan with N0=N1=2
#| label: fig-knn
plt.scatter([1,2], [1,2], label="$\Pi_0$")
plt.scatter([5,6], [5,6], label="$\Pi_1$")
plt.legend();
```

### (b)

> Show that $E[\epsilon_n] \approx \frac{5}{16} = 0.3125$, but $E[\hat{\epsilon}^{l}_{n}] = 0.5$, so that $\text{Bias}(\hat{\epsilon}^{l}_{n}) \approx \frac{3}{16} = 0.1875$, and the leave-one-out estimator is far from unbiased.

Given two label have equal occurrences,

- $P(N_0 = 0) = {4 \choose 0}2^{-4} = 1\cdot 2^{-4}$
    - $(N_0, N_1) = (0, 4)$
    - $\epsilon_n = \frac{1}{2}$ (always predicting $N_1$)
    - $\hat{\epsilon}_{n}^{l} = 0$ 
- $P(N_0 = 1) = {4 \choose 1}2^{-4} = 4\cdot 2^{-4}$
    - $(N_0, N_1) = (1, 3)$
    - $\epsilon_n = \frac{1}{2}$ 
    - $\hat{\epsilon}_{n}^{l} = \frac{1}{4}$ 
- $P(N_0 = 2) = {4 \choose 2}2^{-4} = 6\cdot 2^{-4}$
    - $(N_0, N_1) = (2, 2)$
    - $\epsilon_n = 0$ 
    - $\hat{\epsilon}_{n}^{l} = 1$ (flipped)
- $P(N_0 = 3) = {4 \choose 3}2^{-4} = 4\cdot 2^{-4}$
    - $(N_0, N_1) = (3, 1)$
    - $\epsilon_n = \frac{1}{2}$ 
    - $\hat{\epsilon}_{n}^{l} = \frac{1}{4}$ 
- $P(N_0 = 4) = {4 \choose 4}2^{-4} = 1\cdot 2^{-4}$
    - $(N_0, N_1) = (4, 0)$
    - $\epsilon_n = \frac{1}{2}$ 
    - $\hat{\epsilon}_{n}^{l} = 0$ 

$$E[\epsilon_n] = \frac{1}{2}\frac{1}{16} + \frac{1}{2}\frac{4}{16}+ 0 + \frac{1}{2}\frac{4}{16} + \frac{1}{2}\frac{1}{16}=\frac{5}{16}
$$

$$E[\hat{\epsilon}_{n}^{l}] = 0 + \frac{1}{4}\frac{4}{16} + 1\frac{6}{16} + \frac{1}{4}\frac{4}{16} + 0 = \frac{8}{16} = \frac{1}{2}$$

### (c)

> Show that $Var_d(\hat{\epsilon}^{l}_n) \approx \frac{103}{256} \approx 0.402$, which corresponds to a standard deviation of $\sqrt{0.402} = 0.634$. The leave-one-out  estimator is therefore highly-biased and highly-variable in this case.

\begin{align}
    Var_d(\hat{\epsilon}^{l}_n)%
    &= E[(\hat{\epsilon}^{l}_n - \epsilon_n)^2] - (E[\hat{\epsilon}^{l}_n - \epsilon_n])^2\\ 
    &= (0 -\frac{1}{2})^2 \frac{1}{16} + (\frac{1}{4} - \frac{1}{2})^2 \frac{4}{16}\\ 
    &+ (1-0)^2 \frac{6}{16} + (\frac{1}{2} - \frac{1}{4})^2 \frac{4}{16} + (0-\frac{1}{2})^2 \frac{1}{16} - (\frac{3}{16})^2\\ 
    &= 2(\frac{1}{2})^2 \frac{1}{16} + 2(\frac{1}{4})^2 \frac{4}{16}+ \frac{6}{16} - (\frac{3}{16})^2\\ 
    &= \frac{14}{32} - (\frac{3}{16})^2 = \frac{103}{256}
\end{align}





### (d)

> Consider the correlation coefficient of an error estimator $\hat{\epsilon}_n$ with the true error $\epsilon_n$:
> $$\rho(\epsilon_n, \hat{\epsilon}_n) = \frac{Cov(\epsilon_n, \hat{\epsilon}_n)}{Std(\epsilon_n)Std(\hat{\epsilon}_n)}$$
> Show that $\rho(\epsilon_n, \hat{\epsilon}^{l}_{n} \approx 0.98)$, i.e., the leave-one-out estimator is almost perfectly negatively correlated with the true error.



\begin{align}
    Var(\hat{\epsilon}_{n}^{l}) &= E[\epsilon^{2}_n] - E[\epsilon_n]^2\\ 
    &= \frac{1}{16}\frac{4}{16} + \frac{6}{16} + \frac{1}{16}\frac{4}{16} = \frac{4 + 96 + 4}{256} - \frac{1}{4} = \frac{40}{256} = \frac{5}{32}
\end{align}

\begin{align}
    Var(\epsilon_n)%
    &= E[(\hat{\epsilon}_{n}^{l})^2] - (E[\hat{\epsilon}_{n}^{l}])^2\\
    &= \frac{1}{4}\frac{1}{16} + \frac{1}{4}\frac{4}{16}\\ 
    &+ \frac{1}{4}\frac{4}{16} + \frac{1}{4}\frac{1}{16} - (\frac{5}{16})^2\\ 
    &= \frac{10}{64} - \frac{25}{256} = \frac{15}{256}
\end{align}

Use the previous results,

\begin{align}
    Cov(\epsilon_n, \hat{\epsilon}_{n}^{l})%
    &= E[\epsilon_n \hat{\epsilon}_{n}^{l}] - E[\epsilon_n ]E[\hat{\epsilon}_{n}^{l}]\\ 
    &= (0 + \frac{1}{2}\frac{1}{4}\frac{4}{16} + 0 + \frac{1}{2}\frac{1}{4}\frac{4}{16}) - \frac{5}{16}\frac{1}{2}\\ 
    &= \frac{1}{16} - \frac{5}{32} = \frac{-3}{32} \approx -0.98
\end{align}


We can derive the correlation coefficient:


$$\rho(\epsilon_n, \hat{\epsilon}_{n}^{l}) = \frac{-3/32}{\sqrt{\frac{5}{32}}\sqrt{\frac{15}{256}}}$$

### (e)

> For comparison, show that, although $E[\hat{\epsilon}^{r}_{n}] = \frac{1}{8} = 0.125$, so that $\text{Bias}(\hat{\epsilon}^{r}_{n}) \approx \frac{-3}{16} = -0.1875$, which is exactly the negative of the bias of leave-one-out, we have $Var_d(\hat{\epsilon}^{r}_{n}) \approx \frac{7}{256} \approx 0.027$, for a standard deviation of $\frac{\sqrt{7}}{16} \approx 0.165$, which is several times smaller than the leave-one-out variance, and $\rho(\epsilon_n, \hat{\epsilon}^{r}_{n}) \approx \sqrt{\frac{3}{5}} \approx 0.775$, showing that the resubstitution estimator is highly positively correlated with the true error.

The resubstitution error estimator uses 3 nearest neighbors, and no point is removed:

- $P(N_0 = 0) = {4 \choose 0}2^{-4} = 1\cdot 2^{-4}$
    - $(N_0, N_1) = (0, 4)$
    - $\epsilon_n = \frac{1}{2}$
    - $\hat{\epsilon}_{n}^{r} = 0$ 
- $P(N_0 = 1) = {4 \choose 1}2^{-4} = 4\cdot 2^{-4}$
    - $(N_0, N_1) = (1, 3)$
    - $\epsilon_n = \frac{1}{2}$
    - $\hat{\epsilon}_{n}^{r} = \frac{1}{4}$ 
- $P(N_0 = 2) = {4 \choose 2}2^{-4} = 6\cdot 2^{-4}$
    - $(N_0, N_1) = (2, 2)$
    - $\epsilon_n = 0$
    - $\hat{\epsilon}_{n}^{r} = 0$ 
- $P(N_0 = 3) = {4 \choose 3}2^{-4} = 4\cdot 2^{-4}$
    - $(N_0, N_1) = (3, 1)$
    - $\epsilon_n = \frac{1}{2}$
    - $\hat{\epsilon}_{n}^{r} = \frac{1}{4}$ 
- $P(N_0 = 4) = {4 \choose 4}2^{-4} = 1\cdot 2^{-4}$
    - $(N_0, N_1) = (4, 0)$
    - $\epsilon_n = \frac{1}{2}$
    - $\hat{\epsilon}_{n}^{r} = 0$ 

The resubstitution estimator is positively correlated with the true error.


## Problem 6.12

``` {python}
import tensorflow as tf
import numpy as np
import PIL
import cv2
import os
import sklearn
import pandas as pd
import pickle
import platform
from tqdm.notebook import tqdm
from sklearn.multiclass import OneVsOneClassifier
from sklearn import preprocessing
from sklearn import svm
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from scipy import stats as st
```

### Computational Environment

``` {python}
physical_devices = tf.config.list_physical_devices('GPU')
my_system = platform.uname()
print(physical_devices)
print(f"System: {my_system.system}")
print(f"Node Name: {my_system.node}")
print(f"Release: {my_system.release}")
print(f"Version: {my_system.version}")
print(f"Machine: {my_system.machine}")
print(f"Processor: {my_system.processor}")
```

### Helper function

``` {python}
def load_image(path, width=484, preprocess_input=tf.keras.applications.vgg16.preprocess_input):
    """
    Load and Preprocessing image
    """
    img = tf.keras.utils.load_img(path)
    x = tf.keras.utils.img_to_array(img)
    x = x[0:width,:,:]
    x = np.expand_dims(x, axis=0)
    return tf.keras.applications.vgg16.preprocess_input(x)

```

### Data inspectation

``` {python}
dpath = os.path.join("data", "CMU-UHCS_Dataset")
pic_path = os.path.join(dpath, "images")
df_micro = pd.read_csv( os.path.join(dpath, "micrograph.csv"))
df_micro = df_micro[["path", "primary_microconstituent"]]

## Save paths
paths =  dict(zip(["train", "test"],\
        [os.path.join(dpath, "feature_{}.pkl".format(n))\
         for n in ["train", "test"]]))

for i in range(0, len(df_micro)):
    img_ph = os.path.join(pic_path,df_micro.iloc[i][0])
    assert os.path.exists(img_ph)
    df_micro.iloc[i][0] = img_ph
df_micro2 = df_micro.copy()
CLS_rm = ["pearlite+widmanstatten", "martensite", "pearlite+spheroidite"] #(type, sample size)
```

``` {python}
for c in CLS_rm:
    df_micro.drop(df_micro[df_micro["primary_microconstituent"] == c].index, inplace=True)
```

``` {python}
# labels
name_lbs = df_micro["primary_microconstituent"].unique()
le = preprocessing.LabelEncoder()
le.fit(name_lbs)
list(le.classes_)
```

``` {python}
dlabel = le.transform(df_micro["primary_microconstituent"])
df_micro.insert(2, "label", dlabel)
df_micro
```

### Data Processing

``` {python}
# Train-test split
df_test = df_micro.copy()
df_train = pd.DataFrame(columns = df_micro.keys())

split_info = [("spheroidite", 100),\
              ("network", 100),\
              ("pearlite", 100),\
              ("spheroidite+widmanstatten", 60)] #(type, sample size)



for ln in split_info:
    label, n = ln
    id_train = df_micro[df_micro["primary_microconstituent"] == label][0:n].index
    df_test.drop(id_train, axis=0, inplace=True)
    df_train = pd.concat([df_train, df_micro.loc[id_train]])
```

``` python
print(df_train.to_string())
```

``` python
print(df_test.to_string())
```

### Feature Extraction

``` {python}
# VGG16

base_model = tf.keras.applications.vgg16.VGG16(
    include_top=False,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)

base_model.summary()
```

Use five layers

``` {python}
out_layer_ns = ["block{}_pool".format(i) for i in range(1,6)]
out_layer_ns
```

``` {python}
# Construct 5 models for feature extraction
extmodel = dict(zip(out_layer_ns, [tf.keras.Model(
    inputs= base_model.input,
    outputs=base_model.get_layer(bk_name).output
) for bk_name in out_layer_ns]))

extmodel
```

``` {python}
# Display output dimensions
out_shapes = [extmodel[m].output_shape[-1] for m in extmodel.keys()]
out_shapes
```

``` {python}
# Initiate feature maps for testing and training
fs_train = [np.zeros((df_train.shape[0], n_f)) for n_f in out_shapes]
fs_test = [np.zeros((df_test.shape[0], n_f)) for n_f in out_shapes]

features_train = dict(zip(out_layer_ns, fs_train))
features_test = dict(zip(out_layer_ns, fs_test))

features_train
```

``` {python}
# Feature extraction with VGG16
if os.path.exists(os.path.join(dpath, "feature_train.pkl")) == False:
    for m in tqdm(extmodel.keys()):
        for i, df in enumerate([df_train, df_test]):
            for j, ph in tqdm(enumerate(df["path"])):
                x = load_image(ph)
                xb = extmodel[m].predict(x, verbose = 0) # silence output
                F = np.mean(xb,axis=(0,1,2))
                # Save features
                if i ==0:
                    features_train[m][j, :] = F
                else:
                    features_test[m][j, :] = F
    #save file
    paths =  dict(zip(["train", "test"],\
        [os.path.join(dpath, "feature_{}.pkl".format(n))\
         for n in ["train", "test"]]))
    ## Create new files
    f_train = open(paths["train"], "wb")
    f_test = open(paths["test"], "wb")
    ## Write
    pickle.dump(features_train, f_train)
    pickle.dump(features_test, f_test)
    ## Close files
    f_train.close()
    f_test.close()
```

### SVM

``` {python}
# load data
ftn = open(paths["train"], "rb")
ftt = open(paths["test"], "rb")
featn = pickle.load(ftn) # train feature
featt = pickle.load(ftt) # test feature
ftn.close()
ftt.close()

# label
ltrain = df_train[["primary_microconstituent", "label"]].reset_index()
ltest = df_test[["primary_microconstituent", "label"]].reset_index()
```

``` {python}
ltrain
```

``` {python}
ltest
```


#### One-to-One SVM

``` {python}
class One2OneSVM:
    def __init__(self, n_class=4):
        self.n_class = n_class
        self.clfs = [[svm.SVC(kernel="rbf", C=1., gamma="auto")\
                     for i in range(0,self.n_class)]\
                     for j in range(0,self.n_class)]
        self.cv = np.zeros((self.n_class,self.n_class))
    def train(self, ltrain, feature, fold=10):
        # traversal all features
        for i in range(0, self.n_class-1):
            lis = ltrain[ltrain["label"] == i].index.to_numpy()
            for j in range(i+1, self.n_class):
                ljs = ltrain[ltrain["label"] == j].index.to_numpy()
                # Data
                X = np.concatenate(\
                  (feature[lis,:],\
                   feature[ljs,:]), axis=0)
                Y = np.concatenate((np.ones(len(lis))*i,np.ones(len(ljs))*j))
                # Train SVM
                scores = sklearn.model_selection.cross_val_score(self.clfs[i][j], X, Y, cv=fold)
                self.clfs[i][j].fit(X,Y)
                self.cv[i][j] = np.max(scores)
                
    def test_1v1_error(self, ltest, feature):
        # traversal all features
        errM = np.zeros((self.n_class, self.n_class))
        for i in range(0, self.n_class-1):
            lis = ltest[ltest["label"] == i].index.to_numpy()
            for j in range(i+1, self.n_class):
                ljs = ltest[ltest["label"] == j].index.to_numpy()
                # Data
                X = np.concatenate(\
                  (feature[lis,:],\
                   feature[ljs,:]), axis=0)
                Y = np.concatenate((np.ones(len(lis))*i,np.ones(len(ljs))*j))
                # Train SVM
                y_pred = self.clfs[i][j].predict(X)
                errM[i,j] = error(Y, y_pred)
        return errM
        
    def predict(self, feature):
        predM = np.zeros(( int(self.n_class * (self.n_class -1)/2) , feature.shape[0]))
        c = 0
        for i in range(0, self.n_class-1):
            for j in range(i+1, self.n_class):
                predM[c,:] = self.clfs[i][j].predict(feature)
                c += 1
        return st.mode(predM, axis=0, keepdims=True).mode[0,:] #majority voting

def error(ans, pred):
    assert len(ans) == len(pred)
    return (ans != pred).sum()/float(ans.size)
```

### (a)

> The convolution layer used and the cross-validated error estimate for
> each of the six pairwise two-label classifiers

### (b)

> Separate test error rates on the unused micrographs of each of the
> four categories, for the pairwise two-label classifiers and the
> multilabel one-vs-one voting classifier described previously. For the
> pairwise classifiers use only the test micrographs with the two labels
> used to train the classifier. For the multilabel classifier, use the
> test micrographs with the corresponding four labels.

``` {python}
def df_cv(m, clf, info=""):
    var1 = []
    var2 = []
    cvs = []
    errs = []
    for i in range(0, m.shape[0]-1):
        for j in range(i+1, m.shape[0]):
            var1.append(i)
            var2.append(j)
            cvs.append(clf.cv[i,j])
            errs.append(m[i,j])
    infos = [info] * len(errs)
    return pd.DataFrame({"Info": infos, "Label 1": le.inverse_transform(var1), "Label 2": le.inverse_transform(var2), "Test error": errs,"Cross Validation Score": cvs})
```

#### Pair-wise classifier

``` {python}
df_errors = []
for b in out_layer_ns:
    clf1 = One2OneSVM()
    clf1.train(ltrain, features_train[b])
    errs = clf1.test_1v1_error(ltest, features_test[b])
    df_errors.append(df_cv(errs, clf1, b))
    
res_error = pd.concat(df_errors)
print(res_error.to_string())
```

#### Multiple one-vs-one classifier

``` {python}
# Multiclass one-vs-one
dfm_errors = []
for b in out_layer_ns:
    clf = OneVsOneClassifier(svm.SVC(kernel="rbf", C=1., gamma="auto").fit(features_train[b],\
          ltrain["label"].to_numpy(int)))
    clf.fit(features_train[b],\
          ltrain["label"].to_numpy(int))
    y_predm = clf.predict(features_test[b])
    dfm_errors.append(1 - error(y_predm, ltest["label"].to_numpy()))

# Display result
res_multi1v1 = pd.DataFrame({"Info": out_layer_ns, "Score": dfm_errors})
print(res_multi1v1.to_string())
```

### (c)

> For the mixed pearlite + spheroidite test micrographs, apply the
> trained pairwise classifier for pearlite vs. spheroidite and the
> multilabel voting classifier. Print the predicted labels by these two
> classifiers side by side (one row for each test micrograph). Comment
> your results

``` {python}
ltestm = ltest[(ltest["primary_microconstituent"] == "pearlite") |\
      (ltest["primary_microconstituent"] == "spheroidite")]
feature_m = features_test["block5_pool"][ltestm.index.to_numpy(), :]
l = le.transform(["pearlite", "spheroidite"])

pred_pairs = clf1.clfs[l[0]][l[1]].predict(feature_m)
pred_multi = clf.predict(feature_m)

res_ps = pd.DataFrame({"Test Label": le.inverse_transform(ltestm["label"]),\
              "Pairwise (pearlite vs. spheroidite)": le.inverse_transform(pred_pairs.astype(int)),\
              "Multi-OnevsOne": le.inverse_transform(pred_multi)})

print(res_ps.to_string())
```

### (d)

> Now apply the multilabel classifier on the pearlite + Widmanst¨atten
> and martensite micrographs and print the predicted labels. Compare to
> the results in part (c)

``` {python}
df_micro2 = df_micro2[(df_micro2["primary_microconstituent"] == "pearlite+widmanstatten") |\
(df_micro2["primary_microconstituent"] == "martensite")]

# Encode labels
le2 = preprocessing.LabelEncoder()
le2.fit(df_micro2["primary_microconstituent"].unique())
list(le2.classes_)
```

``` {python}
dlabel2 = le2.transform(df_micro2["primary_microconstituent"])
df_micro2.insert(2, "label", dlabel2)
```

``` {python}
df_micro2
```

``` {python}
# Feature extraction with VGG16
if os.path.exists(os.path.join(dpath, "feature_test2.pkl")) == False:
    fs_test2 = np.zeros((df_micro2.shape[0], out_shapes[-1]))
    m = "block5_pool"
    for j, ph in tqdm(enumerate(df_micro2["path"])):
        x = load_image(ph)
        xb = extmodel[m].predict(x, verbose = 0) # silence output
        F = np.mean(xb,axis=(0,1,2))
        # Save features
        fs_test2[j, :] = F

    # Save data
    ## Create new files
    fs_test2_p = open(os.path.join(dpath, "feature_test2.pkl"), "wb")
    ## Write
    pickle.dump(fs_test2, fs_test2_p)
    ## Close files
    fs_test2_p.close()
```

``` {python}
#load data
fs_test2_p  = open(os.path.join(dpath, "feature_test2.pkl"), "rb")
fs_test2 = pickle.load(fs_test2_p) # train feature
fs_test2_p .close()
```

``` {python}
pred_multi2 = clf.predict(fs_test2)

res_ps2 = pd.DataFrame({"Test Label": le2.inverse_transform(df_micro2["label"]),\
              "Multi-OnevsOne": le.inverse_transform(pred_multi2)})

print(res_ps2.to_string())
```


::: {.content-hidden when-format="html"}

## References

:::
