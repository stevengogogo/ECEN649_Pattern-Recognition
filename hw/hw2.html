<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.242">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Shao-Ting Chiu (UIN:433002162)">
<meta name="dcterms.date" content="2023-01-20">

<title>ECEN649: Pattern Recognition - 7&nbsp; Homework 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../hw/hw3.html" rel="next">
<link href="../hw/hw1.html" rel="prev">
<link href="../img/favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Homework 2</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">ECEN649: Pattern Recognition</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/stevengogogo/ECEN649_Pattern-Recognition" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle sidebar-tool" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true"><strong>Lecture Notes</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ch1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 01</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ch2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 2: Optimal Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ch3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 3: Sampled-Based classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ch4.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 4: Parametric Classification</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true"><strong>Assignments</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Homework 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw2.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Homework 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Homework 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw4.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Homework 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw4_p6-12_right_cv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Homework4: Problem 6.12</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw5.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Homework 5</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ref.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#homework-description" id="toc-homework-description" class="nav-link active" data-scroll-target="#homework-description"><span class="toc-section-number">7.1</span>  Homework Description</a></li>
  <li><a href="#computational-enviromnent-setup" id="toc-computational-enviromnent-setup" class="nav-link" data-scroll-target="#computational-enviromnent-setup"><span class="toc-section-number">7.2</span>  Computational Enviromnent Setup</a>
  <ul class="collapse">
  <li><a href="#third-party-libraries" id="toc-third-party-libraries" class="nav-link" data-scroll-target="#third-party-libraries"><span class="toc-section-number">7.2.1</span>  Third-party libraries</a></li>
  <li><a href="#version" id="toc-version" class="nav-link" data-scroll-target="#version"><span class="toc-section-number">7.2.2</span>  Version</a></li>
  </ul></li>
  <li><a href="#problem-3.6-python-assignment" id="toc-problem-3.6-python-assignment" class="nav-link" data-scroll-target="#problem-3.6-python-assignment"><span class="toc-section-number">7.3</span>  Problem 3.6 (Python Assignment)</a></li>
  <li><a href="#problem-4.2" id="toc-problem-4.2" class="nav-link" data-scroll-target="#problem-4.2"><span class="toc-section-number">7.4</span>  Problem 4.2</a>
  <ul class="collapse">
  <li><a href="#sec-42a" id="toc-sec-42a" class="nav-link" data-scroll-target="#sec-42a"><span class="toc-section-number">7.4.1</span>  (a)</a></li>
  <li><a href="#sec-42b" id="toc-sec-42b" class="nav-link" data-scroll-target="#sec-42b"><span class="toc-section-number">7.4.2</span>  (b)</a></li>
  <li><a href="#c" id="toc-c" class="nav-link" data-scroll-target="#c"><span class="toc-section-number">7.4.3</span>  (c)</a></li>
  </ul></li>
  <li><a href="#problem-4.3" id="toc-problem-4.3" class="nav-link" data-scroll-target="#problem-4.3"><span class="toc-section-number">7.5</span>  Problem 4.3</a>
  <ul class="collapse">
  <li><a href="#a" id="toc-a" class="nav-link" data-scroll-target="#a"><span class="toc-section-number">7.5.1</span>  (a)</a></li>
  <li><a href="#b" id="toc-b" class="nav-link" data-scroll-target="#b"><span class="toc-section-number">7.5.2</span>  (b)</a></li>
  </ul></li>
  <li><a href="#problem-4.4" id="toc-problem-4.4" class="nav-link" data-scroll-target="#problem-4.4"><span class="toc-section-number">7.6</span>  Problem 4.4</a>
  <ul class="collapse">
  <li><a href="#a-1" id="toc-a-1" class="nav-link" data-scroll-target="#a-1"><span class="toc-section-number">7.6.1</span>  (a)</a></li>
  <li><a href="#sec-44b" id="toc-sec-44b" class="nav-link" data-scroll-target="#sec-44b"><span class="toc-section-number">7.6.2</span>  (b)</a></li>
  <li><a href="#c-1" id="toc-c-1" class="nav-link" data-scroll-target="#c-1"><span class="toc-section-number">7.6.3</span>  (c)</a></li>
  </ul></li>
  <li><a href="#problem-4.8-python-assignment" id="toc-problem-4.8-python-assignment" class="nav-link" data-scroll-target="#problem-4.8-python-assignment"><span class="toc-section-number">7.7</span>  Problem 4.8 (Python Assignment)</a>
  <ul class="collapse">
  <li><a href="#a-2" id="toc-a-2" class="nav-link" data-scroll-target="#a-2"><span class="toc-section-number">7.7.1</span>  (a)</a></li>
  <li><a href="#b-1" id="toc-b-1" class="nav-link" data-scroll-target="#b-1"><span class="toc-section-number">7.7.2</span>  (b)</a></li>
  <li><a href="#c-2" id="toc-c-2" class="nav-link" data-scroll-target="#c-2"><span class="toc-section-number">7.7.3</span>  (c)</a></li>
  <li><a href="#d" id="toc-d" class="nav-link" data-scroll-target="#d"><span class="toc-section-number">7.7.4</span>  (d)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Homework 2</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Shao-Ting Chiu (UIN:433002162) </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 20, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="homework-description" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="homework-description"><span class="header-section-number">7.1</span> Homework Description</h2>
<ul>
<li>Course: ECEN649, Fall2022</li>
</ul>
<blockquote class="blockquote">
<p>Problems from the book:</p>
<p>3.6 (10 pt)</p>
<p>4.2 (10 pt)</p>
<p>4.3 (10 pt)</p>
<p>4.4 (10 pt)</p>
<p>4.8 (20 pt)</p>
</blockquote>
<ul>
<li>Deadline: <code>Oct. 12th, 11:59 am</code></li>
</ul>
</section>
<section id="computational-enviromnent-setup" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="computational-enviromnent-setup"><span class="header-section-number">7.2</span> Computational Enviromnent Setup</h2>
<section id="third-party-libraries" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="third-party-libraries"><span class="header-section-number">7.2.1</span> Third-party libraries</h3>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys <span class="co"># system information</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="co"># plotting</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> st <span class="co"># scientific computing</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd <span class="co"># data managing</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np <span class="co"># numerical comuptation</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numba</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn <span class="im">as</span> sk</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> linalg <span class="im">as</span> LA</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.optimize <span class="im">as</span> opt</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> inv, det</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> multivariate_normal <span class="im">as</span> mvn</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> binomial <span class="im">as</span> binom</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.discriminant_analysis <span class="im">import</span> LinearDiscriminantAnalysis <span class="im">as</span> LDA <span class="co">#problem 4.8</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Matplotlib setting</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'text.usetex'</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>matplotlib.rcParams[<span class="st">'figure.dpi'</span>]<span class="op">=</span> <span class="dv">300</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">20221011</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="version" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="version"><span class="header-section-number">7.2.2</span> Version</h3>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sys.version)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(matplotlib.__version__)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sp.__version__)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.__version__)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pd.__version__)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sk.__version__)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3.8.14 (default, Sep  6 2022, 23:26:50) 
[Clang 13.1.6 (clang-1316.0.21.2.5)]
3.3.1
1.6.2
1.19.1
1.1.1
1.1.2</code></pre>
</div>
</div>
<hr>
</section>
</section>
<section id="problem-3.6-python-assignment" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="problem-3.6-python-assignment"><span class="header-section-number">7.3</span> Problem 3.6 (Python Assignment)</h2>
<blockquote class="blockquote">
<p>Using the synthetic data model in Section A8.1 for the homoskedastic case with <span class="math inline">\(\mu_0 = (0,\dots,0)\)</span>, <span class="math inline">\(\mu_1=(1,\dots,1)\)</span>, <span class="math inline">\(P(Y=0)=P(Y=1)\)</span>, and <span class="math inline">\(k=d\)</span> (independent features), generate a large number (e.g., <span class="math inline">\(M=1000\)</span>) of training data sets for each sample size <span class="math inline">\(n=20\)</span> to <span class="math inline">\(n=100\)</span>, in steps of <span class="math inline">\(10\)</span>, with <span class="math inline">\(d=2,5,8\)</span>, and <span class="math inline">\(\sigma=1\)</span>. Obtain an approximation of the expected classification error <span class="math inline">\(E[\epsilon_n]\)</span> of the nearest centroid classifier in each case by averaging <span class="math inline">\(\epsilon_n\)</span>, computed using the exact formula (3.13), over the <span class="math inline">\(M\)</span> synthetic training data sets. Plot <span class="math inline">\(E[\epsilon_n]\)</span> as a function of the sample size, for <span class="math inline">\(d=2,5,8\)</span> (join the individual points with lines to obtain a smooth curve). Explain what you see.</p>
</blockquote>
<ul>
<li>The formula in <span class="citation" data-cites="braga2020fundamentals">Braga-Neto (<a href="../ref.html#ref-braga2020fundamentals" role="doc-biblioref">2020, 56</a>, Eq. 3.13)</span></li>
<li><span class="math inline">\(\epsilon_n = \frac{1}{2}\left(\Phi\left(\frac{a_{n}^{T}\hat{\mu}_0 + b_n}{\|a_n\|}\right) + \Phi\left(-\frac{a_{n}^{T}\hat{\mu}_1 + b_n}{\|a_n\|}\right) \right)\)</span>
<ul>
<li><span class="math inline">\(\mu_0 = (0,\dots, 0)\)</span>; <span class="math inline">\(\hat{\mu}_0 = \frac{1}{N_0}\sum^{n}_{i=1}X_i I_{Y_i=0}\)</span></li>
<li><span class="math inline">\(\mu_1 = (1,\dots,1)\)</span>; <span class="math inline">\(\hat{\mu}_1 = \frac{1}{N_1}\sum^{n}_{i=1}X_i I_{Y_i=1}\)</span></li>
<li><span class="math inline">\(a_n = \hat{\mu}_1 - \hat{\mu}_0\)</span></li>
<li><span class="math inline">\(b_n = -\frac{(\hat{\mu}_1 - \hat{\mu}_0)^{T}(\hat{\mu}_1 + \hat{\mu}_0)}{2}\)</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
</ul></li>
<li>As shown in <a href="#fig-p36">Figure&nbsp;<span>7.1</span></a>, the error rate converges to optimal error as the sample size increases.</li>
</ul>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> norml(v):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> LA.norm(v, <span class="dv">2</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_an(hm0,hm1):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hm1 <span class="op">-</span> hm0</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_bn(hm0,hm1):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="bu">float</span>((hm1 <span class="op">-</span> hm0).T <span class="op">@</span> (hm1<span class="op">+</span>hm0))<span class="op">/</span><span class="dv">2</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> epsilon(hmu0, hmu1, mu0, mu1,p0<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    p1 <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>p0</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    an <span class="op">=</span> get_an(hmu0, hmu1)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    bn <span class="op">=</span> get_bn(hmu0, hmu1)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    epsilon0 <span class="op">=</span> st.norm.cdf( (<span class="bu">float</span>(an.T<span class="op">*</span> mu0) <span class="op">+</span> bn)<span class="op">/</span>norml(an))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    epsilon1 <span class="op">=</span> st.norm.cdf(<span class="op">-</span> (<span class="bu">float</span>(an.T<span class="op">*</span>mu1)<span class="op">+</span> bn)<span class="op">/</span>norml(an))</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p0<span class="op">*</span>epsilon0 <span class="op">+</span> p1<span class="op">*</span>epsilon1</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GaussianDataGen:</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n, d, s<span class="op">=</span><span class="dv">1</span>, mu<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> n</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d <span class="op">=</span> d</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mu <span class="op">=</span> np.matrix(np.ones(d) <span class="op">*</span> mu).T</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.s <span class="op">=</span> s</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cov <span class="op">=</span> <span class="va">self</span>.get_cov()</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_cov(<span class="va">self</span>):</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.identity(<span class="va">self</span>.d) <span class="op">*</span> <span class="va">self</span>.s</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>):</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.random.normal(<span class="va">self</span>.mu[<span class="dv">0</span>][<span class="dv">0</span>], <span class="va">self</span>.s, size<span class="op">=</span> (<span class="va">self</span>.d, <span class="va">self</span>.n))</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        hmuV <span class="op">=</span> np.mean(data, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.matrix(hmuV).T</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cal_eps(dg0, dg1, p0<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    hmuV0 <span class="op">=</span> dg0.sample()</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    hmuV1 <span class="op">=</span> dg1.sample()</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    mu0 <span class="op">=</span> np.matrix(np.zeros(dg0.d)).T</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    mu1 <span class="op">=</span> np.matrix(np.ones(dg1.d)).T</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> epsilon(hmuV0, hmuV1, mu0, mu1,p0<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>cal_eps_func <span class="op">=</span> np.vectorize(cal_eps)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> exp_try_nd(n, d, s<span class="op">=</span><span class="dv">1</span>,M<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    gX0 <span class="op">=</span> GaussianDataGen(n<span class="op">=</span>n, d<span class="op">=</span>d, s<span class="op">=</span> s,mu<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    gX1 <span class="op">=</span> GaussianDataGen(n<span class="op">=</span>n, d<span class="op">=</span>d, s<span class="op">=</span> s, mu<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    <span class="co">#eps = cal_eps_func([gX0]*M, gX1)</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> [cal_eps_func(gX0, gX1) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,M)]</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(eps)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>exp_try_nd_func <span class="op">=</span> np.vectorize(exp_try_nd)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bayes_ncc(mu0, mu1):</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> st.norm.cdf(<span class="op">-</span> norml(mu1<span class="op">-</span>mu0)<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>ns <span class="op">=</span> np.arange(<span class="dv">20</span>,<span class="dv">100</span>, <span class="dv">10</span>)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>dres <span class="op">=</span> {<span class="dv">2</span>:[],<span class="dv">5</span>:[],<span class="dv">8</span>:[]}</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> dres.keys():</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    dres[k]<span class="op">=</span> exp_try_nd_func(ns,k)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimal error</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>opts <span class="op">=</span> [bayes_ncc(np.zeros(k), np.ones(k)) <span class="cf">for</span> k <span class="kw">in</span> dres.keys()]</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>sts <span class="op">=</span> [<span class="st">"-"</span>, <span class="st">"--"</span>, <span class="st">":"</span>]</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i, k) <span class="kw">in</span> <span class="bu">enumerate</span>(dres.keys()):</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    ax.plot(ns, dres[k], <span class="st">'o'</span>,label<span class="op">=</span><span class="st">"d=</span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(k))</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>    ax.axhline(y<span class="op">=</span> opts[i], label<span class="op">=</span><span class="st">"Optimal d = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(k), color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span>sts[i])</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"n"</span>)</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"$E[</span><span class="ch">\\</span><span class="st">epsilon_n]$"</span>)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>ax.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-p36" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-p36-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.1: Error of En</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="problem-4.2" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="problem-4.2"><span class="header-section-number">7.4</span> Problem 4.2</h2>
<blockquote class="blockquote">
<p>A common method to extend binary classification rules to <span class="math inline">\(K\)</span> classes, <span class="math inline">\(K&gt;2\)</span>, is the <em>one-vs-one approach</em>, in which <span class="math inline">\(\frac{K(K-1)}{2}\)</span> classifiers are trained between all pairs of classes<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, and a majority vote of assigned labels is taken.</p>
</blockquote>
<section id="sec-42a" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="sec-42a"><span class="header-section-number">7.4.1</span> (a)</h3>
<blockquote class="blockquote">
<p>Formulate a multiclass version of parametric plug-in classification using the one-vs-one approach.</p>
</blockquote>
<p>Let <span class="math inline">\(\psi^{*}_{i,j}\)</span> be a one-one classifiers that <span class="math inline">\(i\neq j\)</span>, and <span class="math inline">\(\{(i,j)| i\in [1,k], j \in [1,k], i\neq j\}\)</span>. For <span class="math inline">\(K\)</span> classes, there are <span class="math inline">\(K(K-1)\)</span> classifiers; for each classifier <span class="math inline">\(\psi^{*}_{i,j}\)</span> and <span class="math inline">\(x\in R^d\)</span>,</p>
<p><span class="math display">\[\begin{equation}
    \psi^{*}_{ij,n} =
    \begin{cases}
        1, &amp; D_{ij, n}(x) &gt; k_{ij,n}\\
        0, &amp; \text{otherwise}
    \end{cases}
\end{equation}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(D_{ij,n}(x) = \ln \frac{p(x|\theta_{i,n})}{p(x|\theta_{j,n})}\)</span></li>
<li><span class="math inline">\(k_{ij,n} = \ln\frac{P(Y=j)}{P(Y=i)}\)</span></li>
<li>Noted that feature-label distribution is expressed via a familty of PDF <span class="math inline">\(\{p(x|\theta_i) | \theta \in \Theta \subseteq R^m\}\)</span>, for <span class="math inline">\(i=1,\dots,K\)</span>.</li>
<li><span class="math inline">\(\psi^{*}_{ij,n} = 1\otimes \psi^{*}_{ji,n}\)</span>. These two are similar classifiers with inverted outcome.</li>
</ul>
<p>Let <span class="math inline">\(\psi^{*}_{i,n} = \sum_{j\neq i} \psi^{*}_{ij,n}\)</span>, and the one-vs-one classifier is</p>
<p><span id="eq-psi-max"><span class="math display">\[\psi^{*}_{n}(x) = \arg\max_{k=1,\dots,K} \psi^{*}_{k,n} \tag{7.1}\]</span></span></p>
</section>
<section id="sec-42b" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="sec-42b"><span class="header-section-number">7.4.2</span> (b)</h3>
<blockquote class="blockquote">
<p>Show that if the threshold <span class="math inline">\(k_{ij,n}\)</span> between classes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is given by <span class="math inline">\(\ln\frac{\hat{c}_j}{\hat{c}_i}\)</span>, then the one-vs-one parametric classification rule is equivalent to the simple decision. <span class="math display">\[\psi_{n}(x) = \arg\max_{k=1,...,K} \hat{c}_{k} p(x|\theta_{k,n}), x\in R^d\]</span> (For simplicity, you may ignore the possibility of ties.)</p>
</blockquote>
<p><span class="math display">\[\begin{align}
    \ln \frac{ p(x|\theta_{i,n})}{p(x|\theta_{j,n})} &amp;&gt; k_{ij,n} = \ln\frac{\hat{c_j}}{\hat{c_i}}\\
    \ln p(x|\theta_{i,n}) - \ln p(x|\theta_{j,n}) &amp;&gt; \ln \hat{c}_j - \ln \hat{c}_{i}\\
    \hat{c}_i p(x|\theta_{i,n}) &amp;&gt; \hat{c}_j p(x|\theta_{j,n})
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
    \psi^{*}_{ij, n}%
     &amp;= \begin{cases}
        1, &amp; \hat{c}_i p(x|\theta_{i,n}) &gt; \hat{c}_j p(x|\theta_{j,n})\\
        0, &amp; otherwise
    \end{cases}\\
    &amp;= I_{\hat{c}_i p(x|\theta_{i,n}) &gt; \hat{c}_j p(x|\theta_{j,n})}
\end{align}\]</span></p>
<p>Then,</p>
<p><span class="math display">\[\begin{align}
    \psi^{*}_{i,n}%
    &amp;=\sum_{j\neq i} \psi^{*}_{ij,n}\\
    &amp;= \sum_{j\neq i}  I_{\hat{c}_i p(x|\theta_{i,n}) &gt; \hat{c}_j p(x|\theta_{j,n})}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
    \psi^{*}_{n}(x)%
    &amp;= \arg\max_{k=1,\dots,K}\psi^{*}_{k,n}\\
    &amp;= \arg\max_{k=1,\dots,K}\sum_{j\neq i}\psi^{*}_{kj,n}\\
    &amp;= \arg\max_{k=1,\dots,K} \sum_{j\neq i}  I_{\hat{c}_k p(x|\theta_{k,n}) &gt; \hat{c}_j p(x|\theta_{j,n})}\\
\end{align}\]</span></p>
<p>Let <span class="math inline">\(\psi^{*}_{n}(x) =\kappa\)</span>, that means <span class="math inline">\(\psi^{*}_{\kappa,n}\)</span> is the maximum among <span class="math inline">\(\{\psi^{*}_{j,n}|j= (1,\dots, K)\}\)</span>. Assume there is an <span class="math inline">\(s\neq \kappa\)</span> s.t. <span class="math inline">\(\hat{c}_s p(x|\theta_{s,n}) &gt; \hat{c}_\kappa p(x|\theta_{\kappa,n}) &gt;\)</span> the rests. Thus, <span class="math inline">\(I_{\hat{c}_s p(x|\theta_{s,n}) &gt; \hat{c}_\kappa p(x|\theta_{\kappa,n})} = 1\)</span></p>
<p><span class="math display">\[\psi^{*}_{s,n} = \sum_{j\neq s}  I_{\hat{c}_s p(x|\theta_{s,n}) &gt; \hat{c}_j p(x|\theta_{j,n})} = \underbrace{\sum_{j\neq s; j\neq \kappa}  I_{\hat{c}_s p(x|\theta_{s,n}) &gt; \hat{c}_j p(x|\theta_{j,n})}}_{=a}+\underbrace{I_{\hat{c}_s p(x|\theta_{s,n}) &gt; \hat{c}_\kappa p(x|\theta_{\kappa,n})}}_{=1}\]</span></p>
<p><span class="math display">\[\psi^{*}_{\kappa,n} = \sum_{j\neq \kappa} I_{\hat{c}_\kappa p(x|\theta_{\kappa,n}) &gt; \hat{c}_j p(x|\theta_{j,n})} = \underbrace{\sum_{j\neq \kappa; j\neq s}I_{\hat{c}_\kappa p(x|\theta_{\kappa,n}) &gt; \hat{c}_j p(x|\theta_{j,n})}}_{=a} + \underbrace{I_{\hat{c}_\kappa p(x|\theta_{\kappa,n}) &gt; \hat{c}_s p(x|\theta_{s,n})}}_{=0}\]</span></p>
<p>where <span class="math inline">\(a\)</span> is a nonnegative number. That means</p>
<p><span id="eq-42b"><span class="math display">\[\psi^{*}_{\kappa,n} &lt; \psi^{*}_{s,n} \tag{7.2}\]</span></span></p>
<p><span class="math inline">\(\psi^{*}_{\kappa,n}\)</span> is not the maximum. <a href="#eq-42b">Equation&nbsp;<span>7.2</span></a> is contradict to the statement that <span class="math inline">\(\psi^{*}_{n}(x)=\kappa\)</span>. In conclusion, <span class="math inline">\(\hat{c}_k p(x|\theta_{\kappa}, n)\)</span> is the maximum if <span class="math inline">\(\psi_n(x) = k\)</span>.</p>
</section>
<section id="c" class="level3" data-number="7.4.3">
<h3 data-number="7.4.3" class="anchored" data-anchor-id="c"><span class="header-section-number">7.4.3</span> (c)</h3>
<blockquote class="blockquote">
<p>Applying the approach in items <a href="#sec-42a">(a)</a> and <a href="#sec-42b">(b)</a>, formulate a multiclass version of Gaussian discriminant analysis. In the case of multiclass NMC, with all thresholds equal to zero, how does the decision boundary look like?</p>
</blockquote>
<p>For Gaussian discriminant analyis, the discriminant is defined as</p>
<p><span class="math display">\[\hat{D}^{*}_{ij}(x) = \frac{1}{2}(x-\hat{\mu}_i)^{T}\hat{\Sigma}^{-1}_{i}(x-\hat{\mu}_i)-\frac{1}{2}(x-\hat{\mu}_j)^{T}\hat{\Sigma}^{-1}_{j}(x-\hat{\mu}_j) + \frac{1}{2}\ln\frac{\det(\hat{\Sigma}_i)}{\det(\hat{\Sigma}_j)}\]</span></p>
<p><span class="math display">\[\begin{equation}
    \psi^{*}_{ij, n}(x) = I_{\hat{D}^{*}_{ij}(x) &gt; 0}
\end{equation}\]</span></p>
<p><span class="math display">\[\psi^{*}_{n} = \arg\max_{k=1,\dots,K} \hat{c}_k p(x|\hat{\mu}_{k,n}, \hat{\Sigma}_{k,n}) = \arg\max_{k=1,\dots,K} Normal(x; \hat{\mu}_{k,n}, \hat{\Sigma}_{k,n})\]</span></p>
<p>For NMC case, suppose there are 3 classes with <span class="math inline">\(d=2\)</span>.</p>
<ul>
<li><span class="math inline">\(K=3\)</span></li>
<li>Number of classifiers: <span class="math inline">\(\frac{3\cdot 2}{2}=3\)</span>
<ul>
<li><span class="math inline">\(\psi^{*}_{1,2}, \psi^{*}_{1,3}, \psi^{*}_{2,3}\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[\begin{equation}
    boundary =
    \begin{cases}
        \hat{\Sigma}^{-1}(\hat{\mu}_1 - \hat{\mu}_2)\begin{bmatrix} x_1\\ x_2 \end{bmatrix} +(\hat{\mu}_2 - \hat{\mu}_1)^T \hat{\Sigma}^{-1}\left(\frac{\hat{\mu}_1+\hat{\mu}_2}{2}\right) = 0\\
        \hat{\Sigma}^{-1}(\hat{\mu}_1 - \hat{\mu}_3)\begin{bmatrix} x_1\\ x_2 \end{bmatrix} +(\hat{\mu}_3 - \hat{\mu}_1)^T \hat{\Sigma}^{-1}\left(\frac{\hat{\mu}_1+\hat{\mu}_3}{2}\right) = 0\\
        \hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_3)\begin{bmatrix} x_1\\ x_2 \end{bmatrix} +(\hat{\mu}_3 - \hat{\mu}_2)^T \hat{\Sigma}^{-1}\left(\frac{\hat{\mu}_2+\hat{\mu}_3}{2}\right) = 0\\
    \end{cases}
\end{equation}\]</span></p>
<div id="fig-p42c" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/hw2_p42c.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.2: Homoskedastic cases</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="problem-4.3" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="problem-4.3"><span class="header-section-number">7.5</span> Problem 4.3</h2>
<blockquote class="blockquote">
<p>Under the general Gaussian model <span class="math inline">\(p(x|Y=0)\sim \mathcal{N}_d(\mu_0, \sum_0)\)</span> and <span class="math inline">\(p(x|Y=1)\sim \mathcal{N}_d(\mu_1, \sum_1)\)</span>, the classification error <span class="math inline">\(\epsilon_n = P(\psi_n(X)\neq Y| S_n)\)</span> of <em>any</em> linear classifier in the form</p>
<p><span class="math display">\[\begin{equation}
   \psi_{n}(x) =
   \begin{cases}
       1,&amp; a_{n}^{T}x + b_n &gt; 0,\\
       0,&amp; \text{otherwise}
   \end{cases}
\end{equation}\]</span></p>
<p>(examples discussed so far include LDA and its variants, and the logistic classifier) can be readily computed in terms of <span class="math inline">\(\Phi\)</span> (the CDF of a standard normal random variable), the classifier parameters <span class="math inline">\(a_n\)</span> and <span class="math inline">\(b_n\)</span>, and the distributional parameters <span class="math inline">\(c=P(Y=1)\)</span>, <span class="math inline">\(\mu_0\)</span>, <span class="math inline">\(\mu_1\)</span>, <span class="math inline">\(\Sigma_0\)</span>, and <span class="math inline">\(\Sigma_1\)</span>.</p>
</blockquote>
<section id="a" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="a"><span class="header-section-number">7.5.1</span> (a)</h3>
<blockquote class="blockquote">
<p>Show that</p>
<p><span class="math display">\[\epsilon_n = (1-c)\Phi\left( \frac{a_{n}^{T}\mu_0 + b_n}{\sqrt{a_{n}^{T}\Sigma_0 a_n}} \right) + c \Phi\left( -\frac{a^{T}_{n}\mu_1 + b_n}{\sqrt{a_{n}^{T}\Sigma_1 a_n}}\right)\]</span></p>
<p>Hint: the discriminant <span class="math inline">\(a^{T}_{n}x+b_n\)</span> has a simple Gaussian distribution in each class.</p>
</blockquote>
<p>From <span class="citation" data-cites="braga2020fundamentals">Braga-Neto (<a href="../ref.html#ref-braga2020fundamentals" role="doc-biblioref">2020</a>, Eq. 2.34)</span>,</p>
<p><span class="math display">\[\epsilon^{*} = \underbrace{P(Y=0)}_{=1-c}\epsilon^{0}[\psi^*] + \underbrace{P(Y=1)}_{=c}\epsilon^{1}[\psi^*]\]</span></p>
<p><span class="math display">\[\begin{align}
\epsilon^{0}[\psi^{*}]%
&amp;= P(a_{n}^{T}x+b_n &gt; 0 | Y=0)\\
\end{align}\]</span></p>
<p>Use the affine property of Gaussian distribution described in <span class="citation" data-cites="braga2020fundamentals">Braga-Neto (<a href="../ref.html#ref-braga2020fundamentals" role="doc-biblioref">2020</a>)</span> [pp.&nbsp;307. G4]<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<ul>
<li><span class="math inline">\(a^{T}_{n}x+b_n | Y=0 \sim N(a^{T}_{n}\mu_0+b_n, \underbrace{a^{T}_{n}\Sigma_{0}a_{n}}_{\sigma^2})\)</span></li>
</ul>
<p><span class="math display">\[\begin{align}
\epsilon^{0}[\psi^{*}]%
&amp;= 1 - P(a^{T}_{n}x+b_n \leq 0 | Y=0)\\
&amp;= 1 - \Phi\left(\frac{0 - (a^{T}_{n}\mu_0 + b_n)}{\sqrt{a^{T}_{n}\Sigma_{0}a_{n}}}\right)\\
&amp;= 1 - \Phi\left(-\frac{a^{T}_{n}\mu_0 + b_n}{\sqrt{a^{T}_{n}\Sigma_{0}a_{n}}}\right)\\
&amp;= \Phi\left(\frac{a^{T}_{n}\mu_0 + b_n}{\sqrt{a^{T}_{n}\Sigma_{0}a_{n}}}\right)
\end{align}\]</span></p>
<p>Similarly,</p>
<p><span class="math display">\[\begin{align}
    \epsilon^{1}(\psi^{*})%
    &amp;= P(a_{n}^{T}x+b_n &lt; 0 | Y=1)\\
    &amp;= \Phi\left(\frac{0 - (a^{T}_{n}\mu_1 + b_n)}{\sqrt{a^{T}_{n}\Sigma_1 a_n}}\right)\\
    &amp;= \Phi\left(-\frac{a^{T}_{n}\mu_1 + b_n}{\sqrt{a^{T}_{n}\Sigma_1 a_n}}\right)
\end{align}\]</span></p>
<p>Combining together,</p>
<p><span class="math display">\[\begin{align}
    \epsilon^{*}%
     &amp;= \underbrace{P(Y=0)}_{=1-c}\epsilon^{0}[\psi^*] + \underbrace{P(Y=1)}_{=c}\epsilon^{1}[\psi^*]\\
     &amp;= (1-c)\epsilon^{0}[\psi^*] + c\epsilon^{1}[\psi^*]\\
     &amp;= (1-c)\Phi\left(\frac{a^{T}_{n}\mu_0 + b_n}{\sqrt{a^{T}_{n}\Sigma_{0}a_{n}}}\right) + c\Phi\left(-\frac{a^{T}_{n}\mu_1 + b_n}{\sqrt{a^{T}_{n}\Sigma_1 a_n}}\right)
\end{align}\]</span></p>
</section>
<section id="b" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="b"><span class="header-section-number">7.5.2</span> (b)</h3>
<blockquote class="blockquote">
<p>Compute the errors of the NMC, LDA, and DLDA classifiers in Example 4.2 if <span class="math inline">\(c=1/2\)</span>, <span class="math display">\[\begin{equation*}
\mu_0 =
\begin{bmatrix}
2\\
3
\end{bmatrix},
\mu_1 =
\begin{bmatrix}
6\\
5
\end{bmatrix},
\Sigma_0 =
\begin{bmatrix}
1 &amp; 1\\
1 &amp; 2
\end{bmatrix},
\text{ and }
\Sigma_1 =
\begin{bmatrix}
4 &amp; 0\\
0 &amp; 1
\end{bmatrix}
\end{equation*}\]</span> Which classifier does the best?</p>
</blockquote>
<p>As shown in <a href="#tbl-43b">Table&nbsp;<span>7.1</span></a>, NMC has the lowest Bayes error.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> epsilon_general(an, bn, c, mu0, mu1, sig0, sig1):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    e0 <span class="op">=</span> st.norm.cdf(<span class="op">\</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span>(an.T <span class="op">@</span> mu0 <span class="op">+</span> bn)<span class="op">/\</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        np.sqrt(<span class="bu">float</span>(an.T <span class="op">@</span> sig0 <span class="op">@</span> an)))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    e1 <span class="op">=</span> st.norm.cdf(<span class="op">\</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="op">-</span><span class="bu">float</span>(an.T <span class="op">@</span> mu1 <span class="op">+</span> bn)<span class="op">/\</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        np.sqrt(<span class="bu">float</span>(an.T <span class="op">@</span> sig1 <span class="op">@</span>an)) )</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">1</span><span class="op">-</span>c)<span class="op">*</span>e0 <span class="op">+</span> c<span class="op">*</span>e1</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>truth <span class="op">=</span> {</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"c"</span>: <span class="fl">0.5</span>,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu0"</span>: np.matrix([[<span class="dv">2</span>],[<span class="dv">3</span>]]),</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu1"</span>: np.matrix([[<span class="dv">6</span>],[<span class="dv">5</span>]]),</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"sig0"</span>: np.matrix([[<span class="dv">1</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">2</span>]]),</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"sig1"</span>: np.matrix([[<span class="dv">4</span>,<span class="dv">0</span>],[<span class="dv">0</span>,<span class="dv">1</span>]]),</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>meth <span class="op">=</span> {</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"NMC"</span>:{</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">"an"</span>: np.matrix([[<span class="dv">4</span>],[<span class="dv">2</span>]]),</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">"bn"</span>: <span class="op">-</span><span class="dv">24</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">"LDA"</span>:{</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="st">"an"</span>: <span class="dv">3</span><span class="op">/</span><span class="dv">7</span><span class="op">*</span>np.matrix([[<span class="dv">5</span>], [<span class="dv">3</span>]]),</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">"bn"</span>: <span class="op">-</span><span class="dv">96</span><span class="op">/</span><span class="dv">7</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">"DLDA"</span>:{</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        <span class="st">"an"</span>: <span class="dv">2</span><span class="op">/</span><span class="dv">5</span><span class="op">*</span>np.matrix([[<span class="dv">6</span>],[<span class="dv">5</span>]]),</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        <span class="st">"bn"</span>: <span class="op">-</span><span class="dv">88</span><span class="op">/</span><span class="dv">5</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>berrors <span class="op">=</span> np.zeros(<span class="bu">len</span>(meth.keys()))</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i,k) <span class="kw">in</span> <span class="bu">enumerate</span>(meth.keys()):</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    berrors[i] <span class="op">=</span> epsilon_general(<span class="op">**</span>meth[k], <span class="op">**</span>truth)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Method"</span>: <span class="bu">list</span>(meth.keys()),<span class="op">\</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Bayes Error"</span>: berrors}).sort_values(<span class="op">\</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Bayes Error"</span>], ascending<span class="op">=</span>[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div id="tbl-43b" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;7.1:  Bayes Errors of NMC, LDA and DLDA </caption>
  <thead>
    <tr>
      <th></th>
      <th>Method</th>
      <th>Bayes Error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NMC</td>
      <td>0.084775</td>
    </tr>
    <tr>
      <th>1</th>
      <td>LDA</td>
      <td>0.085298</td>
    </tr>
    <tr>
      <th>2</th>
      <td>DLDA</td>
      <td>0.087606</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="problem-4.4" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="problem-4.4"><span class="header-section-number">7.6</span> Problem 4.4</h2>
<blockquote class="blockquote">
<p>Even in the Gaussian case, the classification error of quadratic classifiers in general require numerical integration for its computation. In some special simple cases, however, it is possible to obtain exact solutions. Assume a two-dimensional Gaussian problem with <span class="math inline">\(P(Y=1)=\frac{1}{2}\)</span>, <span class="math inline">\(\mu_0=\mu_1 = 0\)</span>, <span class="math inline">\(\Sigma_0=\sigma_{0}^{2}I_2\)</span>, and <span class="math inline">\(\Sigma_1 = \sigma^{2}_{1}I_2\)</span>. For definiteness, assume that <span class="math inline">\(\sigma_0 &lt; \sigma_1\)</span>.</p>
</blockquote>
<section id="a-1" class="level3" data-number="7.6.1">
<h3 data-number="7.6.1" class="anchored" data-anchor-id="a-1"><span class="header-section-number">7.6.1</span> (a)</h3>
<blockquote class="blockquote">
<p>Show that the Bayes classifier is given by <span class="math display">\[\begin{equation}
\psi^{*}(x) =
\begin{cases}
1, &amp;\|x\| &gt; r^{*},\\
0, &amp;\text{ otherwise },
\end{cases}
\quad \text{ where } r^{*} = \sqrt{2\left(\frac{1}{\sigma_{0}^{2}} - \frac{1}{\sigma_{1}^{2}}\right)^{-1}\ln\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}}}
\end{equation}\]</span> In particular, the optimal decision boundary is a circle of radius <span class="math inline">\(r^{*}\)</span>.</p>
</blockquote>
<p>The inverted <span class="math inline">\(\Sigma_1\)</span> and <span class="math inline">\(\Sigma_2\)</span> are<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p><span class="math display">\[\begin{align}
    \Sigma_0 &amp;= \sigma_{0}^2 I_2 = \begin{bmatrix}
        \sigma_{0}^2 &amp; 0 \\
        0 &amp; \sigma_{0}^2
    \end{bmatrix}\\
    \Sigma_{0}^{-1} &amp;= \frac{1}{\sigma_{0}^{4}} \begin{bmatrix}
        \sigma_{0}^2 &amp; 0 \\
        0 &amp; \sigma_{0}^2
    \end{bmatrix} = \sigma_{0}^{-2}\begin{bmatrix}
            1 &amp; 0\\
            0 &amp; 1
        \end{bmatrix} = \sigma^{-2}_{0}I_2\\
    \Sigma^{-1}_{1} &amp;= \sigma^{-2}_{1}I_2
\end{align}\]</span></p>
<p>Use the derivation in <span class="citation" data-cites="braga2020fundamentals">Braga-Neto (<a href="../ref.html#ref-braga2020fundamentals" role="doc-biblioref">2020, 74</a>)</span>,</p>
<p><span class="math display">\[\begin{equation}
    A_n = \begin{bmatrix}
        a_{11} &amp; a_{12}\\
        a_{12} &amp; a_{22}
    \end{bmatrix} = \frac{-1}{2} \Sigma_{1}^{-1} - \Sigma_{0}^{-1} = \frac{-1}{2}(\sigma_{1}^{-2} - \sigma_{0}^{-2}) \begin{bmatrix}
        1 &amp; 0\\
        0 &amp; 1
    \end{bmatrix}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{align}
    b_n &amp;= \begin{bmatrix}
        b_{n,1}\\
        b_{n,2}
    \end{bmatrix}
    = \Sigma_{1}^{-1}\underbrace{\mu_1}_{=0} - \Sigma_{0}^{-1}\underbrace{\mu_{0}}_{=0}\\
        &amp;= \begin{bmatrix}
            0\\
            0
        \end{bmatrix}
\end{align}\]</span></p>
<p><span class="math display">\[c = -\frac{1}{2}\ln\frac{|\Sigma_1|}{|\Sigma_0|} = \frac{-1}{2}\ln\frac{\sigma_{1}^{4}}{\sigma_{0}^{4}} = -\ln \frac{\sigma_{1}^2}{\sigma_{0}^2}\]</span></p>
<p>According to <span class="citation" data-cites="braga2020fundamentals">Braga-Neto (<a href="../ref.html#ref-braga2020fundamentals" role="doc-biblioref">2020</a>, Eq. 4.26)</span>, the 2-dimensional QDA decision boundary is</p>
<p><span class="math display">\[\begin{align}
    D(x) = a_{11}x^{2}_1 + 2 a_{12}x_1x_2 + a_{22}x^{2}_{2} + b_1 x_1 + b_2 x_2 + c &amp;= 0\\
    a_{11}(x_{1}^{2} + x_{2}^{2}) &amp;= \ln \frac{\sigma_{1}^2}{\sigma_{0}^2}\\
    x^{2}_{1} + x^{2}_{2} &amp;= 2(\frac{1}{\sigma^{2}_{0}} - \frac{1}{\sigma^{2}_{1}})^{-1}\ln\frac{\sigma_{1}^2}{\sigma_{0}^2}\\
    r^{*} = \sqrt{x^{2}_{1} + x^{2}_{2}} &amp;= \sqrt{2(\frac{1}{\sigma^{2}_{0}} - \frac{1}{\sigma^{2}_{1}})^{-1}\ln\frac{\sigma_{1}^2}{\sigma_{0}^2}}
\end{align}\]</span></p>
<p>Noted that <span class="math inline">\(\left(\frac{1}{\sigma^{2}_{0}} - \frac{1}{\sigma^{2}_{1}}\right) &gt; 0\)</span> because <span class="math inline">\(\sigma_0 &lt; \sigma_1\)</span></p>
<p>For any point <span class="math inline">\(\|x_j\| &gt; r^{*}\)</span>, the discriminant (<span class="math inline">\(D\)</span>) is larger than <span class="math inline">\(0\)</span>, and <span class="math inline">\(\psi^{*}(x_j) = 1\)</span>.</p>
</section>
<section id="sec-44b" class="level3" data-number="7.6.2">
<h3 data-number="7.6.2" class="anchored" data-anchor-id="sec-44b"><span class="header-section-number">7.6.2</span> (b)</h3>
<blockquote class="blockquote">
<p>Show that the corresponding Bayes error is given by <span class="math display">\[\epsilon^{*} = \frac{1}{2} - \frac{1}{2}(\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}} - 1)e^{-(1-\frac{\sigma^{2}_{0}}{\sigma^{2}_{1}})^{-1}\ln \frac{\sigma^{2}_{1}}{\sigma^{2}_{0}}}\]</span> In particular, the Bayes error is a function only of the ratio of variances <span class="math inline">\(\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}}\)</span>, and <span class="math inline">\(\epsilon^{*}\rightarrow 0\)</span> as <span class="math inline">\(\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}} \rightarrow \infty\)</span>.</p>
<p>Hint: use polar coordinates to solve the required integrals analytically.</p>
</blockquote>
<p><strong>Part I: Definition of errors</strong></p>
<p><span class="math display">\[\begin{align}
    \epsilon^{0}[\psi^{*}]
    &amp;= P(D^{*}(X)&gt;k^{*}|Y=0)\\
    &amp;= P(\|x\|&gt;r^{*} | Y=0)
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
    \epsilon^{1}[\psi^{*}]
    &amp;= P(D^{*}(X)\leq k^{*}|Y=1)\\
    &amp;= P(\|x\|\leq r^{*} | Y=1)
\end{align}\]</span></p>
<p><strong>Part II: PDF of 2D Gaussian</strong></p>
<p><span class="math display">\[\begin{align}
p(x = \begin{bmatrix}
        x_1\\
        x_2
    \end{bmatrix}
    )%
    &amp;= \frac{1}{\sqrt{(2\pi)^2 \sigma^4}} \exp(-\frac{1}{2}x^T \Sigma^{-1}x)\\
    &amp;= \frac{1}{\sqrt{(2\pi)^2 \sigma^4}} \exp(-\frac{1}{2}\frac{x_{1}^{2}+x_{2}^{2}}{\sigma^2})\\
    &amp;= \frac{1}{2\pi \sigma^2}\exp(-\frac{x_{1}^{2} + x_{2}^{2}}{2\sigma^2})\\
\end{align}\]</span></p>
<p>Use the polar coordination, <span class="math inline">\(x_{1} = r\cos\theta\)</span> and <span class="math inline">\(x_{2} = r\sin\theta\)</span>. <span class="math inline">\(x_{1}^{2} + x_{2}^{2} = r^2\)</span>. We can transform 2D gaussian into polar coordination:</p>
<p><span class="math display">\[p(r,\theta) = \frac{1}{2\pi \sigma^2}\exp(-\frac{r^2}{2\sigma^2})\]</span></p>
<p><strong>Part III: Integration</strong></p>
<p><span class="math display">\[\begin{align}
    \epsilon^{0}[\psi^{*}]%
    &amp;= \int_{\theta=0}^{\theta=2\pi}\int_{r=r^{*}}^{\infty}   \frac{1}{2\pi \sigma_{0}^2}\exp(-\frac{r^2}{2\sigma_{0}^2})rdrd\theta\\
    &amp;= \frac{1}{2\pi \sigma_{0}^2}\int_{\theta=0}^{\theta=2\pi}\int_{r=r^{*}}^{\infty}   \exp(-\frac{r^2}{2\sigma_{0}^2})rdrd\theta\\
    &amp;= \frac{1}{2\pi\sigma_{0}^2}\int_{\theta=0}^{\theta=2\pi} \sigma_{0}^{2}\exp(-\frac{r_{*}^{2}}{2\sigma_{0}^{2}})d\theta\\
    &amp;= \exp(-\frac{r_{*}^2}{2\sigma_{0}^{2}})\\
    &amp;= \exp\left(- \frac{1}{\sigma_{0}^{2}(\sigma_{0}^{-2} - \sigma_{1}^{-2})} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &amp;= \exp\left(\frac{-1}{(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &amp;= \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
    \epsilon^{1}[\psi^*] &amp;= \int^{\theta=2\pi}_{\theta=0}\int_{r=0}^{r=r^*}  \frac{1}{2\pi \sigma_{1}^2}\exp(-\frac{r^2}{2\sigma_{1}^2})rdrd\theta\\
    &amp;= 1 - \exp(- \frac{r_{*}^{2}}{2\sigma_{1}^{2}})\\
    &amp;= 1 - \exp\left(- \frac{1}{\sigma_{1}^{2}(\sigma_{0}^{-2} - \sigma_{1}^{-2})} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &amp;= 1 - \exp\left(- \frac{1}{(\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}} - 1)} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &amp;= 1 - \exp\left(- \frac{\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}}{(1 - \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &amp;= 1 - \exp\left(- \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}(1 - \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)
\end{align}\]</span></p>
<p><strong>Part IV: Combining together</strong></p>
<p><span class="math display">\[\begin{align}
    \epsilon^{*}%
    &amp;= P(Y=0)\epsilon^{0}[\psi^{*}] + P(Y=1)\epsilon^{1}[\psi^{*}]\\
    &amp;= \frac{1}{2}\epsilon^{0} + \frac{1}{2}\epsilon^{1}\\
    &amp;= \frac{1}{2} \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right) + \frac{1}{2} - \frac{1}{2} \exp\left(- \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}(1 - \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &amp;= \frac{1}{2} \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right) + \frac{1}{2}\\
    &amp;- \frac{1}{2} \exp\left(- (\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}-1)(1 - \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)  \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &amp;= \frac{1}{2} + \frac{1}{2}\left[ 1 - \exp\left(- \underbrace{(\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}-1)(1 - \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1})}_{=-1} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right) \right] \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &amp;= \frac{1}{2} + \frac{1}{2}\left[ 1 - \exp\left(\ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right) \right] \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &amp;= \frac{1}{2} + \frac{1}{2}\left[ 1 - \frac{\sigma^{2}_{1}}{\sigma^{2}_{0}} \right] \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &amp;= \frac{1}{2} - \frac{1}{2}\left( \frac{\sigma^{2}_{1}}{\sigma^{2}_{0}} -1 \right) \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
\end{align}\]</span></p>
</section>
<section id="c-1" class="level3" data-number="7.6.3">
<h3 data-number="7.6.3" class="anchored" data-anchor-id="c-1"><span class="header-section-number">7.6.3</span> (c)</h3>
<blockquote class="blockquote">
<p>Compare the optimal classifier to the QDA classifier in <span class="citation" data-cites="braga2020fundamentals">Braga-Neto (<a href="../ref.html#ref-braga2020fundamentals" role="doc-biblioref">2020</a>, Example 4.3)</span>. Compute the error of the QDA classifier and compare to the Bayes error. (Given <span class="math inline">\(\sigma_{0}^{2}=2\)</span> and <span class="math inline">\(\sigma_{1}^{2} = 8\)</span>)<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
</blockquote>
<p><strong>Part I: Optimal Error of Example 4.3</strong></p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> berror_two(sig0, sig1):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> sig1 <span class="op">&gt;</span> sig0 </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    rat <span class="op">=</span> sig1<span class="op">/</span>sig0</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">-</span> <span class="fl">0.5</span><span class="op">*</span>(rat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>np.exp(<span class="op">-</span>((<span class="dv">1</span><span class="op">-</span>rat<span class="op">**-</span><span class="dv">1</span>)<span class="op">**-</span><span class="dv">1</span>)<span class="op">*</span>np.log(rat))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Optimal Error"</span>: [berror_two(<span class="dv">2</span>, <span class="dv">8</span>)]})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Optimal Error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.263765</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p><strong>Part II: QDA Error</strong></p>
<p>Use the result in <a href="#sec-44b">Problem 4.4 (b)</a> and let <span class="math inline">\(\hat{r}\)</span> be the boundary of the QDA in <span class="citation" data-cites="braga2020fundamentals">Braga-Neto (<a href="../ref.html#ref-braga2020fundamentals" role="doc-biblioref">2020</a>, Example 4.3)</span>:</p>
<ul>
<li><span class="math inline">\(\epsilon^0\)</span> is <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></li>
</ul>
<p><span class="math display">\[\begin{align}
    \epsilon^{0}%
    &amp;= \int_{\theta=0}^{\theta=2\pi}\int_{r=\hat{r}}^{\infty}   \frac{1}{2\pi \hat{\sigma}_{0}^2}\exp(-\frac{r^2}{2\hat{\sigma}_{0}^2})rdrd\theta\\
    &amp;= \exp(-\frac{\hat{r}^{2}}{2\hat{\sigma}^{2}_{0}})\\
    &amp;= \exp\left(-\frac{\frac{32}{9}\ln2}{2\cdot \frac{2}{3}}\right)\\
    &amp;\approx 0.157
\end{align}\]</span></p>
<ul>
<li><span class="math inline">\(\epsilon^1\)</span> is <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> <span class="math display">\[\begin{align}
  \epsilon^{1}%
  &amp;= 1 - \exp\left(-\frac{\hat{r}^2}{2\hat{\sigma}^2_{1}}\right)\\
  &amp;= 1 - \exp\left(- \frac{\frac{32}{9}\ln2}{2\cdot \frac{8}{3}}\right)\\
  &amp;\approx 0.370
\end{align}\]</span></li>
</ul>
<p>Since <span class="math inline">\(k_n =0\)</span> is assumed, the error of LDA is<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p><span class="math display">\[\begin{align}
\epsilon_{LDA}%
&amp;= \frac{1}{2}(\epsilon_{LDA}^{0} + \epsilon_{LDA}^{1})\\
&amp;= \frac{1}{2}(0.157 + 0.370)\\
&amp;= \underline{0.264}
\end{align}\]</span></p>
<p><strong>Conclusion</strong></p>
<p>The QDA error is larger than the optimal error.</p>
</section>
</section>
<section id="problem-4.8-python-assignment" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="problem-4.8-python-assignment"><span class="header-section-number">7.7</span> Problem 4.8 (Python Assignment)</h2>
<blockquote class="blockquote">
<p>Apply linear discriminant analysis to the stacking fault energy (SFE) dataset (see <span class="citation" data-cites="braga2020fundamentals">Braga-Neto (<a href="../ref.html#ref-braga2020fundamentals" role="doc-biblioref">2020, sec. A8.4</a>)</span>), already mentioned in <span class="citation" data-cites="braga2020fundamentals">Braga-Neto (<a href="../ref.html#ref-braga2020fundamentals" role="doc-biblioref">2020</a>, ch.&nbsp;1)</span>. Categorize the SFE values into two classes, low (SFE <span class="math inline">\(\leq 35\)</span>) and high (SFE <span class="math inline">\(\geq 45\)</span>), excluding the middle values.</p>
</blockquote>
<section id="a-2" class="level3" data-number="7.7.1">
<h3 data-number="7.7.1" class="anchored" data-anchor-id="a-2"><span class="header-section-number">7.7.1</span> (a)</h3>
<blockquote class="blockquote">
<p>Apply the preprocessing steps in <a href="https://drive.google.com/file/d/1n0XLsXCa8qDZxxOPh6e6f2n4OGf2pPpz/view"><code>c01_matex.py</code></a> to obtain a data matrix of dimensions <span class="math inline">\(123 (\text{number of sample points}) \times 7 (\text{number of features})\)</span>, as described in <span class="citation" data-cites="braga2020fundamentals">Braga-Neto (<a href="../ref.html#ref-braga2020fundamentals" role="doc-biblioref">2020, sec. 1.8.2</a>)</span>. Define low (SFE <span class="math inline">\(\leq 35\)</span>) and high (SFE <span class="math inline">\(\geq 45\)</span>) labels for the data. Pick the first <span class="math inline">\(50\%\)</span> of the sampe point s to be the training data and the remaining <span class="math inline">\(50\%\)</span> to be test data<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>.</p>
</blockquote>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_SFE_low(df):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    df_ <span class="op">=</span> df[df[<span class="st">"SFE"</span>]<span class="op">&lt;=</span><span class="dv">35</span>]</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df_.drop(<span class="st">"SFE"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_SFE_high(df):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    df_ <span class="op">=</span> df[df[<span class="st">"SFE"</span>]<span class="op">&gt;=</span><span class="dv">45</span>]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df_.drop(<span class="st">"SFE"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>SFE_data <span class="op">=</span> pd.read_table(<span class="st">"data/Stacking_Fault_Energy_Dataset.txt"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># pre-process the data</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>f_org <span class="op">=</span> SFE_data.columns[:<span class="op">-</span><span class="dv">1</span>]                <span class="co"># original features</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>n_org <span class="op">=</span> SFE_data.shape[<span class="dv">0</span>]                    <span class="co"># original number of training points</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>p_org <span class="op">=</span> np.<span class="bu">sum</span>(SFE_data.iloc[:,:<span class="op">-</span><span class="dv">1</span>]<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">/</span>n_org <span class="co"># fraction of nonzero components for each feature</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>f_drp <span class="op">=</span> f_org[p_org<span class="op">&lt;</span><span class="fl">0.6</span>]                         <span class="co"># features with less than 60% nonzero components</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>SFE1  <span class="op">=</span> SFE_data.drop(f_drp,axis<span class="op">=</span><span class="dv">1</span>)          <span class="co"># drop those features</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>s_min <span class="op">=</span> SFE1.<span class="bu">min</span>(axis<span class="op">=</span><span class="dv">1</span>)                     </span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>SFE2  <span class="op">=</span> SFE1[s_min<span class="op">!=</span><span class="dv">0</span>]                       <span class="co"># drop sample points with any zero values  </span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>SFE   <span class="op">=</span> SFE2[(SFE2.SFE<span class="op">&lt;</span><span class="dv">35</span>)<span class="op">|</span>(SFE2.SFE<span class="op">&gt;</span><span class="dv">45</span>)]    <span class="co"># drop sample points with middle responses</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> SFE <span class="op">&gt;</span> <span class="dv">40</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>train, test, ytrain, ytest  <span class="op">=</span> train_test_split(SFE, Y, test_size<span class="op">=</span><span class="fl">0.5</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(SFE.shape)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>SFE.head(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(123, 8)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div id="tbl-total" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;7.2:  Filtered data </caption>
  <thead>
    <tr>
      <th></th>
      <th>C</th>
      <th>N</th>
      <th>Ni</th>
      <th>Fe</th>
      <th>Mn</th>
      <th>Si</th>
      <th>Cr</th>
      <th>SFE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.004</td>
      <td>0.003</td>
      <td>15.6</td>
      <td>64.317</td>
      <td>0.03</td>
      <td>0.02</td>
      <td>17.5</td>
      <td>51.6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.020</td>
      <td>0.009</td>
      <td>15.6</td>
      <td>64.188</td>
      <td>0.03</td>
      <td>0.03</td>
      <td>17.6</td>
      <td>54.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.020</td>
      <td>0.002</td>
      <td>14.0</td>
      <td>66.409</td>
      <td>0.03</td>
      <td>0.01</td>
      <td>17.1</td>
      <td>50.3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.005</td>
      <td>0.001</td>
      <td>15.6</td>
      <td>63.866</td>
      <td>0.19</td>
      <td>0.01</td>
      <td>17.7</td>
      <td>52.8</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train.shape)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>train.head(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(61, 8)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="8">
<div id="tbl-train" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;7.3:  Train data </caption>
  <thead>
    <tr>
      <th></th>
      <th>C</th>
      <th>N</th>
      <th>Ni</th>
      <th>Fe</th>
      <th>Mn</th>
      <th>Si</th>
      <th>Cr</th>
      <th>SFE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.004</td>
      <td>0.003</td>
      <td>15.6</td>
      <td>64.317</td>
      <td>0.03</td>
      <td>0.02</td>
      <td>17.5</td>
      <td>51.6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.020</td>
      <td>0.009</td>
      <td>15.6</td>
      <td>64.188</td>
      <td>0.03</td>
      <td>0.03</td>
      <td>17.6</td>
      <td>54.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.020</td>
      <td>0.002</td>
      <td>14.0</td>
      <td>66.409</td>
      <td>0.03</td>
      <td>0.01</td>
      <td>17.1</td>
      <td>50.3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.005</td>
      <td>0.001</td>
      <td>15.6</td>
      <td>63.866</td>
      <td>0.19</td>
      <td>0.01</td>
      <td>17.7</td>
      <td>52.8</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test.shape)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>test.head(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(62, 8)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="9">
<div id="tbl-test" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;7.4:  Test data </caption>
  <thead>
    <tr>
      <th></th>
      <th>C</th>
      <th>N</th>
      <th>Ni</th>
      <th>Fe</th>
      <th>Mn</th>
      <th>Si</th>
      <th>Cr</th>
      <th>SFE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>295</th>
      <td>0.07</td>
      <td>0.40</td>
      <td>16.13</td>
      <td>54.818</td>
      <td>9.64</td>
      <td>0.45</td>
      <td>18.48</td>
      <td>65.0</td>
    </tr>
    <tr>
      <th>296</th>
      <td>0.07</td>
      <td>0.54</td>
      <td>16.13</td>
      <td>54.678</td>
      <td>9.64</td>
      <td>0.45</td>
      <td>18.48</td>
      <td>53.0</td>
    </tr>
    <tr>
      <th>297</th>
      <td>0.04</td>
      <td>0.04</td>
      <td>9.00</td>
      <td>70.920</td>
      <td>1.20</td>
      <td>0.40</td>
      <td>18.20</td>
      <td>30.4</td>
    </tr>
    <tr>
      <th>298</th>
      <td>0.04</td>
      <td>0.04</td>
      <td>9.00</td>
      <td>70.920</td>
      <td>1.20</td>
      <td>0.40</td>
      <td>18.20</td>
      <td>25.7</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="b-1" class="level3" data-number="7.7.2">
<h3 data-number="7.7.2" class="anchored" data-anchor-id="b-1"><span class="header-section-number">7.7.2</span> (b)</h3>
<blockquote class="blockquote">
<p>Using the function <code>ttest_ind</code> from the <code>scipy.stats</code> module, apply Welchs two-sample t-test on the training data, and produce a table with the predictors, <em>T</em> statistic, and <em>p</em>-value, ordered with <strong>largest absolute</strong> <em>T</em> statistics at the top.</p>
</blockquote>
<ul>
<li>For calculate independent t-test, we need to use <code>equal_var=False</code> for two different sample sizes<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>.</li>
</ul>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>df_low <span class="op">=</span> get_SFE_low(train)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>df_high <span class="op">=</span> get_SFE_high(train)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>ttest <span class="op">=</span> st.ttest_ind(df_low, df_high, equal_var<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>tdf <span class="op">=</span> pd.DataFrame({</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Features"</span>: df_low.keys(),</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"T-statistics"</span>: ttest.statistic,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"P-Values"</span>: ttest.pvalue</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>tdf <span class="op">=</span> tdf.sort_values([<span class="st">"T-statistics"</span>], ascending<span class="op">=</span>[<span class="dv">0</span>], key<span class="op">=</span><span class="bu">abs</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>tdf </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<div id="tbl-48b" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;7.5:  Results of T-test analysis </caption>
  <thead>
    <tr>
      <th></th>
      <th>Features</th>
      <th>T-statistics</th>
      <th>P-Values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>Ni</td>
      <td>-10.020338</td>
      <td>9.325882e-14</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Fe</td>
      <td>6.050509</td>
      <td>1.327194e-07</td>
    </tr>
    <tr>
      <th>0</th>
      <td>C</td>
      <td>1.912194</td>
      <td>6.242173e-02</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Si</td>
      <td>1.092405</td>
      <td>2.816877e-01</td>
    </tr>
    <tr>
      <th>1</th>
      <td>N</td>
      <td>-0.889981</td>
      <td>3.789871e-01</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Mn</td>
      <td>-0.198507</td>
      <td>8.436217e-01</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Cr</td>
      <td>0.038242</td>
      <td>9.696653e-01</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="c-2" class="level3" data-number="7.7.3">
<h3 data-number="7.7.3" class="anchored" data-anchor-id="c-2"><span class="header-section-number">7.7.3</span> (c)</h3>
<blockquote class="blockquote">
<p>Pick the top two predictors and design an LDA classifier. (This is an example of <em>filter feature selection</em>, to be discussed in Chapter 9.). Plot the training data with the superimposed LDA decision boundary. Plot the testing data with the superimposed previously-obtained LDA decision boundary. Estimate the classification error rate on the training and test data. What do you observe?</p>
</blockquote>
<p>Both train and test data has higher error rate compared to the result in <a href="#tbl-48-survey">Table&nbsp;<span>7.6</span></a>. Because the train data (<a href="#fig-48-train">Figure&nbsp;<span>7.3</span></a>, <a href="#fig-48-test">Figure&nbsp;<span>7.4</span></a>) is inseparable with single boundary. This implies that we need extra informative feature to make this two classes separatable.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> <span class="bu">list</span>(tdf[<span class="st">"Features"</span>][<span class="dv">0</span>:<span class="dv">2</span>])</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>var1, var2 <span class="op">=</span> features</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_data_with_features(data, features):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> data[features].values</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> data.SFE <span class="op">&gt;</span> <span class="dv">40</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_loss(X, Y, clf):</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> sk.metrics.zero_one_loss(Y, clf.predict(X))</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> get_data_with_features(train, features)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>X_test, Y_test <span class="op">=</span> get_data_with_features(test, features)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LDA(priors<span class="op">=</span>(<span class="fl">0.5</span>,<span class="fl">0.5</span>))</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>clf.fit(X,Y.astype(<span class="bu">int</span>))</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> clf.coef_[<span class="dv">0</span>]</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> clf.intercept_[<span class="dv">0</span>]<span class="op">;</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Error</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>err_train <span class="op">=</span> get_loss(X,Y, clf)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>err_test <span class="op">=</span> get_loss(X_test, Y_test, clf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_swe_predict(ax, X, Y, clf, title<span class="op">=</span><span class="st">""</span>):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X[<span class="op">~</span>Y,<span class="dv">0</span>],X[<span class="op">~</span>Y,<span class="dv">1</span>],c<span class="op">=</span><span class="st">'blue'</span>,s<span class="op">=</span><span class="dv">32</span>,label<span class="op">=</span><span class="st">'Low SFE'</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X[Y,<span class="dv">0</span>],X[Y,<span class="dv">1</span>],c<span class="op">=</span><span class="st">'orange'</span>,s<span class="op">=</span><span class="dv">32</span>,label<span class="op">=</span><span class="st">'High SFE'</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    left,right <span class="op">=</span> ax.get_xlim()</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    bottom,top <span class="op">=</span> ax.get_ylim()</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    ax.plot([left,right],[<span class="op">-</span>left<span class="op">*</span>a[<span class="dv">0</span>]<span class="op">/</span>a[<span class="dv">1</span>]<span class="op">-</span>b<span class="op">/</span>a[<span class="dv">1</span>],<span class="op">-</span>right<span class="op">*</span>a[<span class="dv">0</span>]<span class="op">/</span>a[<span class="dv">1</span>]<span class="op">-</span>b<span class="op">/</span>a[<span class="dv">1</span>]],<span class="st">'k'</span>,linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(left,right)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(bottom,top)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(var1)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(var2)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    ax.legend()<span class="op">;</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>fig43tr, ax43tr <span class="op">=</span> plt.subplots()</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>ax43tr <span class="op">=</span> plot_swe_predict(ax43tr, X, Y, clf, title<span class="op">=</span><span class="st">"Train error </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(err_train))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-48-train" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-48-train-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.3: Train data with LDA</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>fig43t, ax43t <span class="op">=</span> plt.subplots()</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>ax43t <span class="op">=</span> plot_swe_predict(ax43t, X_test, Y_test, clf, title<span class="op">=</span><span class="st">"Test error </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(err_test))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-48-test" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-48-test-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.4: Test data with LDA</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="d" class="level3" data-number="7.7.4">
<h3 data-number="7.7.4" class="anchored" data-anchor-id="d"><span class="header-section-number">7.7.4</span> (d)</h3>
<blockquote class="blockquote">
<p>Repeat for the top three, and five predictors. Estimate the errors on the training and testing data (there is no need to plot the classifiers). What can you observe?</p>
</blockquote>
<p>As shown in <a href="#tbl-48-survey">Table&nbsp;<span>7.6</span></a>, the error rate does not decrease as more features get involved. This is because the sign of t-statistics are mixing together, and deviate the boundary to the optimal solution (see <a href="#tbl-48b">Table&nbsp;<span>7.5</span></a>). The LDA approach may not appropriate for features with inversing T-statistics.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>n_features <span class="op">=</span> [<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>err_trains <span class="op">=</span> np.zeros(<span class="bu">len</span>(n_features))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>err_tests <span class="op">=</span> np.zeros(<span class="bu">len</span>(n_features))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j,i) <span class="kw">in</span> <span class="bu">enumerate</span>(n_features):</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    fs <span class="op">=</span> <span class="bu">list</span>(tdf[<span class="st">"Features"</span>][<span class="dv">0</span>:i])</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> get_data_with_features(train, fs)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    X_test, Y_test <span class="op">=</span> get_data_with_features(test, fs)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> LDA(priors<span class="op">=</span>(<span class="fl">0.5</span>,<span class="fl">0.5</span>))</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    clf.fit(X,Y.astype(<span class="bu">int</span>))</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Error</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    err_trains[j] <span class="op">=</span> get_loss(X,Y, clf)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    err_tests[j] <span class="op">=</span> get_loss(X_test, Y_test, clf)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Number of Features"</span>: n_features,</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Training Error Rate"</span>: err_trains,</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Testing Error Rate"</span>: err_tests</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<div id="tbl-48-survey" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;7.6:  Surveying different number of features </caption>
  <thead>
    <tr>
      <th></th>
      <th>Number of Features</th>
      <th>Training Error Rate</th>
      <th>Testing Error Rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>0.114754</td>
      <td>0.145161</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>0.081967</td>
      <td>0.145161</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>0.081967</td>
      <td>0.225806</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5</td>
      <td>0.081967</td>
      <td>0.193548</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-ardianumam2017" class="csl-entry" role="doc-biblioentry">
ardianumam. 2017. <span>Understanding <span>Multivariate Gaussian</span>, <span>Gaussian Properties</span> and <span>Gaussian Mixture Model</span>.</span> <em>Ardian Umam Blog</em>.
</div>
<div id="ref-braga2020fundamentals" class="csl-entry" role="doc-biblioentry">
Braga-Neto, Ulisses. 2020. <em>Fundamentals of Pattern Recognition and Machine Learning</em>. Springer.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><blockquote class="blockquote">
<p>All, in Example 3.4 there is a negative sign missing. The value of bn is -(m1-m0)^T(m1+m0)/2.  <a href="https://ecen649patter-mrc1007.slack.com/archives/C03V4NZAEHJ/p1665447468891869">Ulisses on Slack</a></p>
</blockquote>
<a href="#fnref1" class="footnote-back" role="doc-backlink"></a></li>
<li id="fn2"><blockquote class="blockquote">
<p>Also, in Problem 4.2, the number of classifiers is K(K-1)/2 not K(K-1)  <a href="https://ecen649patter-mrc1007.slack.com/archives/C03V4NZAEHJ/p1665266576837119">Ulisses on TAMU Slack</a></p>
</blockquote>
<a href="#fnref2" class="footnote-back" role="doc-backlink"></a></li>
<li id="fn3"><p>Any affine transformation <span class="math inline">\(f(x)=AX+B\)</span> of a Gaussian is a Guassian. That is, <span class="math inline">\(A\)</span> is a nongingular matrix, and B is a vector. If <span class="math inline">\(X\sim N(\mu, \Sigma)\)</span>, <span class="math inline">\(a^T X+b \sim N(A^T \mu + b, A^T \Sigma A)\)</span>. <span class="citation" data-cites="ardianumam2017">(<a href="../ref.html#ref-ardianumam2017" role="doc-biblioref">ardianumam 2017</a>)</span><a href="#fnref3" class="footnote-back" role="doc-backlink"></a></p></li>
<li id="fn4"><p><span class="math display">\[\begin{equation}\begin{bmatrix}
a &amp; b\\
c &amp; d
\end{bmatrix}^{-1} = \frac{1}{ad-bc}\begin{bmatrix}
d &amp; -b\\
-c &amp; a
\end{bmatrix}
\end{equation}\]</span><a href="#fnref4" class="footnote-back" role="doc-backlink"></a></p></li>
<li id="fn5"><blockquote class="blockquote">
<p>For Problem 4.3(c), please assume sigma_0^2 = 2 and sigma_1^2 = 8.  <a href="https://ecen649patter-mrc1007.slack.com/archives/C03V4NZAEHJ/p1665440901330769">Ulisses (TAMU Slack)</a></p>
</blockquote>
<a href="#fnref5" class="footnote-back" role="doc-backlink"></a></li>
<li id="fn6"><p>Via <a href="https://www.wolframalpha.com/input?i=exp%28-+%2832%2F9*log%282%29%29%2F%282*2%2F3%29%29">WolframAlpha</a><a href="#fnref6" class="footnote-back" role="doc-backlink"></a></p></li>
<li id="fn7"><p>Via <a href="https://www.wolframalpha.com/input?i=1+-+exp%28-+%2832%2F9*log%282%29%29%2F%282*8%2F3%29%29">WolframAlpha</a><a href="#fnref7" class="footnote-back" role="doc-backlink"></a></p></li>
<li id="fn8"><p>Via <a href="https://www.wolframalpha.com/input?i=0.5*%280.157+%2B+0.370%29">WolframAlpha</a><a href="#fnref8" class="footnote-back" role="doc-backlink"></a></p></li>
<li id="fn9"><blockquote class="blockquote">
<p>All, for the last problem, please make sure you are dividing the data 50% - 50% for training and testing, the values in the book are incorrect.  <a href="https://ecen649patter-mrc1007.slack.com/archives/C03V4NZAEHJ/p1665417767039559">Ulisses (TAMU Slack)</a></p>
</blockquote>
<a href="#fnref9" class="footnote-back" role="doc-backlink"></a></li>
<li id="fn10"><p>https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html#r3566833beaa2-1<a href="#fnref10" class="footnote-back" role="doc-backlink"></a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../hw/hw1.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Homework 1</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../hw/hw3.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Homework 3</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>