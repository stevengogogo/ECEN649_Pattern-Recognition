---
title: Homework 2
author: Shao-Ting Chiu (UIN:433002162)
date: today
bibliography: ../ref.bib  
format:
    pdf:
        code-line-numbers: true
        table-of-contents: true 
    html: 
        table-of-contents: true
jupyter: python3  
execute: 
    echo: true
    freeze: auto 
---

## Homework Description

- Course: ECEN649, Fall2022

> Problems from the book:
>
> 3.6 (10 pt)
>
> 4.2 (10 pt)
>
> 4.3 (10 pt)
>
> 4.4 (10 pt)
>
> 4.8 (20 pt)

- Deadline: `Oct. 12th, 11:59 am`


## Computational Enviromnent Setup

### Third-party libraries
``` {python}
%matplotlib inline
import sys # system information
import matplotlib # plotting
import scipy.stats as st # scientific computing
import pandas as pd # data managing
import numpy as np # numerical comuptation
from numpy import linalg as LA
import scipy as sp
import scipy.optimize as opt
import sympy as sp
import matplotlib.pyplot as plt
from numpy.linalg import inv, det
from numpy.random import multivariate_normal as mvn
from numpy.random import binomial as binom
# Matplotlib setting
plt.rcParams['text.usetex'] = True
matplotlib.rcParams['figure.dpi']= 300
```

### Version
``` {python}
print(sys.version)
print(matplotlib.__version__)
print(sp.__version__)
print(np.__version__)
print(pd.__version__)
```

---

## Problem 3.6 (Python Assignment)

> Using the synthetic data model in Section A8.1 for the homoskedastic case with $\mu_0 = (0,\dots,0)$, $\mu_1=(1,\dots,1)$, $P(Y=0)=P(Y=1)$, and $k=d$ (independent features), generate a large number (e.g., $M=1000$) of training data sets for each sample size $n=20$ to $n=100$, in steps of $10$, with $d=2,5,8$, and $\sigma=1$. Obtain an approximation of the expected classification error $E[\epsilon_n]$ of the nearest centroid classifier in each case by averaging $\epsilon_n$, computed using the exact formula (3.13), over the $M$ synthetic training data sets. Plot $E[\epsilon_n]$ as a function of the sample size, for $d=2,5,8$ (join the individual points with lines to obtain a smooth curve). Explain what you see.

- The formula in @braga2020fundamentals [pp. 56, Eq. 3.13]
    - $\epsilon_n = \frac{1}{2}\left(\Phi\left(\frac{a_{n}^{T}\hat{\mu}_0 +  b_n}{\|a_n\|}\right)  + \Phi\left(-\frac{a_{n}^{T}\hat{\mu}_1 + b_n}{\|a_n\|}\right) \right)$
        - $\mu_0 = (0,\dots, 0)$
            - $\hat{\mu}_0 = \frac{1}{N_0}\sum^{n}_{i=1}X_i I_{Y_i=0}$
        - $\mu_1 = (1,\dots,1)$
            - - $\hat{\mu}_1 = \frac{1}{N_1}\sum^{n}_{i=1}X_i I_{Y_i=1}$
        - $a_n = \hat{\mu}_1 - \hat{\mu}_0$
        - $b_n = \frac{(\hat{\mu}_1 - \hat{\mu}_0)(\hat{\mu}_1 + \hat{\mu}_0)}{2}$

``` {python}

def hat_mu(m):
    return np.mean(m, axis=0)

def get_an(hm0,hm1):
    return hm1 - hm0

def get_bn(hm0,hm1):
    return (hm1 - hm0)*(hm1+hm0).T/2

def epsilon(hmu0, hmu1, p0=0.5):
    p1 = 1-p0
    an = get_an(hmu0, hmu1)
    bn = get_bn(hmu0, hmu1)
    epsilon0 = st.norm.cdf((an*hmu0.T + bn)/LA.norm(an))
    epsilon1 = st.norm.cdf(-(an*hmu1.T+ bn)/LA.norm(an))
    return (p0*epsilon0 + p1*epsilon1)[0][0]

class GaussianDataGen:
    def __init__(self, n, d, s=1, mu=0):
        self.n = n
        self.d = d
        self.mu = np.ones(d) * mu
        self.s = s
        self.cov = self.get_cov()

    def get_cov(self):
        return np.identity(self.d) * self.s
    
    def sample(self):
        hmuV = np.zeros(self.d)
        for i in range(0,self.d):
            hmuV[i] = np.mean(np.random.normal(self.mu[0], self.s, self.n))
        return np.matrix(hmuV)

def cal_eps(dg0, dg1, p0=0.5):
    hmuV0 = dg0.sample()
    hmuV1 = dg1.sample()
    return epsilon(hmuV0, hmuV1, p0=0.5)
cal_eps_func = np.vectorize(cal_eps)

def exp_try_nd(n, d, s=1,M=1000):
    gX0 = GaussianDataGen(n=n, d=d, s= s,mu=0)
    gX1 = GaussianDataGen(n=n, d=d, s= s, mu=1)
    eps = cal_eps_func([gX0 for i in range(0,M)], gX1)
    return np.mean(eps)
exp_try_nd_func = np.vectorize(exp_try_nd)

M = 1000
ns = np.arange(20,80, 10)
s = 1
dres = {2:[],5:[],8:[]}

"""
for k in dres.keys():
    dres[k] = exp_try_nd_func(ns,k,M)


fig, ax = plt.subplots()
for k in dres.keys():
    ax.plot(ns, dres[k], 'o',label="d={}".format(k))
ax.set_xlabel("n")
ax.set_ylabel("$E[\\epsilon_n]$")
ax.legend();
"""
```


## Problem 4.2

> A common method to extend binary classification rules to $K$ classes, $K>2$, is the *one-vs-one approach*, in which $K(K-1)$ classifiers are trained between all pairs of classes, and a majority vote of assigned labels is taken.

### (a) {#sec-42a}

> Formulate a multiclass version of parametric plug-in classification using the one-vs-one approach.

Let $\psi^{*}_{i,j}$ be a one-one classifiers that $i\neq j$, and $\{(i,j)| i\in [1,k], j \in [1,k], i\neq j\}$. For $K$ classes, there are $K(K-1)$ classifiers; for each classifier $\psi^{*}_{i,j}$ and $x\in R^d$,

\begin{equation}
    \psi^{*}_{ij,n} = 
    \begin{cases}
        1, & D_{ij, n}(x) > k_{ij,n}\\ 
        0, & \text{otherwise}
    \end{cases}
\end{equation}

where

- $D_{ij,n}(x) = \ln \frac{p(x|\theta_{i,n})}{p(x|\theta_{j,n})}$
- $k_{ij,n} = \ln\frac{P(Y=j)}{P(Y=i)}$
- Noted that feature-label distribution is expressed via a familty of PDF $\{p(x|\theta_i) | \theta \in \Theta \subseteq R^m\}$, for $i=1,\dots,K$.

Let $\psi^{*}_{i,n} = \sum_{j\neq i} I_{\psi^{*}_{ij,n}=1}$, and the one-vs-one classifier is 

$$\psi^{*}_{n}(x) = \arg\max_{k=1,\dots,K} \psi^{*}_{k,n}$$

### (b) {#sec-42b}

> Show that if the threshold $k_{ij,n}$ between classes $i$ and $j$ is given by $\frac{\ln\hat{c}_j}{\ln\hat{c}_i}$, then the one-vs-one parametric classification rule is equivalent to the simple decision.
> $$\psi_{n}(x) = \arg\max_{k=1,...,K} \hat{c}_{k} p(x|\theta_{k,n}), x\in R^d$$
> (For simplicity, you may ignore the possibility of ties.)



### (c)

> Applying the approach in items [(a)](#sec-42a) and [(b)](#sec-42b), formulate a multiclass version of Gaussian discriminant analysis. In the case of multiclass NMC, with all thresholds equal to zero, how does the decision boundary look like?


## Problem 4.3

> Under the general Gaussian model $p(x|Y=0)\sim \mathcal{N}_d(\mu_0, \sum_0)$ and $p(x|Y=1)\sim \mathcal{N}_d(\mu_1, \sum_1)$, the classification error $\epsilon_n = P(\psi_n(X)\neq Y| S_n)$ of *any* linear classifier in the form
>
> \begin{equation}
>    \psi_{n}(x) = 
>    \begin{cases}
>        1,& a_{n}^{T}x + b_n > 0,\\ 
>        0,& \text{otherwise}
>    \end{cases}
> \end{equation}
>
> (examples discussed so far include LDA and its variants, and the logistic classifier) can be readily computed in terms of $\Phi$ (the CDF of a standard normal random variable), the classifier parameters $a_n$ and $b_n$, and the distributional parameters $c=P(Y=1)$, $\mu_0$, $\mu_1$, $\Sigma_0$, and $\Sigma_1$.


### (a)

> Show that 
> 
> $$\epsilon_n = (1-c)\Phi\left( \frac{a_{n}^{T}\mu_0 + b_n}{\sqrt{a_{n}^{T}\Sigma_0 a_n}} \right) + c \Phi\left( -\frac{a^{T}_{n}\mu_1 + b_n}{\sqrt{a_{n}^{T}\Sigma_1 a_n}}\right)$$
>
> Hint: the discriminant $a^{T}_{n}x+b_n$ has a simple Gaussian distribution in each class.

### (b)

> Compute the errors of the NMC, LDA, and DLDA classifiers in Example 4.2 if $c=1/2$, 
> \begin{equation*}
    \mu_0 =
    \begin{bmatrix}
        2\\ 
        3
    \end{bmatrix},
    \mu1 =
    \begin{bmatrix}
        6\\ 
        5
    \end{bmatrix},
    \Sigma_0 = 
    \begin{bmatrix}
        1 & 1\\ 
        1 & 2
    \end{bmatrix},
    \text{ and } 
    \Sigma_1 = 
    \begin{bmatrix}
        4 & 0\\
        0 & 1
    \end{bmatrix}
> \end{equation*}
> Which classifier does the best?



## Problem 4.4

> Even in the Gaussian case, the classification error of quadratic classifiers in general require numerical integration for its computation. In some special simple cases, however, it is possible to obtain exact solutions. Assume a two-dimensional Gaussian problem with $P(Y=1)=\frac{1}{2}$, $\mu_0=\mu_1 = 0$, $\Sigma_0=\sigma_{0}^{2}I_2$, and $\Sigma_1 = \sigma^{2}_{1}I_2$. For definiteness, assume that $\sigma_0 < \sigma_1$.

### (a)

> Show that the Bayes classifier is given by
> \begin{equation}
    \psi^{*}(x) = 
    \begin{cases}
        1, &\|x\| > r^{*},\\
        0, &\text{ otherwise },
    \end{cases}
    \quad \text{ where } r^{*} = \sqrt{2\left(\frac{1}{\sigma_{0}^{2}} - \frac{1}{\sigma_{1}^{2}}\right)^{-1}\ln\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}}}
> \end{equation}
> In particular, the optimal decision boundary is a circle of radius $r^{*}$.

The inverted $\Sigma_1$ and $\Sigma_2$ are[^invert]

\begin{align}
    \Sigma_0 &= \sigma_{0}^2 I_2 = \begin{bmatrix}
        \sigma_{0}^2 & 0 \\
        0 & \sigma_{0}^2
    \end{bmatrix}\\ 
    \Sigma_{0}^{-1} &= \frac{1}{\sigma_{0}^{4}} \begin{bmatrix}
        \sigma_{0}^2 & 0 \\
        0 & \sigma_{0}^2
    \end{bmatrix} = \sigma_{0}^{-2}\begin{bmatrix}
            1 & 0\\
            0 & 1
        \end{bmatrix} = \sigma^{-2}_{0}I_2\\
    \Sigma^{-1}_{1} &= \sigma^{-2}_{1}I_2
\end{align}

Use the derivation in @braga2020fundamentals [pp. 74],

\begin{equation}
    A_n = \begin{bmatrix}
        a_{11} & a_{12}\\ 
        a_{12} & a_{22}
    \end{bmatrix} = \frac{-1}{2} \Sigma_{1}^{-1} - \Sigma_{0}^{-1} = \frac{-1}{2}(\sigma_{1}^{-2} - \sigma_{0}^{-2}) \begin{bmatrix}
        1 & 0\\ 
        0 & 1
    \end{bmatrix}
\end{equation}

\begin{align}
    b_n &= \begin{bmatrix}
        b_{n,1}\\ 
        b_{n,2}
    \end{bmatrix}
    = \Sigma_{1}^{-1}\underbrace{\mu_1}_{=0} - \Sigma_{0}^{-1}\underbrace{\mu_{0}}_{=0}\\
        &= \begin{bmatrix}
            0\\ 
            0
        \end{bmatrix}
\end{align}

$$c = -\frac{1}{2}\ln\frac{|\Sigma_1|}{|\Sigma_0|} = \frac{-1}{2}\ln\frac{\sigma_{1}^{4}}{\sigma_{0}^{4}} = -\ln \frac{\sigma_{1}^2}{\sigma_{0}^2}$$

According to @braga2020fundamentals [Eq. 4.26], the 2-dimensional QDA decision boundary is 


\begin{align}
    D(x) = a_{11}x^{2}_1 + 2 a_{12}x_1x_2 + a_{22}x^{2}_{2} + b_1 x_1 + b_2 x_2 + c &= 0\\
    a_{11}(x_{1}^{2} + x_{2}^{2}) &= \ln \frac{\sigma_{1}^2}{\sigma_{0}^2}\\
    x^{2}_{1} + x^{2}_{2} &= 2(\frac{1}{\sigma^{2}_{0}} - \frac{1}{\sigma^{2}_{1}})^{-1}\ln\frac{\sigma_{1}^2}{\sigma_{0}^2}\\
    r^{*} = \sqrt{x^{2}_{1} + x^{2}_{2}} &= \sqrt{2(\frac{1}{\sigma^{2}_{0}} - \frac{1}{\sigma^{2}_{1}})^{-1}\ln\frac{\sigma_{1}^2}{\sigma_{0}^2}}

\end{align}

Noted that $\left(\frac{1}{\sigma^{2}_{0}} - \frac{1}{\sigma^{2}_{1}}\right) >  0$ because $\sigma_0 < \sigma_1$

For any point $\|x_j\| > r^{*}$, the discriminant ($D$) is larger than $0$, and $\psi^{*}(x_j) = 1$. 

[^invert]: \begin{equation}\begin{bmatrix}
    a & b\\ 
    c & d
\end{bmatrix}^{-1} = \frac{1}{ad-bc}\begin{bmatrix}
    d & -b\\ 
    -c & a
\end{bmatrix}
\end{equation}

### (b)

> Show that the corresponding Bayes error is given by
> $$\epsilon^{*} = \frac{1}{2} - \frac{1}{2}(\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}} - 1)e^{-(1-\frac{\sigma^{2}_{0}}{\sigma^{2}_{1}})^{-1}\ln \frac{\sigma^{2}_{1}}{\sigma^{2}_{0}}}$$
> In particular, the Bayes error is a function only of the ratio of variances $\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}}$, and $\epsilon^{*}\rightarrow 0$ as $\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}} \rightarrow \infty$.
>
> Hint: use polar coordinates to solve the required integrals analytically.

\begin{align}
    \epsilon^{0}[\psi^{*}] 
    &= P(D^{*}(X)>k^{*}|Y=0)\\ 
    &= P(\|x\|>r^{*} | Y=0)

\end{align}



\begin{align}
    \epsilon^{0}[\psi^{*}] 
    &= P(D^{*}(X)\leq k^{*}|Y=1)\\ 
    &= P(\|x\|\leq r^{*} | Y=1)

\end{align}


::: {.callout-note}
- WIP
-  https://ardianumam.wordpress.com/2017/10/19/deriving-gaussian-distribution/
- Integrate the area outside the circle with (Y=0) and Integrate the area inside the circle with (Y=1)
:::

### (c)

> Compare the optimal classifier to the QDA classifier in Example 4.3. Compute the error of the QDA classifier and compare to the Bayes error.


## Problem 4.8 (Python Assignment)

> Apply linear discriminant analysis to the stacking fault energy (SFE) dataset (see @braga2020fundamentals [sec. A8.4]), already mentioned in @braga2020fundamentals [ch. 1]. Categorize the SFE values into two classes, low (SFE $\leq 35$) and high (SFE $\geq 45$), excluding the middle values.


### (a)

> Apply the preprocessing steps in `c01_matex.py` to obtain a data matrix of dimensions $123 (\text{number of sample points}) \times 7 (\text{number of features})$, as described in @braga2020fundamentals [sec. 1.8.2]. Define low (SFE $\leq 35$) and high (SFE $\geq 45$) labels for the data. Pick the first $20\%$ of the sampe point s to be the training data and the remaining $80\%$ to be test data.

### (b)

> Using the function `ttest_ind` from the `scipy.stats` module, apply Welch's two-sample t-test on the training data, and produce a table with the predictors, *T* statistic, and *p*-value, ordered with largest absolute *T* statistics at the top.


### (c)

> Pick the top two predictors and design an LDA classifier. (This is an example of *filter feature selection*, to be discussed in Chapter 9.). Plot the training data with the superimposed LDA decision boundary. Plot the testing data with the superimposed previously-obtained LDA decision boundary. Estimate the classification error rate on the training and test data. What do you observe?

### (d)

> Repeat for the top three, and five predictors. Estimate the errors on the training and testing data (there is no need to plot the classifiers). What can you observe?


::: {.content-hidden when-format="html"}

## References

:::


## Appendix

### Question about Probelm 4.2b

```
[Question][Problem 4.2(b)]
The treshold `k_{ij,n}` is given by `\frac{\ln \hat{c}_j}{\hat{c}_i}`. However, this setting is different from the text book (P. 68), that is `k=\ln \frac{c_0}{c_1}`. 

Is this a typo or intenionally assigned?

```
|Textbook p.68|Problem 4.2 (b)|
|---|---|
|$k^{*}=\ln\frac{c_0}{c_1}$|$k_{ij,n}=\frac{\ln\hat{c_j}}{\ln \hat{c}_i}$|
