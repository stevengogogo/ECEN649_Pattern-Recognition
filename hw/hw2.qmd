---
title: Homework 2
author: Shao-Ting Chiu (UIN:433002162)
date: today
bibliography: ../ref.bib  
format:
    pdf:
        pdf-engine: tectonic
        code-line-numbers: true
        table-of-contents: true 
        
    html: 
        table-of-contents: true
jupyter: python3  
execute: 
    echo: true
    freeze: auto 
---

## Homework Description

- Course: ECEN649, Fall2022

> Problems from the book:
>
> 3.6 (10 pt)
>
> 4.2 (10 pt)
>
> 4.3 (10 pt)
>
> 4.4 (10 pt)
>
> 4.8 (20 pt)

- Deadline: `Oct. 12th, 11:59 am`


## Computational Enviromnent Setup

### Third-party libraries
``` {python}
%matplotlib inline
import sys # system information
import matplotlib # plotting
import scipy.stats as st # scientific computing
import pandas as pd # data managing
import numpy as np # numerical comuptation
import numba
import sklearn as sk
from numpy import linalg as LA
import scipy as sp
import scipy.optimize as opt
import sympy as sp
import matplotlib.pyplot as plt
from numpy.linalg import inv, det
from numpy.random import multivariate_normal as mvn
from numpy.random import binomial as binom
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA #problem 4.8
from sklearn.model_selection import train_test_split
# Matplotlib setting
plt.rcParams['text.usetex'] = True
matplotlib.rcParams['figure.dpi']= 300
```

### Version
``` {python}
print(sys.version)
print(matplotlib.__version__)
print(sp.__version__)
print(np.__version__)
print(pd.__version__)
print(sk.__version__)
```

---

## Problem 3.6 (Python Assignment)

> Using the synthetic data model in Section A8.1 for the homoskedastic case with $\mu_0 = (0,\dots,0)$, $\mu_1=(1,\dots,1)$, $P(Y=0)=P(Y=1)$, and $k=d$ (independent features), generate a large number (e.g., $M=1000$) of training data sets for each sample size $n=20$ to $n=100$, in steps of $10$, with $d=2,5,8$, and $\sigma=1$. Obtain an approximation of the expected classification error $E[\epsilon_n]$ of the nearest centroid classifier in each case by averaging $\epsilon_n$, computed using the exact formula (3.13), over the $M$ synthetic training data sets. Plot $E[\epsilon_n]$ as a function of the sample size, for $d=2,5,8$ (join the individual points with lines to obtain a smooth curve). Explain what you see.

- The formula in @braga2020fundamentals [pp. 56, Eq. 3.13]
    - $\epsilon_n = \frac{1}{2}\left(\Phi\left(\frac{a_{n}^{T}\hat{\mu}_0 +  b_n}{\|a_n\|}\right)  + \Phi\left(-\frac{a_{n}^{T}\hat{\mu}_1 + b_n}{\|a_n\|}\right) \right)$
        - $\mu_0 = (0,\dots, 0)$;
            $\hat{\mu}_0 = \frac{1}{N_0}\sum^{n}_{i=1}X_i I_{Y_i=0}$
        - $\mu_1 = (1,\dots,1)$;
            $\hat{\mu}_1 = \frac{1}{N_1}\sum^{n}_{i=1}X_i I_{Y_i=1}$
        - $a_n = \hat{\mu}_1 - \hat{\mu}_0$
        - $b_n = \frac{(\hat{\mu}_1 - \hat{\mu}_0)(\hat{\mu}_1 + \hat{\mu}_0)}{2}$

``` {python}

def hat_mu(m):
    return np.mean(m, axis=0)

def get_an(hm0,hm1):
    return hm1 - hm0

def get_bn(hm0,hm1):
    return (hm1 - hm0)*(hm1+hm0).T/2

def epsilon(hmu0, hmu1, p0=0.5):
    p1 = 1-p0
    an = get_an(hmu0, hmu1)
    bn = get_bn(hmu0, hmu1)
    epsilon0 = st.norm.cdf((an*hmu0.T + bn)/LA.norm(an))
    epsilon1 = st.norm.cdf(-(an*hmu1.T+ bn)/LA.norm(an))
    return (p0*epsilon0 + p1*epsilon1)[0][0]

class GaussianDataGen:
    def __init__(self, n, d, s=1, mu=0):
        self.n = n
        self.d = d
        self.mu = np.ones(d) * mu
        self.s = s
        self.cov = self.get_cov()

    def get_cov(self):
        return np.identity(self.d) * self.s
    
    def sample(self):
        hmuV = np.zeros(self.d)
        for i in range(0,self.d):
            hmuV[i] = np.mean(np.random.normal(self.mu[0], self.s, self.n))
        return np.matrix(hmuV)

def cal_eps(dg0, dg1, p0=0.5):
    hmuV0 = dg0.sample()
    hmuV1 = dg1.sample()
    return epsilon(hmuV0, hmuV1, p0=0.5)
cal_eps_func = np.vectorize(cal_eps)

def exp_try_nd(n, d, s=1,M=1000):
    gX0 = GaussianDataGen(n=n, d=d, s= s,mu=0)
    gX1 = GaussianDataGen(n=n, d=d, s= s, mu=1)
    eps = cal_eps_func([gX0 for i in range(0,M)], gX1)
    return np.mean(eps)
exp_try_nd_func = np.vectorize(exp_try_nd)

"""
M = 1000
ns = np.arange(20,80, 10)
s = 1
dres = {2:[],5:[],8:[]}


for k in dres.keys():
    dres[k] = exp_try_nd_func(ns,k,M)


fig, ax = plt.subplots()
for k in dres.keys():
    ax.plot(ns, dres[k], 'o',label="d={}".format(k))
ax.set_xlabel("n")
ax.set_ylabel("$E[\\epsilon_n]$")
ax.legend();
"""
```


## Problem 4.2

> A common method to extend binary classification rules to $K$ classes, $K>2$, is the *one-vs-one approach*, in which $K(K-1)$ classifiers are trained between all pairs of classes, and a majority vote of assigned labels is taken.

### (a) {#sec-42a}

> Formulate a multiclass version of parametric plug-in classification using the one-vs-one approach.

Let $\psi^{*}_{i,j}$ be a one-one classifiers that $i\neq j$, and $\{(i,j)| i\in [1,k], j \in [1,k], i\neq j\}$. For $K$ classes, there are $K(K-1)$ classifiers; for each classifier $\psi^{*}_{i,j}$ and $x\in R^d$,

\begin{equation}
    \psi^{*}_{ij,n} = 
    \begin{cases}
        1, & D_{ij, n}(x) > k_{ij,n}\\ 
        0, & \text{otherwise}
    \end{cases}
\end{equation}

where

- $D_{ij,n}(x) = \ln \frac{p(x|\theta_{i,n})}{p(x|\theta_{j,n})}$
- $k_{ij,n} = \ln\frac{P(Y=j)}{P(Y=i)}$
- Noted that feature-label distribution is expressed via a familty of PDF $\{p(x|\theta_i) | \theta \in \Theta \subseteq R^m\}$, for $i=1,\dots,K$.

Let $\psi^{*}_{i,n} = \prod_{j\neq i} I_{\psi^{*}_{ij,n}}$, and the one-vs-one classifier is 

$$\psi^{*}_{n}(x) = \arg\max_{k=1,\dots,K} \psi^{*}_{k,n}$$ {#eq-psi-max}

### (b) {#sec-42b}

> Show that if the threshold $k_{ij,n}$ between classes $i$ and $j$ is given by $\ln\frac{\hat{c}_j}{\hat{c}_i}$, then the one-vs-one parametric classification rule is equivalent to the simple decision.
> $$\psi_{n}(x) = \arg\max_{k=1,...,K} \hat{c}_{k} p(x|\theta_{k,n}), x\in R^d$$
> (For simplicity, you may ignore the possibility of ties.)

\begin{align}
    \ln \frac{ p(x|\theta_{i,n})}{p(x|\theta_{j,n})} &> k_{ij,n} = \ln\frac{\hat{c_j}}{\hat{c_i}}\\ 
    \ln p(x|\theta_{i,n}) - \ln p(x|\theta_{j,n}) &> \ln \hat{c}_j - \ln \hat{c}_{i}\\ 
    \hat{c}_i p(x|\theta_{i,n}) &> \hat{c}_j p(x|\theta_{j,n})
\end{align}

\begin{align}
    \psi^{*}_{ij, n} &= \begin{cases}
        1, & \hat{c}_i p(x|\theta_{i,n}) > \hat{c}_j p(x|\theta_{j,n})\\ 
        0, & otherwise
    \end{cases}\\ 
\end{align}



Continue @eq-psi-max. For given $i\in [1,k]$, let $j_a$, $a\in [1,k-1]$ be the order sequence of $1,\dots, k$ exclude $i$. 

$\psi^{*}_{i,n} = 1$ iff, $\hat{c}_{i}p(x|\theta_{i,n}) > \hat{c}_{j_{1}}p(x|\theta_{j_{1},n}) \geq \hat{c}_{j_{2}}p(x|\theta_{j_{2},n}) \geq \hat{c}_{j_{k-1}}p(x|\theta_{j_{k-1},n})$

Therefore, 

\begin{align}
    \psi^{*}_{1,n} &= \begin{cases}
        1, & \max_{k=1,\dots,K} \hat{c}_k p(x|\theta_{k,n}) = \hat{c}_1 p(x|\theta_{1}, n)\\ 
        0, & otherwise
    \end{cases}\\
    &\vdots\\
    \psi^{*}_{K,n} &= \begin{cases}
        1, & \max_{k=1,\dots,K} \hat{c}_K p(x|\theta_{k,n}) = \hat{c}_K p(x|\theta_{K}, n)\\ 
        0, & otherwise
    \end{cases}\\
\end{align}

We can conclude that 

\begin{align}
    \psi^{*}_{n}(x)%
    &= \arg\max_{k=1,\dots,K} \psi^{*}_{k,n}\\ 
    &= \arg\max_{k=1,\dots,K} \hat{c}_k p(x|\theta_{k}, n)
\end{align}

### (c)

> Applying the approach in items [(a)](#sec-42a) and [(b)](#sec-42b), formulate a multiclass version of Gaussian discriminant analysis. In the case of multiclass NMC, with all thresholds equal to zero, how does the decision boundary look like?

::: {.callout-tip}
- Think about 3-classes, 2-dimension
- Write the $D$ of each classifier

$$D^{*}(x) = \frac{1}{2}(x-\mu_0)^{T}\Sigma^{-1}_{0}(x-\mu_0)-\frac{1}{2}(x-\mu_1)^{T}\Sigma^{-1}_{1}(x-\mu_1) + \frac{1}{2}\ln\frac{\det(\Sigma_0)}{\det(\Sigma_1)}$$
- replace $\mu_{0} \rightarrow \hat{\mu}_{0}$; $\Sigma_{0} \rightarrow \hat{\Sigma}_{0}$
:::

## Problem 4.3

> Under the general Gaussian model $p(x|Y=0)\sim \mathcal{N}_d(\mu_0, \sum_0)$ and $p(x|Y=1)\sim \mathcal{N}_d(\mu_1, \sum_1)$, the classification error $\epsilon_n = P(\psi_n(X)\neq Y| S_n)$ of *any* linear classifier in the form
>
> \begin{equation}
>    \psi_{n}(x) = 
>    \begin{cases}
>        1,& a_{n}^{T}x + b_n > 0,\\ 
>        0,& \text{otherwise}
>    \end{cases}
> \end{equation}
>
> (examples discussed so far include LDA and its variants, and the logistic classifier) can be readily computed in terms of $\Phi$ (the CDF of a standard normal random variable), the classifier parameters $a_n$ and $b_n$, and the distributional parameters $c=P(Y=1)$, $\mu_0$, $\mu_1$, $\Sigma_0$, and $\Sigma_1$.


### (a)

> Show that 
> 
> $$\epsilon_n = (1-c)\Phi\left( \frac{a_{n}^{T}\mu_0 + b_n}{\sqrt{a_{n}^{T}\Sigma_0 a_n}} \right) + c \Phi\left( -\frac{a^{T}_{n}\mu_1 + b_n}{\sqrt{a_{n}^{T}\Sigma_1 a_n}}\right)$$
>
> Hint: the discriminant $a^{T}_{n}x+b_n$ has a simple Gaussian distribution in each class.

From @braga2020fundamentals [Eq. 2.34],

$$\epsilon^{*} = \underbrace{P(Y=0)}_{=1-c}\epsilon^{0}[\psi^*] + \underbrace{P(Y=1)}_{=c}\epsilon^{1}[\psi^*]$$

\begin{align}
\epsilon^{0}[\psi^{*}]%
&= P(a_{n}^{T}x+b_n > 0 | Y=0)\\
\end{align}

Use the affine property of Gaussian distribution described in @braga2020fundamentals [pp. 307. G4][^g4]. 


- $a^{T}_{n}x+b_n | Y=0 \sim N(a^{T}_{n}\mu_0+b_n, \underbrace{a^{T}_{n}\Sigma_{0}a_{n}}_{\sigma^2})$


\begin{align}
\epsilon^{0}[\psi^{*}]%
&= 1 - P(a^{T}_{n}x+b_n \leq 0 | Y=0)\\ 
&= 1 - \Phi\left(\frac{0 - (a^{T}_{n}\mu_0 + b_n)}{\sqrt{a^{T}_{n}\Sigma_{0}a_{n}}}\right)\\ 
&= 1 - \Phi\left(-\frac{a^{T}_{n}\mu_0 + b_n}{\sqrt{a^{T}_{n}\Sigma_{0}a_{n}}}\right)\\ 
&= \Phi\left(\frac{a^{T}_{n}\mu_0 + b_n}{\sqrt{a^{T}_{n}\Sigma_{0}a_{n}}}\right)
\end{align}

Similarly,

\begin{align}
    \epsilon^{1}(\psi^{*})%
    &= P(a_{n}^{T}x+b_n < 0 | Y=1)\\ 
    &= \Phi\left(\frac{0 - (a^{T}_{n}\mu_1 + b_n)}{\sqrt{a^{T}_{n}\Sigma_0 a_n}}\right)\\ 
    &= \Phi\left(-\frac{a^{T}_{n}\mu_1 + b_n}{\sqrt{a^{T}_{n}\Sigma_0 a_n}}\right)
\end{align}

Combining together,

\begin{align}
    \epsilon^{*}%
     &= \underbrace{P(Y=0)}_{=1-c}\epsilon^{0}[\psi^*] + \underbrace{P(Y=1)}_{=c}\epsilon^{1}[\psi^*]\\ 
     &= (1-c)\epsilon^{0}[\psi^*] + c\epsilon^{1}[\psi^*]\\ 
     &= (1-c)\Phi\left(\frac{a^{T}_{n}\mu_0 + b_n}{\sqrt{a^{T}_{n}\Sigma_{0}a_{n}}}\right) + c\Phi\left(-\frac{a^{T}_{n}\mu_1 + b_n}{\sqrt{a^{T}_{n}\Sigma_0 a_n}}\right)
\end{align}

[^g4]: Any affine transformation $f(x)=AX+B$ of a Gaussian is a Guassian. That is, $A$ is a nongingular matrix, and B is a vector. If $X\sim N(\mu, \Sigma)$, $a^T X+b \sim N(A^T \mu + b, A^T \Sigma A)$. [@ardianumam2017]

### (b)

> Compute the errors of the NMC, LDA, and DLDA classifiers in Example 4.2 if $c=1/2$, 
> \begin{equation*}
    \mu_0 =
    \begin{bmatrix}
        2\\ 
        3
    \end{bmatrix},
    \mu1 =
    \begin{bmatrix}
        6\\ 
        5
    \end{bmatrix},
    \Sigma_0 = 
    \begin{bmatrix}
        1 & 1\\ 
        1 & 2
    \end{bmatrix},
    \text{ and } 
    \Sigma_1 = 
    \begin{bmatrix}
        4 & 0\\
        0 & 1
    \end{bmatrix}
> \end{equation*}
> Which classifier does the best?



## Problem 4.4

> Even in the Gaussian case, the classification error of quadratic classifiers in general require numerical integration for its computation. In some special simple cases, however, it is possible to obtain exact solutions. Assume a two-dimensional Gaussian problem with $P(Y=1)=\frac{1}{2}$, $\mu_0=\mu_1 = 0$, $\Sigma_0=\sigma_{0}^{2}I_2$, and $\Sigma_1 = \sigma^{2}_{1}I_2$. For definiteness, assume that $\sigma_0 < \sigma_1$.

### (a)

> Show that the Bayes classifier is given by
> \begin{equation}
    \psi^{*}(x) = 
    \begin{cases}
        1, &\|x\| > r^{*},\\
        0, &\text{ otherwise },
    \end{cases}
    \quad \text{ where } r^{*} = \sqrt{2\left(\frac{1}{\sigma_{0}^{2}} - \frac{1}{\sigma_{1}^{2}}\right)^{-1}\ln\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}}}
> \end{equation}
> In particular, the optimal decision boundary is a circle of radius $r^{*}$.

The inverted $\Sigma_1$ and $\Sigma_2$ are[^invert]

\begin{align}
    \Sigma_0 &= \sigma_{0}^2 I_2 = \begin{bmatrix}
        \sigma_{0}^2 & 0 \\
        0 & \sigma_{0}^2
    \end{bmatrix}\\ 
    \Sigma_{0}^{-1} &= \frac{1}{\sigma_{0}^{4}} \begin{bmatrix}
        \sigma_{0}^2 & 0 \\
        0 & \sigma_{0}^2
    \end{bmatrix} = \sigma_{0}^{-2}\begin{bmatrix}
            1 & 0\\
            0 & 1
        \end{bmatrix} = \sigma^{-2}_{0}I_2\\
    \Sigma^{-1}_{1} &= \sigma^{-2}_{1}I_2
\end{align}

Use the derivation in @braga2020fundamentals [pp. 74],

\begin{equation}
    A_n = \begin{bmatrix}
        a_{11} & a_{12}\\ 
        a_{12} & a_{22}
    \end{bmatrix} = \frac{-1}{2} \Sigma_{1}^{-1} - \Sigma_{0}^{-1} = \frac{-1}{2}(\sigma_{1}^{-2} - \sigma_{0}^{-2}) \begin{bmatrix}
        1 & 0\\ 
        0 & 1
    \end{bmatrix}
\end{equation}

\begin{align}
    b_n &= \begin{bmatrix}
        b_{n,1}\\ 
        b_{n,2}
    \end{bmatrix}
    = \Sigma_{1}^{-1}\underbrace{\mu_1}_{=0} - \Sigma_{0}^{-1}\underbrace{\mu_{0}}_{=0}\\
        &= \begin{bmatrix}
            0\\ 
            0
        \end{bmatrix}
\end{align}

$$c = -\frac{1}{2}\ln\frac{|\Sigma_1|}{|\Sigma_0|} = \frac{-1}{2}\ln\frac{\sigma_{1}^{4}}{\sigma_{0}^{4}} = -\ln \frac{\sigma_{1}^2}{\sigma_{0}^2}$$

According to @braga2020fundamentals [Eq. 4.26], the 2-dimensional QDA decision boundary is 


\begin{align}
    D(x) = a_{11}x^{2}_1 + 2 a_{12}x_1x_2 + a_{22}x^{2}_{2} + b_1 x_1 + b_2 x_2 + c &= 0\\
    a_{11}(x_{1}^{2} + x_{2}^{2}) &= \ln \frac{\sigma_{1}^2}{\sigma_{0}^2}\\
    x^{2}_{1} + x^{2}_{2} &= 2(\frac{1}{\sigma^{2}_{0}} - \frac{1}{\sigma^{2}_{1}})^{-1}\ln\frac{\sigma_{1}^2}{\sigma_{0}^2}\\
    r^{*} = \sqrt{x^{2}_{1} + x^{2}_{2}} &= \sqrt{2(\frac{1}{\sigma^{2}_{0}} - \frac{1}{\sigma^{2}_{1}})^{-1}\ln\frac{\sigma_{1}^2}{\sigma_{0}^2}}
\end{align}

Noted that $\left(\frac{1}{\sigma^{2}_{0}} - \frac{1}{\sigma^{2}_{1}}\right) >  0$ because $\sigma_0 < \sigma_1$

For any point $\|x_j\| > r^{*}$, the discriminant ($D$) is larger than $0$, and $\psi^{*}(x_j) = 1$. 

[^invert]: \begin{equation}\begin{bmatrix}
    a & b\\ 
    c & d
\end{bmatrix}^{-1} = \frac{1}{ad-bc}\begin{bmatrix}
    d & -b\\ 
    -c & a
\end{bmatrix}
\end{equation}

### (b)

> Show that the corresponding Bayes error is given by
> $$\epsilon^{*} = \frac{1}{2} - \frac{1}{2}(\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}} - 1)e^{-(1-\frac{\sigma^{2}_{0}}{\sigma^{2}_{1}})^{-1}\ln \frac{\sigma^{2}_{1}}{\sigma^{2}_{0}}}$$
> In particular, the Bayes error is a function only of the ratio of variances $\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}}$, and $\epsilon^{*}\rightarrow 0$ as $\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}} \rightarrow \infty$.
>
> Hint: use polar coordinates to solve the required integrals analytically.


**Part I: Definition of errors**

\begin{align}
    \epsilon^{0}[\psi^{*}] 
    &= P(D^{*}(X)>k^{*}|Y=0)\\ 
    &= P(\|x\|>r^{*} | Y=0)
\end{align}



\begin{align}
    \epsilon^{1}[\psi^{*}] 
    &= P(D^{*}(X)\leq k^{*}|Y=1)\\ 
    &= P(\|x\|\leq r^{*} | Y=1)
\end{align}

**Part II: PDF of 2D Gaussian**

\begin{align}
p(x = \begin{bmatrix}
        x_1\\
        x_2
    \end{bmatrix}
    )%
    &= \frac{1}{\sqrt{(2\pi)^2 \sigma^4}} \exp(-\frac{1}{2}x^T \Sigma^{-1}x)\\
    &= \frac{1}{\sqrt{(2\pi)^2 \sigma^4}} \exp(-\frac{1}{2}\frac{x_{1}^{2}+x_{2}^{2}}{\sigma^2})\\ 
    &= \frac{1}{2\pi \sigma^2}\exp(-\frac{x_{1}^{2} + x_{2}^{2}}{2\sigma^2})\\ 
\end{align}

Use the polar coordination, $x_{1} = r\cos\theta$ and $x_{2} = r\sin\theta$. $x_{1}^{2} + x_{2}^{2} = r^2$. We can transform 2D gaussian into polar coordination:

$$p(r,\theta) = \frac{1}{2\pi \sigma^2}\exp(-\frac{r^2}{2\sigma^2})$$


**Part III: Integration**

\begin{align}
    \epsilon^{0}[\psi^{*}]%
    &= \int_{\theta=0}^{\theta=2\pi}\int_{r=r^{*}}^{\infty}   \frac{1}{2\pi \sigma_{0}^2}\exp(-\frac{r^2}{2\sigma_{0}^2})rdrd\theta\\
    &= \frac{1}{2\pi \sigma_{0}^2}\int_{\theta=0}^{\theta=2\pi}\int_{r=r^{*}}^{\infty}   \exp(-\frac{r^2}{2\sigma_{0}^2})rdrd\theta\\
    &= \frac{1}{2\pi\sigma_{0}^2}\int_{\theta=0}^{\theta=2\pi} \sigma_{0}^{2}\exp(-\frac{r_{*}^{2}}{2\sigma_{0}^{2}})d\theta\\ 
    &= \exp(-\frac{r_{*}^2}{2\sigma_{0}^{2}})\\ 
    &= \exp\left(- \frac{1}{\sigma_{0}^{2}(\sigma_{0}^{-2} - \sigma_{1}^{-2})} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &= \exp\left(\frac{-1}{(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\ 
    &= \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)
\end{align}



\begin{align}
    \epsilon^{1}[\psi^*] &= \int^{\theta=2\pi}_{\theta=0}\int_{r=0}^{r=r^*}  \frac{1}{2\pi \sigma_{1}^2}\exp(-\frac{r^2}{2\sigma_{1}^2})rdrd\theta\\ 
    &= 1 - \exp(- \frac{r_{*}^{2}}{2\sigma_{1}^{2}})\\ 
    &= 1 - \exp\left(- \frac{1}{\sigma_{1}^{2}(\sigma_{0}^{-2} - \sigma_{1}^{-2})} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &= 1 - \exp\left(- \frac{1}{(\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}} - 1)} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &= 1 - \exp\left(- \frac{\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}}{(1 - \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &= 1 - \exp\left(- \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}(1 - \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)
\end{align}

**Part IV: Combining together**


\begin{align}
    \epsilon^{*}%
    &= P(Y=0)\epsilon^{0}[\psi^{*}] + P(Y=1)\epsilon^{1}[\psi^{*}]\\
    &= \frac{1}{2}\epsilon^{0} + \frac{1}{2}\epsilon^{1}\\ 
    &= \frac{1}{2} \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right) + \frac{1}{2} - \frac{1}{2} \exp\left(- \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}(1 - \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &= \frac{1}{2} \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right) + \frac{1}{2}\\ 
    &- \frac{1}{2} \exp\left(- (\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}-1)(1 - \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)  \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\ 
    &= \frac{1}{2} + \frac{1}{2}\left[ 1 - \exp\left(- \underbrace{(\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}-1)(1 - \frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1})}_{=-1} \ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right) \right] \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &= \frac{1}{2} + \frac{1}{2}\left[ 1 - \exp\left(\ln\frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right) \right] \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &= \frac{1}{2} + \frac{1}{2}\left[ 1 - \frac{\sigma^{2}_{1}}{\sigma^{2}_{0}} \right] \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
    &= \frac{1}{2} - \frac{1}{2}\left( \frac{\sigma^{2}_{1}}{\sigma^{2}_{0}} -1 \right) \exp\left(-(1-\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}})^{-1}\ln \frac{\sigma_{1}^{2}}{\sigma_{0}^{2}}\right)\\
\end{align}



### (c)

> Compare the optimal classifier to the QDA classifier in Example 4.3. [@braga2020fundamentals, pp. 74] Compute the error of the QDA classifier and compare to the Bayes error.



## Problem 4.8 (Python Assignment)

> Apply linear discriminant analysis to the stacking fault energy (SFE) dataset (see @braga2020fundamentals [sec. A8.4]), already mentioned in @braga2020fundamentals [ch. 1]. Categorize the SFE values into two classes, low (SFE $\leq 35$) and high (SFE $\geq 45$), excluding the middle values.


### (a)

> Apply the preprocessing steps in [`c01_matex.py`](https://drive.google.com/file/d/1n0XLsXCa8qDZxxOPh6e6f2n4OGf2pPpz/view) to obtain a data matrix of dimensions $123 (\text{number of sample points}) \times 7 (\text{number of features})$, as described in @braga2020fundamentals [sec. 1.8.2]. Define low (SFE $\leq 35$) and high (SFE $\geq 45$) labels for the data. Pick the first $20\%$ of the sampe point s to be the training data and the remaining $80\%$ to be test data.

``` {python}
# Setting
def get_SFE_low(df):
    df_ = df[df["SFE"]<=35]
    return df_.loc[:, df_.columns!='SFE']
def get_SFE_high(df):
    df_ = df[df["SFE"]>=45]
    return df_.loc[:, df_.columns!='SFE']
# Load data
SFE_data = pd.read_table("data/Stacking_Fault_Energy_Dataset.txt")
# pre-process the data
f_org = SFE_data.columns[:-1]                # original features
n_org = SFE_data.shape[0]                    # original number of training points
p_org = np.sum(SFE_data.iloc[:,:-1]>0)/n_org # fraction of nonzero components for each feature
f_drp = f_org[p_org<0.6]		                 # features with less than 60% nonzero components
SFE1  = SFE_data.drop(f_drp,axis=1)          # drop those features
s_min = SFE1.min(axis=1)                     
SFE2  = SFE1[s_min!=0]                       # drop sample points with any zero values  
SFE   = SFE2[(SFE2.SFE<35)|(SFE2.SFE>45)]    # drop sample points with middle responses
train, test = train_test_split(SFE, test_size=0.8, shuffle=False)
```

``` {python}
#| label: tbl-total
#| tbl-cap: Filtered data

print(SFE.shape)
SFE.head(4)
```
```{python}
#| label: tbl-train
#| tbl-cap: Train data

print(train.shape)
train.head(4)
```
```{python}
#| label: tbl-test
#| tbl-cap: Test data

print(test.shape)
test.head(4)
```
 
### (b)

> Using the function `ttest_ind` from the `scipy.stats` module, apply Welch's two-sample t-test on the training data, and produce a table with the predictors, *T* statistic, and *p*-value, ordered with largest absolute *T* statistics at the top.

``` {python}
df_low = get_SFE_low(train)
df_high = get_SFE_high(train)

ttest = st.ttest_ind(df_low, df_high)

tdf = pd.DataFrame({
    "Features": df_low.keys(),
    "T-statistics": ttest.statistic,
    "P-Values": ttest.pvalue
})

tdf = tdf.sort_values(["T-statistics"], ascending=[0])
tdf 
```

### (c)

> Pick the top two predictors and design an LDA classifier. (This is an example of *filter feature selection*, to be discussed in Chapter 9.). Plot the training data with the superimposed LDA decision boundary. Plot the testing data with the superimposed previously-obtained LDA decision boundary. Estimate the classification error rate on the training and test data. What do you observe?

The classifier is well fit with the train data (@fig-48-train), but the error is comparatively high for test data (@fig-48-test). This indicates that the train data doesn't represent the distribution. On the other hand, the test data (@fig-48-test) implies that these two classes are inseparable with single boundary.

``` {python}
features = list(tdf["Features"][0:2])
var1, var2 = features
```

``` {python}
def get_data_with_features(data, features):
    X = data[features].values
    Y = data.SFE > 45
    return X, Y

def get_loss(X, Y, clf):
    loss = sk.metrics.zero_one_loss(Y, clf.predict(X))
    return loss

X, Y = get_data_with_features(train, features)
X_test, Y_test = get_data_with_features(test, features)

clf = LDA(priors=(0.5,0.5))
clf.fit(X,Y.astype(int))
a = clf.coef_[0]
b = clf.intercept_[0];

# Error
err_train = get_loss(X,Y, clf)
err_test = get_loss(X_test, Y_test, clf)
```


``` {python}
#| label: fig-48-train
#| fig-cap: Train data with LDA

def plot_swe_predict(ax, X, Y, clf, title=""):
    ax.scatter(X[~Y,0],X[~Y,1],c='blue',s=32,label='Low SFE')
    ax.scatter(X[Y,0],X[Y,1],c='orange',s=32,label='High SFE')
    left,right = ax.get_xlim()
    bottom,top = ax.get_ylim()
    ax.plot([left,right],[-left*a[0]/a[1]-b/a[1],-right*a[0]/a[1]-b/a[1]],'k',linewidth=2)
    ax.set_title(title)
    ax.set_xlim(left,right)
    ax.set_ylim(bottom,top)
    ax.set_xlabel(var1)
    ax.set_ylabel(var2)
    ax.legend();

fig43tr, ax43tr = plt.subplots()
ax43tr = plot_swe_predict(ax43tr, X, Y, clf, title="Train error {}".format(err_train));
```

``` {python}
#| label: fig-48-test
#| fig-cap: Test data with LDA

fig43t, ax43t = plt.subplots()
ax43t = plot_swe_predict(ax43t, X_test, Y_test, clf, title="Test error {}".format(err_test))
```


### (d)

> Repeat for the top three, and five predictors. Estimate the errors on the training and testing data (there is no need to plot the classifiers). What can you observe?

As shown in @tbl-48-survey, $5$ features are enough to lower the testing error. That means we need at least $5$ informative features to separate two classes.

``` {python}
#| label: tbl-48-survey
#| tbl-cap: Surveying different number of features

n_features = [2,3,5]
err_trains = np.zeros(len(n_features))
err_tests = np.zeros(len(n_features))
for (j,i) in enumerate(n_features):
    fs = list(tdf["Features"][0:i])
    X, Y = get_data_with_features(train, fs)
    X_test, Y_test = get_data_with_features(test, fs)

    clf = LDA(priors=(0.5,0.5))
    clf.fit(X,Y.astype(int))

    # Error
    err_trains[j] = get_loss(X,Y, clf)
    err_tests[j] = get_loss(X_test, Y_test, clf)

pd.DataFrame({
    "Number of Features": n_features,
    "Training Error Rate": err_trains,
    "Testing Error Rate": err_tests
})
```

::: {.content-hidden when-format="html"}

## References

:::


